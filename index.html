<!DOCTYPE html>
<html lang="en">

<head>
    <title>Wenxun-Arxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                wenxun-private-arxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-12T00:00:00Z">2025-02-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images
  with Depth and Normal Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Liu, Shuangming Zhao, Wanshou Jiang, Bingxuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With advancements in satellite imaging technology, acquiring high-resolution
multi-view satellite imagery has become increasingly accessible, enabling rapid
and location-independent ground model reconstruction. However, traditional
stereo matching methods struggle to capture fine details, and while neural
radiance fields (<span class="highlight-title">NeRF</span>s) achieve high-quality reconstructions, their training
time is prohibitively long. Moreover, challenges such as low visibility of
building facades, illumination and style differences between pixels, and weakly
textured regions in satellite imagery further make it hard to reconstruct
reasonable terrain geometry and detailed building facades. To address these
issues, we propose Sat-DN, a novel framework leveraging a progressively trained
multi-resolution hash grid reconstruction architecture with explicit depth
guidance and surface normal consistency constraints to enhance reconstruction
quality. The multi-resolution hash grid accelerates training, while the
progressive strategy incrementally increases the learning frequency, using
coarse low-frequency geometry to guide the reconstruction of fine
high-frequency details. The depth and normal constraints ensure a clear
building outline and correct planar distribution. Extensive experiments on the
DFC2019 <span class="highlight-title">dataset</span> demonstrate that Sat-DN outperforms existing methods, achieving
state-of-the-art results in both qualitative and quantitative evaluations. The
code is available at https://github.com/costune/SatDN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Appearance Particle Neural Radiance Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ancheng Lin, Yusheng Xiang, Jun Li, Mukesh Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) have shown great potential in modeling 3D
scenes. Dynamic <span class="highlight-title">NeRF</span>s extend this model by capturing time-varying elements,
typically using deformation fields. The existing dynamic <span class="highlight-title">NeRF</span>s employ a similar
Eulerian representation for both light radiance and deformation fields. This
leads to a close coupling of appearance and motion and lacks a physical
interpretation. In this work, we propose Dynamic Appearance Particle Neural
Radiance Field (DAP-<span class="highlight-title">NeRF</span>), which introduces particle-based representation to
model the motions of visual elements in a dynamic 3D scene. DAP-<span class="highlight-title">NeRF</span> consists
of the superposition of a static field and a dynamic field. The dynamic field
is quantized as a collection of appearance particles, which carries the visual
information of a small dynamic element in the scene and is equipped with a
motion model. All components, including the static field, the visual features
and the motion models of particles, are learned from monocular videos without
any prior geometric knowledge of the scene. We develop an efficient
computational framework for the particle-based model. We also construct a new
<span class="highlight-title">dataset</span> to evaluate motion modeling. Experimental results show that DAP-<span class="highlight-title">NeRF</span> is
an effective technique to capture not only the appearance but also the
physically meaningful motions in a 3D dynamic scene. Code is available at:
https://github.com/Cenbylin/DAP-<span class="highlight-title">NeRF</span>.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Learning With Sine-Activated Low-rank Matrices <span class="chip">ICLR
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model performance.
Our method proves to be a plug in enhancement for existing low-rank models, as
evidenced by its successful application in Vision <span class="highlight-title">Transformer</span>s (ViT), Large
Language Models (LLMs), Neural Radiance Fields (<span class="highlight-title">NeRF</span>) and 3D shape modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Paper accepted at ICLR
  2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Image Quality Assessment: Insights, Analysis, and Future
  Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengqian Ma, Zhengyi Shi, Zhiqiang Lu, Shenghao Xie, Fei Chao, Yao Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality assessment (IQA) represents a pivotal challenge in
image-focused technologies, significantly influencing the advancement
trajectory of image processing and computer vision. Recently, IQA has witnessed
a notable surge in innovative research efforts, driven by the emergence of
novel architectural paradigms and sophisticated computational techniques. This
<span class="highlight-title">survey</span> delivers an extensive analysis of contemporary IQA methodologies,
organized according to their application scenarios, serving as a beneficial
reference for both beginners and experienced researchers. We analyze the
advantages and limitations of current approaches and suggest potential future
research pathways. The <span class="highlight-title">survey</span> encompasses both general and specific IQA
methodologies, including conventional statistical measures, machine learning
techniques, and cutting-edge deep learning models such as convolutional neural
networks (CNNs) and <span class="highlight-title">Transformer</span> models. The analysis within this <span class="highlight-title">survey</span>
highlights the necessity for distortion-specific IQA methods tailored to
various application scenarios, emphasizing the significance of practicality,
interpretability, and ease of implementation in future developments.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-11T00:00:00Z">2025-02-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeshSplats: Mesh-Based <span class="highlight-title">Rendering</span> with Gaussian Splatting Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafał Tobiasz, Grzegorz Wilczyński, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer
graphics. GS-based algorithms almost always bypass classical methods such as
ray tracing, which offers numerous inherent advantages for <span class="highlight-title">rendering</span>. For
example, ray tracing is able to handle incoherent rays for advanced lighting
effects, including shadows and reflections. To address this limitation, we
introduce MeshSplats, a method which converts GS to a mesh-like format.
Following the completion of training, MeshSplats transforms Gaussian elements
into mesh faces, enabling <span class="highlight-title">rendering</span> using ray tracing methods with all their
associated benefits. Our model can be utilized immediately following
transformation, yielding a mesh of slightly reduced quality without additional
training. Furthermore, we can enhance the reconstruction quality through the
application of a dedicated optimization algorithm that operates on mesh faces
rather than Gaussian components. The efficacy of our method is substantiated by
experimental results, underscoring its extensive applications in computer
graphics and image processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Flow Distillation Sampling: Regularizing 3D Gaussians with <span class="highlight-title">Pre-train</span>ed
  Matching Priors <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin-Zhuo Chen, Kangjie Liu, Youtian Lin, Si<span class="highlight-author">yu Zhu</span>, Zhihao Li, Xun Cao, Yao Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has achieved excellent <span class="highlight-title">rendering</span> quality with
fast training and <span class="highlight-title">rendering</span> speed. However, its optimization process lacks
explicit geometric constraints, leading to suboptimal geometric reconstruction
in regions with sparse or no observational input views. In this work, we try to
mitigate the issue by incorporating a <span class="highlight-title">pre-train</span>ed matching prior to the 3DGS
optimization process. We introduce Flow Distillation Sampling (FDS), a
technique that leverages <span class="highlight-title">pre-train</span>ed geometric knowledge to bolster the
accuracy of the Gaussian radiance field. Our method employs a strategic
sampling technique to target unobserved views adjacent to the input views,
utilizing the optical flow calculated from the matching model (Prior Flow) to
guide the flow analytically calculated from the 3DGS geometry (Radiance Flow).
Comprehensive experiments in depth <span class="highlight-title">rendering</span>, mesh reconstruction, and novel
view synthesis showcase the significant advantages of FDS over state-of-the-art
methods. Additionally, our interpretive experiments and analysis aim to shed
light on the effects of FDS on geometric accuracy and <span class="highlight-title">rendering</span> quality,
potentially providing readers with insights into its performance. Project page:
https://nju-3dv.github.io/projects/fds
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mesh2SSM++: A Probabilistic Framework for Unsupervised Learning of
  Statistical Shape Model of Anatomies from Surface Meshes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krithika Iyer, Mokshagna Sai Teja Karanam, Shireen Elhabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomy evaluation is crucial for understanding the physiological state,
diagnosing abnormalities, and guiding medical interventions. Statistical shape
modeling (SSM) is vital in this process. By enabling the extraction of
quantitative morphological shape descriptors from MRI and CT scans, SSM
provides comprehensive descriptions of anatomical variations within a
population. However, the effectiveness of SSM in anatomy evaluation hinges on
the quality and robustness of the shape models. While deep learning techniques
show promise in addressing these challenges by learning complex nonlinear
representations of shapes, existing models still have limitations and often
require pre-established shape models for training. To overcome these issues, we
propose Mesh2SSM++, a novel approach that learns to estimate correspondences
from meshes in an unsupervised manner. This method leverages unsupervised,
permutation-invariant representation learning to estimate how to deform a
template point cloud into subject-specific meshes, forming a
correspondence-based shape model. Additionally, our probabilistic formulation
allows learning a population-specific template, reducing potential biases
associated with template selection. A key feature of Mesh2SSM++ is its ability
to quantify aleatoric uncertainty, which captures inherent data variability and
is essential for ensuring reliable model predictions and robust decision-making
in clinical tasks, especially under challenging imaging conditions. Through
extensive validation across diverse anatomies, evaluation metrics, and
downstream tasks, we demonstrate that Mesh2SSM++ outperforms existing methods.
Its ability to operate directly on meshes, combined with computational
efficiency and interpretability through its probabilistic framework, makes it
an attractive alternative to traditional and deep learning-based SSM
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Multi-Human Neural <span class="highlight-title">Rendering</span> Using Geometry Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian li, Victoria Fernàndez Abrevaya, Franck Multon, Adnane Boukhayma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for recovering the shape and radiance of a scene
consisting of multiple people given solely a few images. Multi-human scenes are
complex due to additional occlusion and clutter. For single-human settings,
existing approaches using implicit neural representations have achieved
impressive results that deliver accurate geometry and appearance. However, it
remains challenging to extend these methods for estimating multiple humans from
sparse views. We propose a neural implicit reconstruction method that addresses
the inherent challenges of this task through the following contributions:
First, we propose to use geometry constraints by exploiting pre-computed meshes
using a human body model (SMPL). Specifically, we regularize the signed
distances using the SMPL mesh and leverage bounding boxes for improved
<span class="highlight-title">rendering</span>. Second, we propose a ray regularization scheme to minimize <span class="highlight-title">rendering</span>
inconsistencies, and a saturation regularization for robust optimization in
variable illumination. Extensive experiments on both real and synthetic
<span class="highlight-title">dataset</span>s demonstrate the benefits of our approach and show state-of-the-art
performance against existing neural reconstruction methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpaceMesh: A Continuous Representation for Learning Manifold Surface
  Meshes <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchang Shen, Zhaoshuo Li, Marc Law, Matan Atzmon, Sanja Fidler, James Lucas, Jun Gao, Nicholas Sharp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meshes are ubiquitous in visual computing and simulation, yet most existing
machine learning techniques represent meshes only indirectly, e.g. as the level
set of a scalar field or deformation of a template, or as a disordered triangle
soup lacking local structure. This work presents a scheme to directly generate
manifold, polygonal meshes of complex connectivity as the output of a neural
network. Our key innovation is to define a continuous latent connectivity space
at each mesh vertex, which implies the discrete mesh. In particular, our vertex
embeddings generate cyclic neighbor relationships in a halfedge mesh
representation, which gives a guarantee of edge-manifoldness and the ability to
represent general polygonal meshes. This representation is well-suited to
machine learning and stochastic optimization, without restriction on
connectivity or topology. We first explore the basic properties of this
representation, then use it to fit distributions of meshes from large <span class="highlight-title">dataset</span>s.
The resulting models generate diverse meshes with tessellation structure
learned from the <span class="highlight-title">dataset</span> population, with concise details and high-quality mesh
elements. In applications, this approach not only yields high-quality outputs
from generative models, but also enables directly learning challenging geometry
processing tasks such as mesh repair.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at SIGGRAPH Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel
  View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present billboard Splatting (BBSplat) - a novel approach for 3D scene
representation based on textured geometric primitives. BBSplat represents the
scene as a set of optimizable textured planar primitives with learnable RGB
textures and alpha-maps to control their shape. BBSplat primitives can be used
in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. The
proposed primitives close the <span class="highlight-title">rendering</span> quality gap between 2D and 3D Gaussian
Splatting (GS), preserving the accurate mesh extraction ability of 2D
primitives. Our novel regularization term encourages textures to have a sparser
structure, unlocking an efficient compression that leads to a reduction in the
storage space of the model. Our experiments show the efficiency of BBSplat on
standard <span class="highlight-title">dataset</span>s of real indoor and outdoor scenes such as Tanks&Temples, DTU,
and Mip-<span class="highlight-title">NeRF</span>-360.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual
  Descriptions Using Gaussian Splatting, Chat<span class="highlight-title">GPT</span>/Deepseek, and Google Maps
  Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban digital twins are virtual replicas of cities that use multi-source data
and data analytics to optimize urban planning, infrastructure management, and
decision-making. Towards this, we propose a framework focused on the
single-building scale. By connecting to cloud mapping platforms such as Google
Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language
Models data analysis using Chat<span class="highlight-title">GPT</span>(4o) and Deepseek-V3/R1, and by using our
Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings
framework can retrieve a building's 3D model, visual descriptions, and achieve
cloud-based mapping integration with large language model-based data analytics
using a building's address, postal code, or geographic coordinates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-Fixed minor typo</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PoI: Pixel of Interest for Novel View Synthesis Assisted Scene
  Coordinate Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of estimating camera poses can be enhanced through novel view
synthesis techniques such as <span class="highlight-title">NeRF</span> and Gaussian Splatting to increase the
diversity and extension of training data. However, these techniques often
produce rendered images with issues like blurring and ghosting, which
compromise their reliability. These issues become particularly pronounced for
Scene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the
pixel level. To mitigate the problems associated with unreliable rendered
images, we introduce a novel filtering approach, which selectively extracts
well-rendered pixels while discarding the inferior ones. This filter
simultaneously measures the SCR model's real-time reprojection loss and
gradient during training. Building on this filtering technique, we also develop
a new strategy to improve scene coordinate regression using sparse inputs,
drawing on successful applications of sparse input techniques in novel view
synthesis. Our experimental results validate the effectiveness of our method,
demonstrating state-of-the-art performance on indoor and outdoor <span class="highlight-title">dataset</span>s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel
  View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present billboard Splatting (BBSplat) - a novel approach for 3D scene
representation based on textured geometric primitives. BBSplat represents the
scene as a set of optimizable textured planar primitives with learnable RGB
textures and alpha-maps to control their shape. BBSplat primitives can be used
in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. The
proposed primitives close the <span class="highlight-title">rendering</span> quality gap between 2D and 3D Gaussian
Splatting (GS), preserving the accurate mesh extraction ability of 2D
primitives. Our novel regularization term encourages textures to have a sparser
structure, unlocking an efficient compression that leads to a reduction in the
storage space of the model. Our experiments show the efficiency of BBSplat on
standard <span class="highlight-title">dataset</span>s of real indoor and outdoor scenes such as Tanks&Temples, DTU,
and Mip-<span class="highlight-title">NeRF</span>-360.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamCatalyst: Fast and High-Quality 3D Editing via Controlling
  Editability and Identity Preservation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwook Kim, Seonho Lee, Jaeyo Shin, Jiho Choi, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score distillation sampling (SDS) has emerged as an effective framework in
text-driven 3D editing tasks, leveraging <span class="highlight-title">diffusion</span> models for 3D-consistent
editing. However, existing SDS-based 3D editing methods suffer from long
training times and produce low-quality results. We identify that the root cause
of this performance degradation is \textit{their conflict with the sampling
dynamics of <span class="highlight-title">diffusion</span> models}. Addressing this conflict allows us to treat SDS
as a <span class="highlight-title">diffusion</span> reverse process for 3D editing via sampling from data space. In
contrast, existing methods naively distill the score function using <span class="highlight-title">diffusion</span>
models. From these insights, we propose DreamCatalyst, a novel framework that
considers these sampling dynamics in the SDS framework. Specifically, we devise
the optimization process of our DreamCatalyst to approximate the <span class="highlight-title">diffusion</span>
reverse process in editing tasks, thereby aligning with <span class="highlight-title">diffusion</span> sampling
dynamics. As a result, DreamCatalyst successfully reduces training time and
improves editing quality. Our method offers two modes: (1) a fast mode that
edits Neural Radiance Fields (<span class="highlight-title">NeRF</span>) scenes approximately 23 times faster than
current state-of-the-art <span class="highlight-title">NeRF</span> editing methods, and (2) a high-quality mode that
produces superior results about 8 times faster than these methods. Notably, our
high-quality mode outperforms current state-of-the-art <span class="highlight-title">NeRF</span> editing methods in
terms of both speed and quality. DreamCatalyst also surpasses the
state-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing
itself as an effective and model-agnostic 3D editing solution. See more
extensive results on our project page: https://dream-catalyst.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Optimal Proximal Gradient Algorithm for Non-Blind Image
  Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingsong Wang, Shengze Xu, Xiaojiao Tong, Tieyong Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image deblurring remains a central research area within image processing,
critical for its role in enhancing image quality and facilitating clearer
visual representations across diverse applications. This paper tackles the
optimization problem of image deblurring, assuming a known blurring kernel. We
introduce an improved optimal proximal gradient algorithm (IOptISTA), which
builds upon the optimal gradient method and a weighting matrix, to efficiently
address the non-blind image deblurring problem. Based on two regularization
cases, namely the $l_1$ norm and total variation norm, we perform numerical
experiments to assess the performance of our proposed algorithm. The results
indicate that our algorithm yields enhanced PSNR and SSIM values, as well as a
reduced tolerance, compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-10T00:00:00Z">2025-02-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified
  Flow Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in <span class="highlight-title">diffusion</span> techniques have propelled image and video
generation to unprece- dented levels of quality, significantly accelerating the
deployment and application of generative AI. However, 3D shape generation
technology has so far lagged behind, constrained by limitations in 3D data
scale, complexity of 3D data process- ing, and insufficient exploration of
advanced tech- niques in the 3D domain. Current approaches to 3D shape
generation face substantial challenges in terms of output quality,
generalization capa- bility, and alignment with input conditions. We present
TripoSG, a new streamlined shape diffu- sion paradigm capable of generating
high-fidelity 3D meshes with precise correspondence to input images.
Specifically, we propose: 1) A large-scale rectified flow <span class="highlight-title">transformer</span> for 3D
shape generation, achieving state-of-the-art fidelity through training on
extensive, high-quality data. 2) A hybrid supervised training strategy
combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality
3D reconstruction performance. 3) A data processing pipeline to generate 2
million high- quality 3D samples, highlighting the crucial rules for data
quality and quantity in training 3D gen- erative models. Through comprehensive
experi- ments, we have validated the effectiveness of each component in our new
framework. The seamless integration of these parts has enabled TripoSG to
achieve state-of-the-art performance in 3D shape generation. The resulting 3D
shapes exhibit en- hanced detail due to high-resolution capabilities and
demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG
demonstrates improved versatility in generating 3D models from diverse image
styles and contents, showcasing strong gen- eralization capabilities. To foster
progress and innovation in the field of 3D generation, we will make our model
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural
  Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18917v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18917v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyao Duan, Zhiliu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous attempts to integrate Neural Radiance Fields (<span class="highlight-title">NeRF</span>) into the
Simultaneous Localization and Mapping (SLAM) framework either rely on the
assumption of static scenes or require the ground truth camera poses, which
impedes their application in real-world scenarios. This paper proposes a
time-varying representation to track and reconstruct the dynamic scenes.
Firstly, two processes, a tracking process and a mapping process, are
maintained simultaneously in our framework. In the tracking process, all input
images are uniformly sampled and then progressively trained in a
<span class="highlight-title">self-supervised</span> paradigm. In the mapping process, we leverage motion masks to
distinguish dynamic objects from the static background, and sample more pixels
from dynamic areas. Secondly, the parameter optimization for both processes is
comprised of two stages: the first stage associates time with 3D positions to
convert the deformation field to the canonical field. The second stage
associates time with the embeddings of the canonical field to obtain colors and
a Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection
strategy based on the overlapping rate. Our approach is evaluated on two
synthetic <span class="highlight-title">dataset</span>s and one real-world <span class="highlight-title">dataset</span>, and the experiments validate
that our method achieves competitive results in both tracking and mapping when
compared to existing state-of-the-art <span class="highlight-title">NeRF</span>-based dynamic SLAM systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrismAvatar: Real-time animated 3D neural head avatars on edge devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PrismAvatar: a 3D head avatar model which is designed specifically
to enable real-time animation and <span class="highlight-title">rendering</span> on resource-constrained edge
devices, while still enjoying the benefits of neural volumetric <span class="highlight-title">rendering</span> at
training time. By integrating a rigged prism lattice with a 3D morphable head
model, we use a hybrid <span class="highlight-title">rendering</span> model to simultaneously reconstruct a
mesh-based head and a deformable <span class="highlight-title">NeRF</span> model for regions not represented by the
3DMM. We then distill the deformable <span class="highlight-title">NeRF</span> into a rigged mesh and neural
textures, which can be animated and rendered efficiently within the constraints
of the traditional triangle <span class="highlight-title">rendering</span> pipeline. In addition to running at 60
fps with low memory usage on mobile devices, we find that our trained models
have comparable quality to state-of-the-art 3D avatar models on desktop
devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified
  Flow Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in <span class="highlight-title">diffusion</span> techniques have propelled image and video
generation to unprece- dented levels of quality, significantly accelerating the
deployment and application of generative AI. However, 3D shape generation
technology has so far lagged behind, constrained by limitations in 3D data
scale, complexity of 3D data process- ing, and insufficient exploration of
advanced tech- niques in the 3D domain. Current approaches to 3D shape
generation face substantial challenges in terms of output quality,
generalization capa- bility, and alignment with input conditions. We present
TripoSG, a new streamlined shape diffu- sion paradigm capable of generating
high-fidelity 3D meshes with precise correspondence to input images.
Specifically, we propose: 1) A large-scale rectified flow <span class="highlight-title">transformer</span> for 3D
shape generation, achieving state-of-the-art fidelity through training on
extensive, high-quality data. 2) A hybrid supervised training strategy
combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality
3D reconstruction performance. 3) A data processing pipeline to generate 2
million high- quality 3D samples, highlighting the crucial rules for data
quality and quantity in training 3D gen- erative models. Through comprehensive
experi- ments, we have validated the effectiveness of each component in our new
framework. The seamless integration of these parts has enabled TripoSG to
achieve state-of-the-art performance in 3D shape generation. The resulting 3D
shapes exhibit en- hanced detail due to high-resolution capabilities and
demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG
demonstrates improved versatility in generating 3D models from diverse image
styles and contents, showcasing strong gen- eralization capabilities. To foster
progress and innovation in the field of 3D generation, we will make our model
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeshUp: Multi-Target Mesh Deformation via Blended Score Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14899v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14899v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoo Kim, Itai Lang, Noam Aigerman, Thibault Groueix, Vladimir G. Kim, Rana Hanocka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MeshUp, a technique that deforms a 3D mesh towards multiple target
concepts, and intuitively controls the region where each concept is expressed.
Conveniently, the concepts can be defined as either text queries, e.g., "a dog"
and "a turtle," or inspirational images, and the local regions can be selected
as any number of vertices on the mesh. We can effectively control the influence
of the concepts and mix them together using a novel score distillation
approach, referred to as the Blended Score Distillation (BSD). BSD operates on
each attention layer of the denoising U-Net of a <span class="highlight-title">diffusion</span> model as it extracts
and injects the per-objective activations into a unified denoising pipeline
from which the deformation gradients are calculated. To localize the expression
of these activations, we create a probabilistic Region of Interest (ROI) map on
the surface of the mesh, and turn it into 3D-consistent masks that we use to
control the expression of these activations. We demonstrate the effectiveness
of BSD empirically and show that it can deform various meshes towards multiple
objectives. Our project page is at https://threedle.github.io/MeshUp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://threedle.github.io/MeshUp</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrismAvatar: Real-time animated 3D neural head avatars on edge devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PrismAvatar: a 3D head avatar model which is designed specifically
to enable real-time animation and <span class="highlight-title">rendering</span> on resource-constrained edge
devices, while still enjoying the benefits of neural volumetric <span class="highlight-title">rendering</span> at
training time. By integrating a rigged prism lattice with a 3D morphable head
model, we use a hybrid <span class="highlight-title">rendering</span> model to simultaneously reconstruct a
mesh-based head and a deformable <span class="highlight-title">NeRF</span> model for regions not represented by the
3DMM. We then distill the deformable <span class="highlight-title">NeRF</span> into a rigged mesh and neural
textures, which can be animated and rendered efficiently within the constraints
of the traditional triangle <span class="highlight-title">rendering</span> pipeline. In addition to running at 60
fps with low memory usage on mobile devices, we find that our trained models
have comparable quality to state-of-the-art 3D avatar models on desktop
devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Creativity in Physics: A Brief <span class="highlight-title">Survey</span> of Physical Priors in
  AIGC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwei Meng, Yawei Luo, Ping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in AI-generated content have significantly improved the
realism of 3D and 4D generation. However, most existing methods prioritize
appearance consistency while neglecting underlying physical principles, leading
to artifacts such as unrealistic deformations, unstable dynamics, and
implausible objects interactions. Incorporating physics priors into generative
models has become a crucial research direction to enhance structural integrity
and motion realism. This <span class="highlight-title">survey</span> provides a <span class="highlight-title">review</span> of physics-aware generative
methods, systematically analyzing how physical constraints are integrated into
3D and 4D generation. First, we examine recent works in incorporating physical
priors into static and dynamic 3D generation, categorizing methods based on
representation types, including vision-based, <span class="highlight-title">NeRF</span>-based, and Gaussian
Splatting-based approaches. Second, we explore emerging techniques in 4D
generation, focusing on methods that model temporal dynamics with physical
simulations. Finally, we conduct a comparative analysis of major methods,
highlighting their strengths, limitations, and suitability for different
materials and motion dynamics. By presenting an in-depth analysis of
physics-grounded AIGC, this <span class="highlight-title">survey</span> aims to bridge the gap between generative
models and physical realism, providing insights that inspire future research in
physically consistent content generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural
  Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18917v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18917v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyao Duan, Zhiliu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous attempts to integrate Neural Radiance Fields (<span class="highlight-title">NeRF</span>) into the
Simultaneous Localization and Mapping (SLAM) framework either rely on the
assumption of static scenes or require the ground truth camera poses, which
impedes their application in real-world scenarios. This paper proposes a
time-varying representation to track and reconstruct the dynamic scenes.
Firstly, two processes, a tracking process and a mapping process, are
maintained simultaneously in our framework. In the tracking process, all input
images are uniformly sampled and then progressively trained in a
<span class="highlight-title">self-supervised</span> paradigm. In the mapping process, we leverage motion masks to
distinguish dynamic objects from the static background, and sample more pixels
from dynamic areas. Secondly, the parameter optimization for both processes is
comprised of two stages: the first stage associates time with 3D positions to
convert the deformation field to the canonical field. The second stage
associates time with the embeddings of the canonical field to obtain colors and
a Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection
strategy based on the overlapping rate. Our approach is evaluated on two
synthetic <span class="highlight-title">dataset</span>s and one real-world <span class="highlight-title">dataset</span>, and the experiments validate
that our method achieves competitive results in both tracking and mapping when
compared to existing state-of-the-art <span class="highlight-title">NeRF</span>-based dynamic SLAM systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Indoor Light and Heat Estimation from a Single Panorama 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanzhou Ji, Sriram Narayanan, Azadeh Sawyer, Srinivasa Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel application for directly estimating indoor light
and heat maps from captured indoor-outdoor High Dynamic Range (HDR) panoramas.
In our image-based <span class="highlight-title">rendering</span> method, the indoor panorama is used to estimate
the 3D room layout, while the corresponding outdoor panorama serves as an
environment map to infer spatially-varying light and material properties. We
establish a connection between indoor light transport and heat transport and
implement transient heat simulation to generate indoor heat panoramas. The
sensitivity analysis of various thermal parameters is conducted, and the
resulting heat maps are compared with the images captured by the thermal camera
in real-world scenarios. This digital application enables automatic indoor
light and heat estimation without manual inputs and cumbersome field
measurements.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Intrinsic Scale Assessment: Bridging the Gap Between Quality and
  Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Hosu, Lorenzo Agnolucci, Daisuke Iso, Dietmar Saupe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Quality Assessment (IQA) measures and predicts perceived image quality
by human observers. Although recent studies have highlighted the critical
influence that variations in the scale of an image have on its perceived
quality, this relationship has not been systematically quantified. To bridge
this gap, we introduce the Image Intrinsic Scale (IIS), defined as the largest
scale where an image exhibits its highest perceived quality. We also present
the Image Intrinsic Scale Assessment (IISA) task, which involves subjectively
measuring and predicting the IIS based on human judgments. We develop a
subjective annotation methodology and create the IISA-DB <span class="highlight-title">dataset</span>, comprising
785 image-IIS pairs annotated by experts in a rigorously controlled
crowdsourcing study. Furthermore, we propose WIISA (Weak-labeling for Image
Intrinsic Scale Assessment), a strategy that leverages how the IIS of an image
varies with downscaling to generate weak labels. Experiments show that applying
WIISA during the training of several IQA methods adapted for IISA consistently
improves the performance compared to using only ground-truth labels. We will
release the code, <span class="highlight-title">dataset</span>, and <span class="highlight-title">pre-train</span>ed models upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-09T00:00:00Z">2025-02-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingsong Yan, Qiang Wang, Kaiyong Zhao, Jie Chen, Bo Li, Xiaowen Chu, Fei Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the rapid development of panorama cameras, the task of estimating
panorama depth has attracted significant attention from the computer vision
community, especially in applications such as robot sensing and autonomous
driving. However, existing methods relying on different projection formats
often encounter challenges, either struggling with distortion and discontinuity
in the case of equirectangular, cubemap, and tangent projections, or
experiencing a loss of texture details with the spherical projection. To tackle
these concerns, we present SphereFusion, an end-to-end framework that combines
the strengths of various projection methods. Specifically, SphereFusion
initially employs 2D image convolution and mesh operations to extract two
distinct types of features from the panorama image in both equirectangular and
spherical projection domains. These features are then projected onto the
spherical domain, where a gate fusion module selects the most reliable features
for fusion. Finally, SphereFusion estimates panorama depth within the spherical
domain. Meanwhile, SphereFusion employs a cache strategy to improve the
efficiency of mesh operation. Extensive experiments on three public panorama
<span class="highlight-title">dataset</span>s demonstrate that SphereFusion achieves competitive results with other
state-of-the-art methods, while presenting the fastest inference speed at only
17 ms on a 512$\times$1024 panorama image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based
  Implicit Neural Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Pan, Xingguang Zhong, Liren Jin, Louis Wiesmann, Marija Popović, Jens Behley, Cyrill Stachniss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots require high-fidelity reconstructions of their environment for
effective operation. Such scene representations should be both, geometrically
accurate and photorealistic to support downstream tasks. While this can be
achieved by building distance fields from range sensors and radiance fields
from cameras, the scalable incremental mapping of both fields consistently and
at the same time with high quality remains challenging. In this paper, we
propose a novel map representation that unifies a continuous signed distance
field and a Gaussian splatting radiance field within an elastic and compact
point-based implicit neural map. By enforcing geometric consistency between
these fields, we achieve mutual improvements by exploiting both modalities. We
devise a LiDAR-visual SLAM system called PINGS using the proposed map
representation and evaluate it on several challenging large-scale <span class="highlight-title">dataset</span>s.
Experimental results demonstrate that PINGS can incrementally build globally
consistent distance and radiance fields encoded with a compact set of neural
points. Compared to the state-of-the-art methods, PINGS achieves superior
photometric and geometric <span class="highlight-title">rendering</span> at novel views by leveraging the
constraints from the distance field. Furthermore, by utilizing dense
photometric cues and multi-view consistency from the radiance field, PINGS
produces more accurate distance fields, leading to improved odometry estimation
and mesh reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Text-to-3D Generation via Surface-Aligned Gaussian
  Splatting <span class="chip">3DV-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09981v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09981v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-3D and image-to-3D generation tasks have received considerable
attention, one important but under-explored field between them is controllable
text-to-3D generation, which we mainly focus on in this work. To address this
task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network
architecture designed to enhance existing <span class="highlight-title">pre-train</span>ed multi-view <span class="highlight-title">diffusion</span>
models by integrating additional input conditions, such as edge, depth, normal,
and scribble maps. Our innovation lies in the introduction of a conditioning
module that controls the base <span class="highlight-title">diffusion</span> model using both local and global
embeddings, which are computed from the input condition images and camera
poses. Once trained, MVControl is able to offer 3D <span class="highlight-title">diffusion</span> guidance for
optimization-based 3D generation. And, 2) we propose an efficient multi-stage
3D generation pipeline that leverages the benefits of recent large
reconstruction models and score distillation algorithm. Building upon our
MVControl architecture, we employ a unique hybrid <span class="highlight-title">diffusion</span> guidance method to
direct the optimization process. In pursuit of efficiency, we adopt 3D
Gaussians as our representation instead of the commonly used implicit
representations. We also pioneer the use of SuGaR, a hybrid representation that
binds Gaussians to mesh triangle faces. This approach alleviates the issue of
poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained
geometry on the mesh. Extensive experiments demonstrate that our method
achieves robust generalization and enables the controllable generation of
high-quality 3D content. Project page: https://lizhiqi49.github.io/MVControl/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV-2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-07T00:00:00Z">2025-02-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AniSDF: Fused-Granularity Neural Surfaces with Anisotropic Encoding for
  High-Fidelity 3D Reconstruction <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingnan Gao, Zhuo Chen, Xiaokang Yang, Yichao Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields have recently revolutionized novel-view synthesis and
achieved high-fidelity <span class="highlight-title">rendering</span>s. However, these methods sacrifice the
geometry for the <span class="highlight-title">rendering</span> quality, limiting their further applications
including relighting and deformation. How to synthesize photo-realistic
<span class="highlight-title">rendering</span> while reconstructing accurate geometry remains an unsolved problem.
In this work, we present AniSDF, a novel approach that learns fused-granularity
neural surfaces with physics-based encoding for high-fidelity 3D
reconstruction. Different from previous neural surfaces, our fused-granularity
geometry structure balances the overall structures and fine geometric details,
producing accurate geometry reconstruction. To disambiguate geometry from
reflective appearance, we introduce blended radiance fields to model diffuse
and specularity following the anisotropic spherical Gaussian encoding, a
physics-based <span class="highlight-title">rendering</span> pipeline. With these designs, AniSDF can reconstruct
objects with complex structures and produce high-quality <span class="highlight-title">rendering</span>s.
Furthermore, our method is a unified model that does not require complex
hyperparameter tuning for specific objects. Extensive experiments demonstrate
that our method boosts the quality of SDF-based methods by a great scale in
both geometry reconstruction and novel-view synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025, Project Page:
  https://g-1nonly.github.io/AniSDF_Website/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NextBestPath: Efficient 3D Mapping of Unseen Environments <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyao Li, Antoine Guédon, Clémentin Boittiaux, Shizhe Chen, Vincent Lepetit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the problem of active 3D mapping, where an agent must
find an efficient trajectory to exhaustively reconstruct a new scene. Previous
approaches mainly predict the next best view near the agent's location, which
is prone to getting stuck in local areas. Additionally, existing indoor
<span class="highlight-title">dataset</span>s are insufficient due to limited geometric complexity and inaccurate
ground truth meshes. To overcome these limitations, we introduce a novel
<span class="highlight-title">dataset</span> AiMDoom with a map generator for the Doom video game, enabling to
better benchmark active 3D mapping in diverse indoor environments. Moreover, we
propose a new method we call next-best-path (NBP), which predicts long-term
goals rather than focusing solely on short-sighted views. The model jointly
predicts accumulated surface coverage gains for long-term goals and obstacle
maps, allowing it to efficiently plan optimal paths with a unified model. By
leveraging online data collection, data augmentation and curriculum learning,
NBP significantly outperforms state-of-the-art methods on both the existing
MP3D <span class="highlight-title">dataset</span> and our AiMDoom <span class="highlight-title">dataset</span>, achieving more efficient mapping in
indoor environments of varying complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICLR 2025. Project webpage:
  https://shiyao-li.github.io/nbp/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Clustering for Prefractured Mesh Generation in Real-time Object
  Destruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghwan Kim, Sunha Park, Seungkyu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prefracture method is a practical implementation for real-time object
destruction that is hardly achievable within performance constraints, but can
produce unrealistic results due to its heuristic nature. To mitigate it, we
approach the clustering of prefractured mesh generation as an unordered
segmentation on point cloud data, and propose leveraging the deep neural
network trained on a physics-based <span class="highlight-title">dataset</span>. Our novel paradigm successfully
predicts the structural weakness of object that have been limited, exhibiting
ready-to-use results with remarkable quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Surface Priors for Editable Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Szymkowiak, Weronika Jakubowska, Dawid Malarz, Weronika Smolak-Dyżewska, Maciej Zięba, Przemyslaw Musialski, Wojtek Pałubicki, Przemysław Spurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer graphics and vision, recovering easily modifiable scene
appearance from image data is crucial for applications such as content
creation. We introduce a novel method that integrates 3D Gaussian Splatting
with an implicit surface representation, enabling intuitive editing of
recovered scenes through mesh manipulation. Starting with a set of input images
and camera poses, our approach reconstructs the scene surface using a neural
signed distance field. This neural surface acts as a geometric prior guiding
the training of Gaussian Splatting components, ensuring their alignment with
the scene geometry. To facilitate editing, we encode the visual and geometric
information into a lightweight triangle soup proxy. Edits applied to the mesh
extracted from the neural surface propagate seamlessly through this
intermediate structure to update the recovered appearance. Unlike previous
methods relying on the triangle soup proxy representation, our approach
supports a wider range of modifications and fully leverages the mesh topology,
enabling a more flexible and intuitive editing process. The complete source
code for this project can be accessed at:
https://github.com/WJakubowska/NeuralSurfacePriors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Mobile Display Photometric Stereo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gawoon Ban, Hyeongjun Kim, Seokjun Choi, Seungwoo Yoon, Seung-Hwan Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Display photometric stereo uses a display as a programmable light source to
illuminate a scene with diverse illumination conditions. Recently,
differentiable display photometric stereo (DDPS) demonstrated improved normal
reconstruction accuracy by using learned display patterns. However, DDPS faced
limitations in practicality, requiring a fixed desktop imaging setup using a
polarization camera and a desktop-scale monitor. In this paper, we propose a
more practical physics-based photometric stereo, differentiable mobile display
photometric stereo (DMDPS), that leverages a mobile phone consisting of a
display and a camera. We overcome the limitations of using a mobile device by
developing a mobile app and method that simultaneously displays patterns and
captures high-quality HDR images. Using this technique, we capture real-world
3D-printed objects and learn display patterns via a differentiable learning
process. We demonstrate the effectiveness of DMDPS on both a 3D printed <span class="highlight-title">dataset</span>
and a first <span class="highlight-title">dataset</span> of fallen leaves. The leaf <span class="highlight-title">dataset</span> contains reconstructed
surface normals and albedos of fallen leaves that may enable future research
beyond computer graphics and vision. We believe that DMDPS takes a step forward
for practical physics-based photometric stereo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Conformal Prediction for Uncertainty Quantification in
  Imaging Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasper M. Everink, Bernardin Tamo Amougou, Marcelo Pereyra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most image restoration problems are ill-conditioned or ill-posed and hence
involve significant uncertainty. Quantifying this uncertainty is crucial for
reliably interpreting experimental results, particularly when reconstructed
images inform critical decisions and science. However, most existing image
restoration methods either fail to quantify uncertainty or provide estimates
that are highly inaccurate. Conformal prediction has recently emerged as a
flexible framework to equip any estimator with uncertainty quantification
capabilities that, by construction, have nearly exact marginal coverage. To
achieve this, conformal prediction relies on abundant ground truth data for
calibration. However, in image restoration problems, reliable ground truth data
is often expensive or not possible to acquire. Also, reliance on ground truth
data can introduce large biases in situations of distribution shift between
calibration and deployment. This paper seeks to develop a more robust approach
to conformal prediction for image restoration problems by proposing a
<span class="highlight-title">self-supervised</span> conformal prediction method that leverages Stein's Unbiased
Risk Estimator (SURE) to self-calibrate itself directly from the observed noisy
measurements, bypassing the need for ground truth. The method is suitable for
any linear imaging inverse problem that is ill-conditioned, and it is
especially powerful when used with modern <span class="highlight-title">self-supervised</span> image restoration
techniques that can also be trained directly from measurement data. The
proposed approach is demonstrated through numerical experiments on image
denoising and deblurring, where it delivers results that are remarkably
accurate and comparable to those obtained by supervised conformal prediction
with ground truth data.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-06T00:00:00Z">2025-02-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Factorized Implicit Global Convolution for Automotive Computational
  Fluid Dynamics Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Choy, Alexey Kamenev, Jean Kossaifi, Max Rietmann, Jan Kautz, Kamyar Azizzadenesheli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational Fluid Dynamics (CFD) is crucial for automotive design,
requiring the analysis of large 3D point clouds to study how vehicle geometry
affects pressure fields and drag forces. However, existing deep learning
approaches for CFD struggle with the computational complexity of processing
high-resolution 3D data. We propose Factorized Implicit Global Convolution
(FIGConv), a novel architecture that efficiently solves CFD problems for very
large 3D meshes with arbitrary input and output geometries. FIGConv achieves
quadratic complexity $O(N^2)$, a significant improvement over existing 3D
neural CFD models that require cubic complexity $O(N^3)$. Our approach combines
Factorized Implicit Grids to approximate high-resolution domains, efficient
global convolutions through 2D reparameterization, and a U-shaped architecture
for effective information gathering and integration. We validate our approach
on the industry-standard Ahmed body <span class="highlight-title">dataset</span> and the large-scale DrivAerNet
<span class="highlight-title">dataset</span>. In DrivAerNet, our model achieves an $R^2$ value of 0.95 for drag
prediction, outperforming the previous state-of-the-art by a significant
margin. This represents a 40% improvement in relative mean squared error and a
70% improvement in absolute mean squared error over previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Human Mesh Recovery with Vision-Language Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongyang Xu, Buzhen Huang, Chengfang Zhang, Ziliang Feng, Yangang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery can be approached using either regression-based or
optimization-based methods. Regression models achieve high pose accuracy but
struggle with model-to-image alignment due to the lack of explicit 2D-3D
correspondences. In contrast, optimization-based methods align 3D models to 2D
observations but are prone to local minima and depth ambiguity. In this work,
we leverage large vision-language models (VLMs) to generate interactive body
part descriptions, which serve as implicit constraints to enhance 3D perception
and limit the optimization space. Specifically, we formulate monocular human
mesh recovery as a distribution adaptation task by integrating both 2D
observations and language descriptions. To bridge the gap between text and 3D
pose signals, we first train a text encoder and a pose VQ-VAE, aligning texts
to body poses in a shared latent space using contrastive learning.
Subsequently, we employ a <span class="highlight-title">diffusion</span>-based framework to refine the initial
parameters guided by gradients derived from both 2D observations and text
descriptions. Finally, the model can produce poses with accurate 3D perception
and image consistency. Experimental results on multiple benchmarks validate its
effectiveness. The code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Articulate-Anything: Automatic Modeling of Articulated Objects via a
  Vision-Language Foundation Model <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13882v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13882v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Le, Jason Xie, William Liang, Hung-Ju Wang, Yue Yang, Yecheng Jason Ma, Kyle Vedder, Arjun Krishna, Dinesh Jayaraman, Eric Eaton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive 3D simulated objects are crucial in AR/VR, animations, and
robotics, driving immersive experiences and advanced automation. However,
creating these articulated objects requires extensive human effort and
expertise, limiting their broader applications. To overcome this challenge, we
present Articulate-Anything, a system that automates the articulation of
diverse, complex objects from many input modalities, including text, images,
and videos. Articulate-Anything leverages vision-language models (VLMs) to
generate code that can be compiled into an interactable digital twin for use in
standard 3D simulators. Our system exploits existing 3D asset <span class="highlight-title">dataset</span>s via a
mesh retrieval mechanism, along with an actor-critic system that iteratively
proposes, evaluates, and refines solutions for articulating the objects,
self-correcting errors to achieve a robust outcome. Qualitative evaluations
demonstrate Articulate-Anything's capability to articulate complex and even
ambiguous object affordances by leveraging rich grounded inputs. In extensive
quantitative experiments on the standard PartNet-Mobility <span class="highlight-title">dataset</span>,
Articulate-Anything substantially outperforms prior work, increasing the
success rate from 8.7-11.6% to 75% and setting a new bar for state-of-the-art
performance. We further showcase the utility of our system by generating 3D
assets from in-the-wild video inputs, which are then used to train robotic
policies for fine-grained manipulation tasks in simulation that go beyond basic
pick and place. These policies are then transferred to a real robotic system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project website and open-source code:
  https://articulate-anything.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeblurDiff: Real-World Image Deblurring with Generative <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingshun Kong, Jiawei Zhang, Dongqing Zou, Jimmy Ren, Xiaohe Wu, Jiangxin Dong, Jinshan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Diffusion</span> models have achieved significant progress in image generation. The
<span class="highlight-title">pre-train</span>ed Stable <span class="highlight-title">Diffusion</span> (SD) models are helpful for image deblurring by
providing clear image priors. However, directly using a blurry image or
pre-deblurred one as a conditional control for SD will either hinder accurate
structure extraction or make the results overly dependent on the deblurring
network. In this work, we propose a Latent Kernel Prediction Network (LKPN) to
achieve robust real-world image deblurring. Specifically, we co-train the LKPN
in latent space with conditional <span class="highlight-title">diffusion</span>. The LKPN learns a spatially variant
kernel to guide the restoration of sharp images in the latent space. By
applying element-wise adaptive convolution (EAC), the learned kernel is
utilized to adaptively process the input feature, effectively preserving the
structural information of the input. This process thereby more effectively
guides the generative process of Stable <span class="highlight-title">Diffusion</span> (SD), enhancing both the
deblurring efficacy and the quality of detail reconstruction. Moreover, the
results at each <span class="highlight-title">diffusion</span> step are utilized to iteratively estimate the kernels
in LKPN to better restore the sharp latent by EAC. This iterative refinement
enhances the accuracy and robustness of the deblurring process. Extensive
experimental results demonstrate that the proposed method outperforms
state-of-the-art image deblurring methods on both benchmark and real-world
images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Control for Guidance in <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushagra Pandey, Farrin Marouf Sofian, Felix Draxler, Theofanis Karaletsos, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Diffusion</span> models exhibit excellent sample quality, but existing guidance
methods often require additional model training or are limited to specific
tasks. We revisit guidance in <span class="highlight-title">diffusion</span> models from the perspective of
variational inference and control, introducing <span class="highlight-title">Diffusion</span> Trajectory Matching
(DTM) that enables guiding <span class="highlight-title">pretrain</span>ed <span class="highlight-title">diffusion</span> trajectories to satisfy a
terminal cost. DTM unifies a broad class of guidance methods and enables novel
instantiations. We introduce a new method within this framework that achieves
state-of-the-art results on several linear and (blind) non-linear inverse
problems without requiring additional model training or modifications. For
instance, in ImageNet non-linear deblurring, our model achieves an FID score of
34.31, significantly improving over the best <span class="highlight-title">pretrain</span>ed-method baseline (FID
78.07). We will make the code available in a future update.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages in main text. Total of 20 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-05T00:00:00Z">2025-02-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INST-Sculpt: Interactive Stroke-based Neural SDF Sculpting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fizza Rubab, Yiying Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in implicit neural representations have made them a popular
choice for modeling 3D geometry, achieving impressive results in tasks such as
shape representation, reconstruction, and learning priors. However, directly
editing these representations poses challenges due to the complex relationship
between model weights and surface regions they influence. Among such editing
tools, sculpting, which allows users to interactively carve or extrude the
surface, is a valuable editing operation to the graphics and modeling
community. While traditional mesh-based tools like ZBrush facilitate fast and
intuitive edits, a comparable toolkit for sculpting neural SDFs is currently
lacking. We introduce a framework that enables interactive surface sculpting
edits directly on neural implicit representations. Unlike previous works
limited to spot edits, our approach allows users to perform stroke-based
modifications on the fly, ensuring intuitive shape manipulation without
switching representations. By employing tubular neighborhoods to sample strokes
and custom brush profiles, we achieve smooth deformations along user-defined
curves, providing precise control over the sculpting process. Our method
demonstrates that intricate and versatile edits can be made while preserving
the smooth nature of implicit representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Effective <span class="highlight-title">NeRF</span>s and SDFs Representations with 3D Generative
  Adversarial Networks for 3D Object Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Yang, Yibo Liu, Guile Wu, Tongtong Cao, Yuan Ren, Yang Liu, Bingbing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a solution for 3D object generation of ICCV 2023 OmniObject3D
Challenge. In recent years, 3D object generation has made great process and
achieved promising results, but it remains a challenging task due to the
difficulty of generating complex, textured, and high-fidelity results. To
resolve this problem, we study learning effective <span class="highlight-title">NeRF</span>s and SDFs
representations with 3D Generative Adversarial Networks (GANs) for 3D object
generation. Specifically, inspired by recent works, we use the efficient
geometry-aware 3D GANs as the backbone incorporating with label embedding and
color mapping, which enables to train the model on different taxonomies
simultaneously. Then, through a decoder, we aggregate the resulting features to
generate Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) based representations for <span class="highlight-title">rendering</span>
high-fidelity synthetic images. Meanwhile, we optimize Signed Distance
Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we
observe that this model can be effectively trained with only a few images of
each object from a variety of classes, instead of using a great number of
images per object or training one model per class. With this pipeline, we can
optimize an effective model for 3D object generation. This solution is among
the top 3 in the ICCV 2023 OmniObject3D Challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowSDF: Flow Matching for Medical Image Segmentation Using Distance
  Transforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lea Bogensperger, Dominik Narnhofer, Alexander Falk, Konrad Schindler, Thomas Pock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation plays an important role in accurately identifying
and isolating regions of interest within medical images. Generative approaches
are particularly effective in modeling the statistical properties of
segmentation masks that are closely related to the respective structures. In
this work we introduce FlowSDF, an image-guided conditional flow matching
framework, designed to represent the signed distance function (SDF), and, in
turn, to represent an implicit distribution of segmentation masks. The
advantage of leveraging the SDF is a more natural distortion when compared to
that of binary masks. Through the learning of a vector field associated with
the probability path of conditional SDF distributions, our framework enables
accurate sampling of segmentation masks and the computation of relevant
statistical measures. This probabilistic approach also facilitates the
generation of uncertainty maps represented by the variance, thereby supporting
enhanced robustness in prediction and further analysis. We qualitatively and
quantitatively illustrate competitive performance of the proposed method on a
public nuclei and gland segmentation data set, highlighting its utility in
medical image segmentation applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INST-Sculpt: Interactive Stroke-based Neural SDF Sculpting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fizza Rubab, Yiying Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in implicit neural representations have made them a popular
choice for modeling 3D geometry, achieving impressive results in tasks such as
shape representation, reconstruction, and learning priors. However, directly
editing these representations poses challenges due to the complex relationship
between model weights and surface regions they influence. Among such editing
tools, sculpting, which allows users to interactively carve or extrude the
surface, is a valuable editing operation to the graphics and modeling
community. While traditional mesh-based tools like ZBrush facilitate fast and
intuitive edits, a comparable toolkit for sculpting neural SDFs is currently
lacking. We introduce a framework that enables interactive surface sculpting
edits directly on neural implicit representations. Unlike previous works
limited to spot edits, our approach allows users to perform stroke-based
modifications on the fly, ensuring intuitive shape manipulation without
switching representations. By employing tubular neighborhoods to sample strokes
and custom brush profiles, we achieve smooth deformations along user-defined
curves, providing precise control over the sculpting process. Our method
demonstrates that intricate and versatile edits can be made while preserving
the smooth nature of implicit representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Effective <span class="highlight-title">NeRF</span>s and SDFs Representations with 3D Generative
  Adversarial Networks for 3D Object Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Yang, Yibo Liu, Guile Wu, Tongtong Cao, Yuan Ren, Yang Liu, Bingbing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a solution for 3D object generation of ICCV 2023 OmniObject3D
Challenge. In recent years, 3D object generation has made great process and
achieved promising results, but it remains a challenging task due to the
difficulty of generating complex, textured, and high-fidelity results. To
resolve this problem, we study learning effective <span class="highlight-title">NeRF</span>s and SDFs
representations with 3D Generative Adversarial Networks (GANs) for 3D object
generation. Specifically, inspired by recent works, we use the efficient
geometry-aware 3D GANs as the backbone incorporating with label embedding and
color mapping, which enables to train the model on different taxonomies
simultaneously. Then, through a decoder, we aggregate the resulting features to
generate Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) based representations for <span class="highlight-title">rendering</span>
high-fidelity synthetic images. Meanwhile, we optimize Signed Distance
Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we
observe that this model can be effectively trained with only a few images of
each object from a variety of classes, instead of using a great number of
images per object or training one model per class. With this pipeline, we can
optimize an effective model for 3D object generation. This solution is among
the top 3 in the ICCV 2023 OmniObject3D Challenge.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VistaFlow: Photorealistic Volumetric Reconstruction with Dynamic
  Resolution Management via Q-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayram Palamadai, William Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VistaFlow, a scalable three-dimensional imaging technique
capable of reconstructing fully interactive 3D volumetric images from a set of
2D photographs. Our model synthesizes novel viewpoints through a differentiable
<span class="highlight-title">rendering</span> system capable of dynamic resolution management on photorealistic 3D
scenes. We achieve this through the introduction of QuiQ, a novel intermediate
video controller trained through Q-learning to maintain a consistently high
framerate by adjusting render resolution with millisecond precision. Notably,
VistaFlow runs natively on integrated CPU graphics, making it viable for mobile
and entry-level devices while still delivering high-performance <span class="highlight-title">rendering</span>.
VistaFlow bypasses Neural Radiance Fields (<span class="highlight-title">NeRF</span>s), using the PlenOctree data
structure to render complex light interactions such as reflection and
subsurface scattering with minimal hardware requirements. Our model is capable
of outperforming state-of-the-art methods with novel view synthesis at a
resolution of 1080p at over 100 frames per second on consumer hardware. By
tailoring render quality to the capabilities of each device, VistaFlow has the
potential to improve the efficiency and accessibility of photorealistic 3D
scene <span class="highlight-title">rendering</span> across a wide spectrum of hardware, from high-end workstations
to inexpensive microcontrollers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Effective <span class="highlight-title">NeRF</span>s and SDFs Representations with 3D Generative
  Adversarial Networks for 3D Object Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Yang, Yibo Liu, Guile Wu, Tongtong Cao, Yuan Ren, Yang Liu, Bingbing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a solution for 3D object generation of ICCV 2023 OmniObject3D
Challenge. In recent years, 3D object generation has made great process and
achieved promising results, but it remains a challenging task due to the
difficulty of generating complex, textured, and high-fidelity results. To
resolve this problem, we study learning effective <span class="highlight-title">NeRF</span>s and SDFs
representations with 3D Generative Adversarial Networks (GANs) for 3D object
generation. Specifically, inspired by recent works, we use the efficient
geometry-aware 3D GANs as the backbone incorporating with label embedding and
color mapping, which enables to train the model on different taxonomies
simultaneously. Then, through a decoder, we aggregate the resulting features to
generate Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) based representations for <span class="highlight-title">rendering</span>
high-fidelity synthetic images. Meanwhile, we optimize Signed Distance
Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we
observe that this model can be effectively trained with only a few images of
each object from a variety of classes, instead of using a great number of
images per object or training one model per class. With this pipeline, we can
optimize an effective model for 3D object generation. This solution is among
the top 3 in the ICCV 2023 OmniObject3D Challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Ming Cheng, Zirui Wang, Victor Adrian Prisacariu, Tristan Braud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We leverage 3D Gaussian Splatting (3DGS) as a scene representation and
propose a novel test-time camera pose refinement (CPR) framework, GS-CPR. This
framework enhances the localization accuracy of state-of-the-art absolute pose
regression and scene coordinate regression methods. The 3DGS model renders
high-quality synthetic images and depth maps to facilitate the establishment of
2D-3D correspondences. GS-CPR obviates the need for training feature extractors
or descriptors by operating directly on RGB images, utilizing the 3D foundation
model, MASt3R, for precise 2D matching. To improve the robustness of our model
in challenging outdoor environments, we incorporate an exposure-adaptive module
within the 3DGS framework. Consequently, GS-CPR enables efficient one-shot pose
refinement given a single RGB query and a coarse initial pose estimation. Our
proposed approach surpasses leading <span class="highlight-title">NeRF</span>-based optimization methods in both
accuracy and runtime across indoor and outdoor visual localization benchmarks,
achieving new state-of-the-art accuracy on two indoor <span class="highlight-title">dataset</span>s. The project
page is available at https://gsloc.active.vision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR2025. During the ICLR review process, we changed the
  name of our framework from GSLoc to GS-CPR (Camera Pose Refinement) according
  to the comments of reviewers. The project page is available at
  https://gsloc.active.vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhe Jiang, Chun Gu, Yurui Chen, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR
simulation, offering valuable simulated point cloud data from novel viewpoints
to aid in autonomous driving systems. However, existing LiDAR NVS methods
typically rely on neural radiance fields (<span class="highlight-title">NeRF</span>) as their 3D representation,
which incurs significant computational costs in both training and <span class="highlight-title">rendering</span>.
Moreover, <span class="highlight-title">NeRF</span> and its variants are designed for symmetrical scenes, making
them ill-suited for driving scenarios. To address these challenges, we propose
GS-LiDAR, a novel framework for generating realistic LiDAR point clouds with
panoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with
periodic vibration properties, allowing for precise geometric reconstruction of
both static and dynamic elements in driving scenarios. We further introduce a
novel panoramic <span class="highlight-title">rendering</span> technique with explicit ray-splat intersection,
guided by panoramic LiDAR supervision. By incorporating intensity and ray-drop
spherical harmonic (SH) coefficients into the Gaussian primitives, we enhance
the realism of the rendered point clouds. Extensive experiments on KITTI-360
and nuScenes demonstrate the superiority of our method in terms of quantitative
metrics, visual quality, as well as training and <span class="highlight-title">rendering</span> efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A study of why we need to reassess full reference image quality
  assessment with medical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19097v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19097v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Breger, Ander Biguri, Malena Sabaté Landman, Ian Selby, Nicole Amberg, Elisabeth Brunner, Janek Gröhl, Sepideh Hatamikia, Clemens Karner, Lipeng Ning, Sören Dittmer, Michael Roberts, AIX-COVNET Collaboration, Carola-Bibiane Schönlieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality assessment (IQA) is indispensable in clinical practice to
ensure high standards, as well as in the development stage of machine learning
algorithms that operate on medical images. The popular full reference (FR) IQA
measures PSNR and SSIM are known and tested for working successfully in many
natural imaging tasks, but discrepancies in medical scenarios have been
reported in the literature, highlighting the gap between development and actual
clinical application. Such inconsistencies are not surprising, as medical
images have very different properties than natural images, and PSNR and SSIM
have neither been targeted nor properly tested for medical images. This may
cause unforeseen problems in clinical applications due to wrong judgment of
novel methods. This paper provides a structured and comprehensive <span class="highlight-title">overview</span> of
examples where PSNR and SSIM prove to be unsuitable for the assessment of novel
algorithms using different kinds of medical images, including real-world MRI,
CT, OCT, X-Ray, digital pathology and photoacoustic imaging data. Therefore,
improvement is urgently needed in particular in this era of AI to increase
reliability and explainability in machine learning for medical imaging and
beyond. Lastly, we will provide ideas for future research as well as suggesting
guidelines for the usage of FR-IQA measures applied to medical images.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-04T00:00:00Z">2025-02-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel
  <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nissim Maruani, Wang Yifan, Matthew Fisher, Pierre Alliez, Mathieu Desbrun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes ShapeShifter, a new 3D generative model that learns to
synthesize shape variations based on a single reference model. While generative
methods for 3D objects have recently attracted much attention, current
techniques often lack geometric details and/or require long training times and
large resources. Our approach remedies these issues by combining sparse voxel
grids and point, normal, and color sampling within a multiscale neural
architecture that can be trained efficiently and in parallel. We show that our
resulting variations better capture the fine details of their original input
and can handle more general types of surfaces than previous SDF-based methods.
Moreover, we offer interactive generation of 3D shape variants, allowing more
human control in the design loop if needed.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D articulated objects modeling has long been a challenging problem, since it
requires to capture both accurate surface geometries and semantically
meaningful and spatially precise structures, parts, and joints. Existing
methods heavily depend on training data from a limited set of handcrafted
articulated object categories (e.g., cabinets and drawers), which restricts
their ability to model a wide range of articulated objects in an
open-vocabulary context. To address these limitations, we propose Articulate
Anymesh, an automated framework that is able to convert any rigid 3D mesh into
its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our
framework utilizes advanced Vision-Language Models and visual <span class="highlight-title">prompt</span>ing
techniques to extract semantic information, allowing for both the segmentation
of object parts and the construction of functional joints. Our experiments show
that Articulate Anymesh can generate large-scale, high-quality 3D articulated
objects, including tools, toys, mechanical devices, and vehicles, significantly
expanding the coverage of existing 3D articulated object <span class="highlight-title">dataset</span>s.
Additionally, we show that these generated assets can facilitate the
acquisition of new articulated object manipulation skills in simulation, which
can then be transferred to a real robotic system. Our Github website is
https://articulate-anymesh.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with
  Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Tao, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a neural radiance field (<span class="highlight-title">NeRF</span>) based large-scale reconstruction
system that fuses lidar and vision data to generate high-quality
reconstructions that are geometrically accurate and capture photorealistic
texture. Our system adopts the state-of-the-art <span class="highlight-title">NeRF</span> representation to
additionally incorporate lidar. Adding lidar data adds strong geometric
constraints on the depth and surface normals, which is particularly useful when
modelling uniform texture surfaces which contain ambiguous visual
reconstruction cues. Furthermore, we estimate the epistemic uncertainty of the
reconstruction as the spatial variance of each point location in the radiance
field given the sensor observations from camera and lidar. This enables the
identification of areas that are reliably reconstructed by each sensor
modality, allowing the map to be filtered according to the estimated
uncertainty. Our system can also exploit the trajectory produced by a real-time
pose-graph lidar SLAM system during online mapping to bootstrap a
(post-processed) Structure-from-Motion (SfM) reconstruction procedure reducing
SfM training time by up to 70%. It also helps to properly constrain the overall
metric scale which is essential for the lidar depth loss. The
globally-consistent trajectory can then be divided into submaps using Spectral
Clustering to group sets of co-visible images together. This submapping
approach is more suitable for visual reconstruction than distance-based
partitioning. Each submap is filtered according to point-wise uncertainty
estimates and merged to obtain the final large-scale 3D reconstruction. We
demonstrate the reconstruction system using a multi-camera, lidar sensor suite
in experiments involving both robot-mounted and handheld scanning. Our test
<span class="highlight-title">dataset</span>s cover a total area of more than 20,000 square metres, including
multiple university buildings and an aerial <span class="highlight-title">survey</span> of a multi-storey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>webpage: https://dynamic.robots.ox.ac.uk/projects/silvr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by
  Continual Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generation of a virtual digital avatar is a crucial research topic in the
field of computer vision. Many existing works utilize Neural Radiance Fields
(<span class="highlight-title">NeRF</span>) to address this issue and have achieved impressive results. However,
previous works assume the images of the training person are available and fixed
while the appearances and poses of a subject could constantly change and
increase in real-world scenarios. How to update the human avatar but also
maintain the ability to render the old appearance of the person is a practical
challenge. One trivial solution is to combine the existing virtual avatar
models based on <span class="highlight-title">NeRF</span> with continual learning methods. However, there are some
critical issues in this approach: learning new appearances and poses can cause
the model to forget past information, which in turn leads to a degradation in
the <span class="highlight-title">rendering</span> quality of past appearances, especially color bleeding issues,
and incorrect human body poses. In this work, we propose a maintainable avatar
(MaintaAvatar) based on neural radiance fields by continual learning, which
resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose
Distillation Module. Overall, our model requires only limited data collection
to quickly fine-tune the model while avoiding catastrophic forgetting, thus
achieving a maintainable virtual avatar. The experimental results validate the
effectiveness of our MaintaAvatar model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025. 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel
  View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable volumetric <span class="highlight-title">rendering</span>-based methods made significant progress
in novel view synthesis. On one hand, innovative methods have replaced the
Neural Radiance Fields (<span class="highlight-title">NeRF</span>) network with locally parameterized structures,
enabling high-quality <span class="highlight-title">rendering</span>s in a reasonable time. On the other hand,
approaches have used differentiable splatting instead of <span class="highlight-title">NeRF</span>'s ray casting to
optimize radiance fields rapidly using Gaussian kernels, allowing for fine
adaptation to the scene. However, differentiable ray casting of irregularly
spaced kernels has been scarcely explored, while splatting, despite enabling
fast <span class="highlight-title">rendering</span> times, is susceptible to clearly visible artifacts.
  Our work closes this gap by providing a physically consistent formulation of
the emitted radiance c and density {\sigma}, decomposed with Gaussian functions
associated with Spherical Gaussians/Harmonics for all-frequency colorimetric
representation. We also introduce a method enabling differentiable ray casting
of irregularly distributed Gaussians using an algorithm that integrates
radiance fields slab by slab and leverages a BVH structure. This allows our
approach to finely adapt to the scene while avoiding splatting artifacts. As a
result, we achieve superior <span class="highlight-title">rendering</span> quality compared to the state-of-the-art
while maintaining reasonable training times and achieving inference speeds of
25 FPS on the Blender <span class="highlight-title">dataset</span>. Project page with videos and code:
https://raygauss.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page with videos and code: https://raygauss.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-03T00:00:00Z">2025-02-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and
  Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent success in legged robot locomotion is attributed to the integration of
reinforcement learning and physical simulators. However, these policies often
encounter challenges when deployed in real-world environments due to
sim-to-real gaps, as simulators typically fail to replicate visual realism and
complex real-world geometry. Moreover, the lack of realistic visual <span class="highlight-title">rendering</span>
limits the ability of these policies to support high-level tasks requiring
RGB-based perception like ego-centric navigation. This paper presents a
Real-to-Sim-to-Real framework that generates photorealistic and physically
interactive "digital twin" simulation environments for visual navigation and
locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based
scene reconstruction from multi-view images and integrates these environments
into simulations that support ego-centric visual perception and mesh-based
physical interactions. To demonstrate its effectiveness, we train a
reinforcement learning policy within the simulator to perform a visual
goal-tracking task. Extensive experiments show that our framework achieves
RGB-only sim-to-real policy transfer. Additionally, our framework facilitates
the rapid adaptation of robot policies with effective exploration capability in
complex new environments, highlighting its potential for applications in
households and factories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://vr-robo.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Radiant Foam: Real-Time Differentiable Ray Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on differentiable scene representations is consistently moving
towards more efficient, real-time models. Recently, this has led to the
popularization of splatting methods, which eschew the traditional ray-based
<span class="highlight-title">rendering</span> of radiance fields in favor of rasterization. This has yielded a
significant improvement in <span class="highlight-title">rendering</span> speeds due to the efficiency of
rasterization algorithms and hardware, but has come at a cost: the
approximations that make rasterization efficient also make implementation of
light transport phenomena like reflection and refraction much more difficult.
We propose a novel scene representation which avoids these approximations, but
keeps the efficiency and reconstruction quality of splatting by leveraging a
decades-old efficient volumetric mesh ray tracing algorithm which has been
largely overlooked in recent computer vision research. The resulting model,
which we name Radiant Foam, achieves <span class="highlight-title">rendering</span> speed and quality comparable to
Gaussian Splatting, without the constraints of rasterization. Unlike ray traced
Gaussian models that use hardware ray tracing acceleration, our method requires
no special hardware or APIs beyond the standard features of a programmable GPU.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BD-Diff: Generative <span class="highlight-title">Diffusion</span> Model for Image Deblurring on Unknown
  Domains with Blur-Decoupled Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Cheng, Wei-Ting Chen, Xi Lu, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative <span class="highlight-title">diffusion</span> models trained on large-scale <span class="highlight-title">dataset</span>s have achieved
remarkable progress in image synthesis. In favor of their ability to supplement
missing details and generate aesthetically pleasing contents, recent works have
applied them to image deblurring tasks via training an adapter on blurry-sharp
image pairs to provide structural conditions for restoration. However,
acquiring substantial amounts of realistic paired data is challenging and
costly in real-world scenarios. On the other hand, relying solely on synthetic
data often results in overfitting, leading to unsatisfactory performance when
confronted with unseen blur patterns. To tackle this issue, we propose BD-Diff,
a generative-<span class="highlight-title">diffusion</span>-based model designed to enhance deblurring performance
on unknown domains by decoupling structural features and blur patterns through
joint training on three specially designed tasks. We employ two Q-Formers as
structural representations and blur patterns extractors separately. The
features extracted by them will be used for the supervised deblurring task on
synthetic data and the unsupervised blur-transfer task by leveraging unpaired
blurred images from the target domain simultaneously. Furthermore, we introduce
a reconstruction task to make the structural features and blur patterns
complementary. This blur-decoupled learning process enhances the generalization
capabilities of BD-Diff when encountering unknown domain blur patterns.
Experiments on real-world <span class="highlight-title">dataset</span>s demonstrate that BD-Diff outperforms
existing state-of-the-art methods in blur removal and structural preservation
in various challenging scenarios. The codes will be released in
https://github.com/donahowe/BD-Diff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We propose BD-Diff to integrate generative diffusion model into
  unpaired deblurring tasks</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-02T00:00:00Z">2025-02-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Xuanyu Yi, Haohan Weng, Qingshan Xu, Xiaokang Wei, Xianghui Yang, Chunchao Guo, Long Chen, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Triangle meshes are fundamental to 3D applications, enabling efficient
modification and rasterization while maintaining compatibility with standard
<span class="highlight-title">rendering</span> pipelines. However, current automatic mesh generation methods
typically rely on intermediate representations that lack the continuous surface
quality inherent to meshes. Converting these representations into meshes
produces dense, suboptimal outputs. Although recent autoregressive approaches
demonstrate promise in directly modeling mesh vertices and faces, they are
constrained by the limitation in face count, scalability, and structural
fidelity. To address these challenges, we propose Nautilus, a locality-aware
autoencoder for artist-like mesh generation that leverages the local properties
of manifold meshes to achieve structural fidelity and efficient representation.
Our approach introduces a novel tokenization algorithm that preserves face
proximity relationships and compresses sequence length through locally shared
vertices and edges, enabling the generation of meshes with an unprecedented
scale of up to 5,000 faces. Furthermore, we develop a Dual-stream Point
Conditioner that provides multi-scale geometric guidance, ensuring global
consistency and local structural fidelity by capturing fine-grained geometric
features. Extensive experiments demonstrate that Nautilus significantly
outperforms state-of-the-art methods in both fidelity and scalability. The
project page is at https://nautilusmeshgen.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://nautilusmeshgen.github.io, Tencent Hunyuan, 14
  pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Continual Human Motion in Diverse 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02061v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02061v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aymen Mir, Xavier Puig, Angjoo Kanazawa, Gerard Pons-Moll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method to synthesize animator guided human motion across 3D
scenes. Given a set of sparse (3 or 4) joint locations (such as the location of
a person's hand and two feet) and a seed motion sequence in a 3D scene, our
method generates a plausible motion sequence starting from the seed motion
while satisfying the constraints imposed by the provided keypoints. We
decompose the continual motion synthesis problem into walking along paths and
transitioning in and out of the actions specified by the keypoints, which
enables long generation of motions that satisfy scene constraints without
explicitly incorporating scene information. Our method is trained only using
scene agnostic mocap data. As a result, our approach is deployable across 3D
scenes with various geometries. For achieving plausible continual motion
synthesis without drift, our key contribution is to generate motion in a
goal-centric canonical coordinate frame where the next immediate target is
situated at the origin. Our model can generate long sequences of diverse
actions such as grabbing, sitting and leaning chained together in arbitrary
order, demonstrated on scenes of varying geometry: HPS, Replica, Matterport,
ScanNet and scenes represented using <span class="highlight-title">NeRF</span>s. Several experiments demonstrate
that our method outperforms existing methods that navigate paths in 3D scenes.
For more results we urge the reader to watch our supplementary video available
at: https://www.youtube.com/watch?v=0wZgsdyCT4A&t=1s
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Webpage: https://virtualhumans.mpi-inf.mpg.de/origin_2/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DWT<span class="highlight-title">NeRF</span>: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet
  Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Nguyen, Blark Runfa Li, Truong Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>) has achieved superior performance in novel view
synthesis and 3D scene representation, but its practical applications are
hindered by slow convergence and reliance on dense training views. To this end,
we present DWT<span class="highlight-title">NeRF</span>, a unified framework based on Instant-NGP's fast-training
hash encoding. It is coupled with regularization terms designed for few-shot
<span class="highlight-title">NeRF</span>, which operates on sparse training views. Our DWT<span class="highlight-title">NeRF</span> additionally
includes a novel Discrete Wavelet loss that allows explicit prioritization of
low frequencies directly in the training objective, reducing few-shot <span class="highlight-title">NeRF</span>'s
overfitting on high frequencies in earlier training stages. We also introduce a
model-based approach, based on multi-head attention, that is compatible with
INGP, which are sensitive to architectural changes. On the 3-shot LLFF
benchmark, DWT<span class="highlight-title">NeRF</span> outperforms Vanilla INGP by 15.07% in PSNR, 24.45% in SSIM
and 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot
approaches for fast-converging implicit representations like INGP or 3DGS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CULTURE3D: Cultural Landmarks and Terrain <span class="highlight-title">Dataset</span> for 3D Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zheng, Steve Zhang, Weizhe Lin, Aaron Zhang, Walterio W. Mayol-Cuevas, Junxiao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a large-scale fine-grained <span class="highlight-title">dataset</span> using
high-resolution images captured from locations worldwide. Compared to existing
<span class="highlight-title">dataset</span>s, our <span class="highlight-title">dataset</span> offers a significantly larger size and includes a higher
level of detail, making it uniquely suited for fine-grained 3D applications.
Notably, our <span class="highlight-title">dataset</span> is built using drone-captured aerial imagery, which
provides a more accurate perspective for capturing real-world site layouts and
architectural structures. By reconstructing environments with these detailed
images, our <span class="highlight-title">dataset</span> supports applications such as the <span class="highlight-title">COLMAP</span> format for
Gaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible
with widely-used techniques including SLAM, Multi-View Stereo, and Neural
Radiance Fields (<span class="highlight-title">NeRF</span>), enabling accurate 3D reconstructions and point clouds.
This makes it a benchmark for reconstruction and segmentation tasks. The
<span class="highlight-title">dataset</span> enables seamless integration with multi-modal data, supporting a range
of 3D applications, from architectural reconstruction to virtual tourism. Its
flexibility promotes innovation, facilitating breakthroughs in 3D modeling and
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality Assessment for AI Generated Images with Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Wang, Huiyu Duan, Guangtao Zhai, Xiongkuo Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence Generated Content (AIGC) has grown rapidly in recent
years, among which AI-based image generation has gained widespread attention
due to its efficient and imaginative image creation ability. However,
AI-generated Images (AIGIs) may not satisfy human preferences due to their
unique distortions, which highlights the necessity to understand and evaluate
human preferences for AIGIs. To this end, in this paper, we first establish a
novel Image Quality Assessment (IQA) database for AIGIs, termed AIGCIQA2023+,
which provides human visual preference scores and detailed preference
explanations from three perspectives including quality, authenticity, and
correspondence. Then, based on the constructed AIGCIQA2023+ database, this
paper presents a MINT-IQA model to evaluate and explain human preferences for
AIGIs from Multi-perspectives with INstruction Tuning. Specifically, the
MINT-IQA model first learn and evaluate human preferences for AI-generated
Images from multi-perspectives, then via the vision-language instruction tuning
strategy, MINT-IQA attains powerful understanding and explanation ability for
human visual preference on AIGIs, which can be used for feedback to further
improve the assessment capabilities. Extensive experimental results demonstrate
that the proposed MINT-IQA model achieves state-of-the-art performance in
understanding and evaluating human visual preferences for AIGIs, and the
proposed model also achieves competing results on traditional IQA tasks
compared with state-of-the-art IQA models. The AIGCIQA2023+ database and
MINT-IQA model are available at: https://github.com/IntMeGroup/MINT-IQA.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-01T00:00:00Z">2025-02-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shape from Semantics: 3D Shape Generation from Multi-View Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangchen Li, Caoliwen Wang, Yuqi Zhou, Bailin Deng, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose ``Shape from Semantics'', which is able to create 3D models whose
geometry and appearance match given semantics when observed from different
views. Traditional ``Shape from X'' tasks usually use visual input (e.g., RGB
images or depth maps) to reconstruct geometry, imposing strict constraints that
limit creative explorations. As applications, works like Shadow Art and Wire
Art often struggle to grasp the embedded semantics of their design through
direct observation and rely heavily on specific setups for proper display. To
address these limitations, our framework uses semantics as input, greatly
expanding the design space to create objects that integrate multiple semantic
elements and are easily discernible by observers. Considering that this task
requires a rich imagination, we adopt various generative models and
structure-to-detail pipelines. Specifically, we adopt multi-semantics Score
Distillation Sampling (SDS) to distill 3D geometry and appearance from 2D
<span class="highlight-title">diffusion</span> models, ensuring that the initial shape is consistent with the
semantic input. We then use image restoration and video generation models to
add more details as supervision. Finally, we introduce neural signed distance
field (SDF) representation to achieve detailed shape reconstruction. Our
framework generates meshes with complex details, well-structured geometry,
coherent textures, and smooth transitions, resulting in visually appealing and
eye-catching designs. Project page: https://shapefromsemantics.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://shapefromsemantics.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shape from Semantics: 3D Shape Generation from Multi-View Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangchen Li, Caoliwen Wang, Yuqi Zhou, Bailin Deng, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose ``Shape from Semantics'', which is able to create 3D models whose
geometry and appearance match given semantics when observed from different
views. Traditional ``Shape from X'' tasks usually use visual input (e.g., RGB
images or depth maps) to reconstruct geometry, imposing strict constraints that
limit creative explorations. As applications, works like Shadow Art and Wire
Art often struggle to grasp the embedded semantics of their design through
direct observation and rely heavily on specific setups for proper display. To
address these limitations, our framework uses semantics as input, greatly
expanding the design space to create objects that integrate multiple semantic
elements and are easily discernible by observers. Considering that this task
requires a rich imagination, we adopt various generative models and
structure-to-detail pipelines. Specifically, we adopt multi-semantics Score
Distillation Sampling (SDS) to distill 3D geometry and appearance from 2D
<span class="highlight-title">diffusion</span> models, ensuring that the initial shape is consistent with the
semantic input. We then use image restoration and video generation models to
add more details as supervision. Finally, we introduce neural signed distance
field (SDF) representation to achieve detailed shape reconstruction. Our
framework generates meshes with complex details, well-structured geometry,
coherent textures, and smooth transitions, resulting in visually appealing and
eye-catching designs. Project page: https://shapefromsemantics.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://shapefromsemantics.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XYScanNet: An Interpretable State Space Model for Perceptual Image
  Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhou Liu, Chengkai Liu, Jiacong Xu, Peng Jiang, Mi Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep state-space models (SSMs), like recent Mamba architectures, are emerging
as a promising alternative to CNN and <span class="highlight-title">Transformer</span> networks. Existing
Mamba-based restoration methods process the visual data by leveraging a
flatten-and-scan strategy that converts image patches into a 1D sequence before
scanning. However, this scanning paradigm ignores local pixel dependencies and
introduces spatial misalignment by positioning distant pixels incorrectly
adjacent, which reduces local noise-awareness and degrades image sharpness in
low-level vision tasks. To overcome these issues, we propose a novel
slice-and-scan strategy that alternates scanning along intra- and inter-slices.
We further design a new Vision State Space Module (VSSM) for image deblurring,
and tackle the inefficiency challenges of the current Mamba-based vision
module. Building upon this, we develop XYScanNet, an SSM architecture
integrated with a lightweight feature fusion module for enhanced image
deblurring. XYScanNet, maintains competitive distortion metrics and
significantly improves perceptual performance. Experimental results show that
XYScanNet enhances KID by $17\%$ compared to the nearest competitor. Our code
will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-31T00:00:00Z">2025-01-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Sevastopolsky, Philip-William Grassal, Simon Giebenhain, ShahRukh Athar, Luisa Verdoliva, Matthias Niessner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current advances in human head modeling allow the generation of
plausible-looking 3D head models via neural representations, such as <span class="highlight-title">NeRF</span>s and
SDFs. Nevertheless, constructing complete high-fidelity head models with
explicitly controlled animation remains an issue. Furthermore, completing the
head geometry based on a partial observation, e.g., coming from a depth sensor,
while preserving a high level of detail is often problematic for the existing
methods. We introduce a generative model for detailed 3D head meshes on top of
an articulated 3DMM, simultaneously allowing explicit animation and high-detail
preservation. Our method is trained in two stages. First, we register a
parametric head model with vertex displacements to each mesh of the recently
introduced NPHM <span class="highlight-title">dataset</span> of accurate 3D head scans. The estimated displacements
are baked into a hand-crafted UV layout. Second, we train a StyleGAN model to
generalize over the UV maps of displacements, which we later refer to as
HeadCraft. The decomposition of the parametric model and high-quality vertex
displacements allows us to animate the model and modify the regions
semantically. We demonstrate the results of unconditional sampling, fitting to
a scan and editing. The project page is available at
https://seva100.github.io/headcraft.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2nd version includes updated method and results. Project page:
  https://seva100.github.io/headcraft. Video: https://youtu.be/uBeBT2f1CL0. 24
  pages, 21 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RaySplats: Ray Tracing based Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krzysztof Byrski, Marcin Mazur, Jacek Tabor, Tadeusz Dziarmaga, Marcin Kądziołka, Dawid Baran, Przemysław Spurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) is a process that enables the direct creation of
3D objects from 2D images. This representation offers numerous advantages,
including rapid training and <span class="highlight-title">rendering</span>. However, a significant limitation of
3DGS is the challenge of incorporating light and shadow reflections, primarily
due to the utilization of rasterization rather than ray tracing for <span class="highlight-title">rendering</span>.
This paper introduces RaySplats, a model that employs ray-tracing based
Gaussian Splatting. Rather than utilizing the projection of Gaussians, our
method employs a ray-tracing mechanism, operating directly on Gaussian
primitives represented by confidence ellipses with RGB colors. In practice, we
compute the intersection between ellipses and rays to construct ray-tracing
algorithms, facilitating the incorporation of meshes with Gaussian Splatting
models and the addition of lights, shadows, and other related effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoutao Sun, Xukun Shen, Yong Hu, Yuyou Zhong, Xueyang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since hands are the primary interface in daily interactions, modeling
high-quality digital human hands and <span class="highlight-title">rendering</span> realistic images is a critical
research problem. Furthermore, considering the requirements of interactive and
<span class="highlight-title">rendering</span> applications, it is essential to achieve real-time <span class="highlight-title">rendering</span> and
driveability of the digital model without compromising <span class="highlight-title">rendering</span> quality. Thus,
we propose Jointly 3D Gaussian Hand (JGHand), a novel joint-driven 3D Gaussian
Splatting (3DGS)-based hand representation that renders high-fidelity hand
images in real-time for various poses and characters. Distinct from existing
articulated neural <span class="highlight-title">rendering</span> techniques, we introduce a differentiable process
for spatial transformations based on 3D key points. This process supports
deformations from the canonical template to a mesh with arbitrary bone lengths
and poses. Additionally, we propose a real-time shadow simulation method based
on per-pixel depth to simulate self-occlusion shadows caused by finger
movements. Finally, we embed the hand prior and propose an animatable 3DGS
representation of the hand driven solely by 3D key points. We validate the
effectiveness of each component of our approach through comprehensive ablation
studies. Experimental results on public <span class="highlight-title">dataset</span>s demonstrate that JGHand
achieves real-time <span class="highlight-title">rendering</span> speeds with enhanced quality, surpassing
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Sevastopolsky, Philip-William Grassal, Simon Giebenhain, ShahRukh Athar, Luisa Verdoliva, Matthias Niessner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current advances in human head modeling allow the generation of
plausible-looking 3D head models via neural representations, such as <span class="highlight-title">NeRF</span>s and
SDFs. Nevertheless, constructing complete high-fidelity head models with
explicitly controlled animation remains an issue. Furthermore, completing the
head geometry based on a partial observation, e.g., coming from a depth sensor,
while preserving a high level of detail is often problematic for the existing
methods. We introduce a generative model for detailed 3D head meshes on top of
an articulated 3DMM, simultaneously allowing explicit animation and high-detail
preservation. Our method is trained in two stages. First, we register a
parametric head model with vertex displacements to each mesh of the recently
introduced NPHM <span class="highlight-title">dataset</span> of accurate 3D head scans. The estimated displacements
are baked into a hand-crafted UV layout. Second, we train a StyleGAN model to
generalize over the UV maps of displacements, which we later refer to as
HeadCraft. The decomposition of the parametric model and high-quality vertex
displacements allows us to animate the model and modify the regions
semantically. We demonstrate the results of unconditional sampling, fitting to
a scan and editing. The project page is available at
https://seva100.github.io/headcraft.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2nd version includes updated method and results. Project page:
  https://seva100.github.io/headcraft. Video: https://youtu.be/uBeBT2f1CL0. 24
  pages, 21 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Sevastopolsky, Philip-William Grassal, Simon Giebenhain, ShahRukh Athar, Luisa Verdoliva, Matthias Niessner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current advances in human head modeling allow the generation of
plausible-looking 3D head models via neural representations, such as <span class="highlight-title">NeRF</span>s and
SDFs. Nevertheless, constructing complete high-fidelity head models with
explicitly controlled animation remains an issue. Furthermore, completing the
head geometry based on a partial observation, e.g., coming from a depth sensor,
while preserving a high level of detail is often problematic for the existing
methods. We introduce a generative model for detailed 3D head meshes on top of
an articulated 3DMM, simultaneously allowing explicit animation and high-detail
preservation. Our method is trained in two stages. First, we register a
parametric head model with vertex displacements to each mesh of the recently
introduced NPHM <span class="highlight-title">dataset</span> of accurate 3D head scans. The estimated displacements
are baked into a hand-crafted UV layout. Second, we train a StyleGAN model to
generalize over the UV maps of displacements, which we later refer to as
HeadCraft. The decomposition of the parametric model and high-quality vertex
displacements allows us to animate the model and modify the regions
semantically. We demonstrate the results of unconditional sampling, fitting to
a scan and editing. The project page is available at
https://seva100.github.io/headcraft.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2nd version includes updated method and results. Project page:
  https://seva100.github.io/headcraft. Video: https://youtu.be/uBeBT2f1CL0. 24
  pages, 21 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Nowak, Wojciech Jarosz, Peter Chin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing a 3D scene from images is challenging due to the different
ways light interacts with surfaces depending on the viewer's position and the
surface's material. In classical computer graphics, materials can be classified
as diffuse or specular, interacting with light differently. The standard 3D
Gaussian Splatting model struggles to represent view-dependent content, since
it cannot differentiate an object within the scene from the light interacting
with its specular surfaces, which produce highlights or reflections. In this
paper, we propose to extend the 3D Gaussian Splatting model by introducing an
additional symmetric matrix to enhance the opacity representation of each 3D
Gaussian. This improvement allows certain Gaussians to be suppressed based on
the viewer's perspective, resulting in a more accurate representation of
view-dependent reflections and specular highlights without compromising the
scene's integrity. By allowing the opacity to be view dependent, our enhanced
model achieves state-of-the-art performance on Mip-<span class="highlight-title">Nerf</span>, Tanks&Temples, Deep
Blending, and <span class="highlight-title">Nerf</span>-Synthetic <span class="highlight-title">dataset</span>s without a significant loss in <span class="highlight-title">rendering</span>
speed, achieving >60FPS, and only incurring a minimal increase in memory used.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Frame Blind Manifold Deconvolution for Rotating Synthetic Aperture
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dao Lin, Jian Zhang, Martin Benning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rotating synthetic aperture (RSA) imaging system captures images of the
target scene at different rotation angles by rotating a rectangular aperture.
Deblurring acquired RSA images plays a critical role in reconstructing a latent
sharp image underlying the scene. In the past decade, the emergence of blind
convolution technology has revolutionised this field by its ability to model
complex features from acquired images. Most of the existing methods attempt to
solve the above ill-posed inverse problem through maximising a posterior.
  Despite this progress, researchers have paid limited attention to exploring
low-dimensional manifold structures of the latent image within a
high-dimensional ambient-space. Here, we propose a novel method to process RSA
images using manifold fitting and penalisation in the content of multi-frame
blind convolution. We develop fast algorithms for implementing the proposed
procedure. Simulation studies demonstrate that manifold-based deconvolution can
outperform conventional deconvolution algorithms in the sense that it can
generate a sharper estimate of the latent image in terms of estimating pixel
intensities and preserving structural details.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DarkIR: Robust Low-Light Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Feijoo, Juan C. Benito, Alvaro Garcia, Marcos V. Conde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photography during night or in dark conditions typically suffers from noise,
low light and blurring issues due to the dim environment and the common use of
long exposure. Although Deblurring and Low-light Image Enhancement (LLIE) are
related under these conditions, most approaches in image restoration solve
these tasks separately. In this paper, we present an efficient and robust
neural network for multi-task low-light image restoration. Instead of following
the current tendency of <span class="highlight-title">Transformer</span>-based models, we propose new attention
mechanisms to enhance the receptive field of efficient CNNs. Our method reduces
the computational costs in terms of parameters and MAC operations compared to
previous methods. Our model, DarkIR, achieves new state-of-the-art results on
the popular LOLBlur, LOLv2 and Real-LOLBlur <span class="highlight-title">dataset</span>s, being able to generalize
on real-world night and dark images. Code and models at
https://github.com/cidautai/DarkIR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-30T00:00:00Z">2025-01-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROSA: Reconstructing Object Shape and Appearance Textures by Adaptive
  Detail Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Kaltheuner, Patrick Stotko, Reinhard Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing an object's shape and appearance in terms of a mesh textured
by a spatially-varying bidirectional reflectance distribution function (SVBRDF)
from a limited set of images captured under collocated light is an ill-posed
problem. Previous state-of-the-art approaches either aim to reconstruct the
appearance directly on the geometry or additionally use texture normals as part
of the appearance features. However, this requires detailed but inefficiently
large meshes, that would have to be simplified in a post-processing step, or
suffers from well-known limitations of normal maps such as missing shadows or
incorrect silhouettes. Another limiting factor is the fixed and typically low
resolution of the texture estimation resulting in loss of important surface
details. To overcome these problems, we present ROSA, an inverse <span class="highlight-title">rendering</span>
method that directly optimizes mesh geometry with spatially adaptive mesh
resolution solely based on the image data. In particular, we refine the mesh
and locally condition the surface smoothness based on the estimated normal
texture and mesh curvature. In addition, we enable the reconstruction of fine
appearance details in high-resolution textures through a pioneering tile-based
method that operates on a single <span class="highlight-title">pre-train</span>ed decoder network but is not limited
by the network output resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surface Defect Identification using Bayesian Filtering on a 3D Mesh 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Dalle Vedove, Matteo Bonetto, Edoardo Lamon, Luigi Palopoli, Matteo Saveriano, Daniele Fontanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a CAD-based approach for automated surface defect
detection. We leverage the a-priori knowledge embedded in a CAD model and
integrate it with point cloud data acquired from commercially available stereo
and depth cameras. The proposed method first transforms the CAD model into a
high-density polygonal mesh, where each vertex represents a state variable in
3D space. Subsequently, a weighted least squares algorithm is employed to
iteratively estimate the state of the scanned workpiece based on the captured
point cloud measurements. This framework offers the potential to incorporate
information from diverse sensors into the CAD domain, facilitating a more
comprehensive analysis. Preliminary results demonstrate promising performance,
with the algorithm achieving convergence to a sub-millimeter standard deviation
in the region of interest using only approximately 50 point cloud samples. This
highlights the potential of utilising commercially available stereo cameras for
high-precision quality control applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at IMEKO2024 World Congress, Hamburg, Germany, 26-29
  October 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">Transformer</span> for High Resolution Image Motion Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amanturdieva Akmaral, Muhammad Hamza Zafar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive study and improvement of the Restormer
architecture for high-resolution image motion deblurring. We introduce
architectural modifications that reduce model complexity by 18.4% while
maintaining or improving performance through optimized attention mechanisms.
Our enhanced training pipeline incorporates additional transformations
including color jitter, Gaussian blur, and perspective transforms to improve
model robustness as well as a new frequency loss term. Extensive experiments on
the RealBlur-R, RealBlur-J, and Ultra-High-Definition Motion blurred (UHDM)
<span class="highlight-title">dataset</span>s demonstrate the effectiveness of our approach. The improved
architecture shows better convergence behavior and reduced training time while
maintaining competitive performance across challenging scenarios. We also
provide detailed ablation studies analyzing the impact of our modifications on
model behavior and performance. Our results suggest that thoughtful
architectural simplification combined with enhanced training strategies can
yield more efficient yet equally capable models for motion deblurring tasks.
Code and Data Available at: https://github.com/hamzafer/image-deblurring
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 18 figures Submitted as a preprint, no prior
  journal/conference submission</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-29T00:00:00Z">2025-01-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pressure Field Reconstruction with SIREN: A Mesh-Free Approach for Image
  Velocimetry in Complex Noisy Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renato F. Miotto, William R. Wolf, Fernando Zigunov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a novel approach for pressure field reconstruction from
image velocimetry data using SIREN (Sinusoidal Representation Network),
emphasizing its effectiveness as an implicit neural representation in noisy
environments and its mesh-free nature. While we briefly assess two recently
proposed methods - one-shot matrix-omnidirectional integration (OS-MODI) and
Green's function integral (GFI) - the primary focus is on the advantages of the
SIREN approach. The OS-MODI technique performs well in noise-free conditions
and with structured meshes but struggles when applied to unstructured meshes
with high aspect ratio. Similarly, the GFI method encounters difficulties due
to singularities inherent from the Newtonian kernel. In contrast, the proposed
SIREN approach is a mesh-free method that directly reconstructs the pressure
field, bypassing the need for an intrinsic grid connectivity and, hence,
avoiding the challenges associated with ill-conditioned cells and unstructured
meshes. This provides a distinct advantage over traditional mesh-based methods.
Moreover, it is shown that changes in the architecture of the SIREN can be used
to filter out inherent noise from velocimetry data. This work positions SIREN
as a robust and versatile solution for pressure reconstruction, particularly in
noisy environments characterized by the absence of mesh structure, opening new
avenues for innovative applications in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for
  Geometrically Accurate and Artifact-Reduced Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miriam Jäger, Markus Hillemann, Boris Jutzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene
reconstruction using 3D Gaussians. However, neither the centers nor surfaces of
the Gaussians are accurately aligned to the object surface, complicating their
direct use in point cloud and mesh reconstruction. Additionally, 3DGS typically
produces floater artifacts, increasing the number of Gaussians and storage
requirements. To address these issues, we present FeatureGS, which incorporates
an additional geometric loss term based on an eigenvalue-derived 3D shape
feature into the optimization process of 3DGS. The goal is to improve geometric
accuracy and enhance properties of planar surfaces with reduced structural
entropy in local 3D neighborhoods.We present four alternative formulations for
the geometric loss term based on 'planarity' of Gaussians, as well as
'planarity', 'omnivariance', and 'eigenentropy' of Gaussian neighborhoods. We
provide quantitative and qualitative evaluations on 15 scenes of the DTU
benchmark <span class="highlight-title">dataset</span> focusing on following key aspects: Geometric accuracy and
artifact-reduction, measured by the Chamfer distance, and memory efficiency,
evaluated by the total number of Gaussians. Additionally, <span class="highlight-title">rendering</span> quality is
monitored by Peak Signal-to-Noise Ratio. FeatureGS achieves a 30 % improvement
in geometric accuracy, reduces the number of Gaussians by 90 %, and suppresses
floater artifacts, while maintaining comparable photometric <span class="highlight-title">rendering</span> quality.
The geometric loss with 'planarity' from Gaussians provides the highest
geometric accuracy, while 'omnivariance' in Gaussian neighborhoods reduces
floater artifacts and number of Gaussians the most. This makes FeatureGS a
strong method for geometrically accurate, artifact-reduced and memory-efficient
3D scene reconstruction, enabling the direct use of Gaussian centers for
geometric representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-28T00:00:00Z">2025-01-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating CrowdSplat: Perceived Level of Detail for Gaussian Crowds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Sun, Yinghan Xu, John Dingliana, Carol O'Sullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and realistic crowd <span class="highlight-title">rendering</span> is an important element of many
real-time graphics applications such as Virtual Reality (VR) and games. To this
end, Levels of Detail (LOD) avatar representations such as polygonal meshes,
image-based impostors, and point clouds have been proposed and evaluated. More
recently, 3D Gaussian Splatting has been explored as a potential method for
real-time crowd <span class="highlight-title">rendering</span>. In this paper, we present a two-alternative forced
choice (2AFC) experiment that aims to determine the perceived quality of 3D
Gaussian avatars. Three factors were explored: Motion, LOD (i.e., #Gaussians),
and the avatar height in Pixels (corresponding to the viewing distance).
Participants viewed pairs of animated 3D Gaussian avatars and were tasked with
choosing the most detailed one. Our findings can inform the optimization of LOD
strategies in Gaussian-based crowd <span class="highlight-title">rendering</span>, thereby helping to achieve
efficient <span class="highlight-title">rendering</span> while maintaining visual quality in real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PokeFlex: A Real-World <span class="highlight-title">Dataset</span> of Volumetric Deformable Objects for
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Obrist, Miguel Zamora, Hehui Zheng, Ronan Hinchet, Firat Ozdemir, Juan Zarate, Robert K. Katzschmann, Stelian Coros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven methods have shown great potential in solving challenging
manipulation tasks; however, their application in the domain of deformable
objects has been constrained, in part, by the lack of data. To address this
lack, we propose PokeFlex, a <span class="highlight-title">dataset</span> featuring real-world multimodal data that
is paired and annotated. The modalities include 3D textured meshes, point
clouds, RGB images, and depth maps. Such data can be leveraged for several
downstream tasks, such as online 3D mesh reconstruction, and it can potentially
enable underexplored applications such as the real-world deployment of
traditional control methods based on mesh simulations. To deal with the
challenges posed by real-world 3D mesh reconstruction, we leverage a
professional volumetric capture system that allows complete 360{\deg}
reconstruction. PokeFlex consists of 18 deformable objects with varying
stiffness and shapes. Deformations are generated by dropping objects onto a
flat surface or by poking the objects with a robot arm. Interaction wrenches
and contact locations are also reported for the latter case. Using different
data modalities, we demonstrated a use case for our <span class="highlight-title">dataset</span> training models
that, given the novelty of the multimodal nature of Pokeflex, constitute the
state-of-the-art in multi-object online template-based mesh reconstruction from
multimodal data, to the best of our knowledge. We refer the reader to our
website ( https://pokeflex-<span class="highlight-title">dataset</span>.github.io/ ) for further demos and examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Acquiring Submillimeter-Accurate Multi-Task Vision <span class="highlight-title">Dataset</span>s for
  Computer-Assisted Orthopedic Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Most, Jonas Hein, Frédéric Giraud, Nicola A. Cavalcanti, Lukas Zingg, Baptiste Brument, Nino Louman, Fabio Carrillo, Philipp Fürnstahl, Lilian Calvet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in computer vision, particularly in optical image-based 3D
reconstruction and feature matching, enable applications like marker-less
surgical navigation and digitization of surgery. However, their development is
hindered by a lack of suitable <span class="highlight-title">dataset</span>s with 3D ground truth. This work
explores an approach to generating realistic and accurate ex vivo <span class="highlight-title">dataset</span>s
tailored for 3D reconstruction and feature matching in open orthopedic surgery.
A set of posed images and an accurately registered ground truth surface mesh of
the scene are required to develop vision-based 3D reconstruction and matching
methods suitable for surgery. We propose a framework consisting of three core
steps and compare different methods for each step: 3D scanning, calibration of
viewpoints for a set of high-resolution RGB images, and an optical-based method
for scene registration. We evaluate each step of this framework on an ex vivo
scoliosis surgery using a pig spine, conducted under real operating room
conditions. A mean 3D Euclidean error of 0.35 mm is achieved with respect to
the 3D ground truth. The proposed method results in submillimeter accurate 3D
ground truths and surgical images with a spatial resolution of 0.1 mm. This
opens the door to acquiring future surgical <span class="highlight-title">dataset</span>s for high-precision
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures. Submitted to the 16th International Conference
  on Information Processing in Computer-Assisted Interventions (IPCAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinPrim: Linear Primitives for Differentiable Volumetric <span class="highlight-title">Rendering</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas von Lützow, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric <span class="highlight-title">rendering</span> has become central to modern novel view synthesis
methods, which use differentiable <span class="highlight-title">rendering</span> to optimize 3D scene
representations directly from observed views. While many recent works build on
<span class="highlight-title">NeRF</span> or 3D Gaussians, we explore an alternative volumetric scene
representation. More specifically, we introduce two new scene representations
based on linear primitives-octahedra and tetrahedra-both of which define
homogeneous volumes bounded by triangular faces. This formulation aligns
naturally with standard mesh-based tools, minimizing overhead for downstream
applications. To optimize these primitives, we present a differentiable
rasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based
optimization while maintaining realtime <span class="highlight-title">rendering</span> capabilities. Through
experiments on real-world <span class="highlight-title">dataset</span>s, we demonstrate comparable performance to
state-of-the-art volumetric methods while requiring fewer primitives to achieve
similar reconstruction fidelity. Our findings provide insights into the
geometry of volumetric <span class="highlight-title">rendering</span> and suggest that adopting explicit polyhedra
can expand the design space of scene representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://nicolasvonluetzow.github.io/LinPrim ; Project
  video: https://youtu.be/P2yeHwmGaeM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent
  <span class="highlight-title">Diffusion</span> <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01826v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01826v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Lin, Zhaoxin Fan, Xianjia Wu, Lingyu Xiong, Liang Peng, Xiandong Li, Wenxiong Kang, Songju Lei, Huang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven talking head generation is a critical yet challenging task with
applications in augmented reality and virtual human modeling. While recent
approaches using autoregressive and <span class="highlight-title">diffusion</span>-based models have achieved
notable progress, they often suffer from modality inconsistencies, particularly
misalignment between audio and mesh, leading to reduced motion diversity and
lip-sync accuracy. To address this, we propose GLDiTalker, a novel
speech-driven 3D facial animation model based on a Graph Latent <span class="highlight-title">Diffusion</span>
<span class="highlight-title">Transformer</span>. GLDiTalker resolves modality misalignment by diffusing signals
within a quantized spatiotemporal latent space. It employs a two-stage training
pipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync
accuracy, while the Space-Time Powered Latent <span class="highlight-title">Diffusion</span> Stage enhances motion
diversity. Together, these stages enable GLDiTalker to generate realistic,
temporally stable 3D facial animations. Extensive evaluations on standard
benchmarks demonstrate that GLDiTalker outperforms existing methods, achieving
superior results in both lip-sync accuracy and motion diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmoFace: Emotion-Content Disentangled Speech-Driven 3D Talking Face
  Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Lin, Liang Peng, Xianjia Wu, Jianqiao Hu, Xiandong Li, Wenxiong Kang, Songju Lei, Huang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of increasingly vivid 3D talking face has become a hot topic in
recent years. Currently, most speech-driven works focus on lip synchronisation
but neglect to effectively capture the correlations between emotions and facial
motions. To address this problem, we propose a two-stream network called
EmoFace, which consists of an emotion branch and a content branch. EmoFace
employs a novel Mesh Attention mechanism to analyse and fuse the emotion
features and content features. Particularly, a newly designed spatio-temporal
graph-based convolution, SpiralConv3D, is used in Mesh Attention to learn
potential temporal and spatial feature dependencies between mesh vertices. In
addition, to the best of our knowledge, it is the first time to introduce a new
self-growing training scheme with intermediate supervision to dynamically
adjust the ratio of groundtruth adopted in the 3D face animation task.
Comprehensive quantitative and qualitative evaluations on our high-quality 3D
emotional facial animation <span class="highlight-title">dataset</span>, 3D-RAVDESS ($4.8863\times 10^{-5}$mm for
LVE and $0.9509\times 10^{-5}$mm for EVE), together with the public <span class="highlight-title">dataset</span>
VOCASET ($2.8669\times 10^{-5}$mm for LVE and $0.4664\times 10^{-5}$mm for
EVE), demonstrate that our approach achieves state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinPrim: Linear Primitives for Differentiable Volumetric <span class="highlight-title">Rendering</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas von Lützow, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric <span class="highlight-title">rendering</span> has become central to modern novel view synthesis
methods, which use differentiable <span class="highlight-title">rendering</span> to optimize 3D scene
representations directly from observed views. While many recent works build on
<span class="highlight-title">NeRF</span> or 3D Gaussians, we explore an alternative volumetric scene
representation. More specifically, we introduce two new scene representations
based on linear primitives-octahedra and tetrahedra-both of which define
homogeneous volumes bounded by triangular faces. This formulation aligns
naturally with standard mesh-based tools, minimizing overhead for downstream
applications. To optimize these primitives, we present a differentiable
rasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based
optimization while maintaining realtime <span class="highlight-title">rendering</span> capabilities. Through
experiments on real-world <span class="highlight-title">dataset</span>s, we demonstrate comparable performance to
state-of-the-art volumetric methods while requiring fewer primitives to achieve
similar reconstruction fidelity. Our findings provide insights into the
geometry of volumetric <span class="highlight-title">rendering</span> and suggest that adopting explicit polyhedra
can expand the design space of scene representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://nicolasvonluetzow.github.io/LinPrim ; Project
  video: https://youtu.be/P2yeHwmGaeM</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-27T00:00:00Z">2025-01-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhysAnimator: Physics-Guided Generative Cartoon Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Xie, Yiwei Zhao, Ying Jiang, Chenfanfu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating hand-drawn animation sequences is labor-intensive and demands
professional expertise. We introduce PhysAnimator, a novel approach for
generating physically plausible meanwhile anime-stylized animation from static
anime illustrations. Our method seamlessly integrates physics-based simulations
with data-driven generative models to produce dynamic and visually compelling
animations. To capture the fluidity and exaggeration characteristic of anime,
we perform image-space deformable body simulations on extracted mesh
geometries. We enhance artistic control by introducing customizable energy
strokes and incorporating rigging point support, enabling the creation of
tailored animation effects such as wind interactions. Finally, we extract and
warp sketches from the simulation sequence, generating a texture-agnostic
representation, and employ a sketch-guided video <span class="highlight-title">diffusion</span> model to synthesize
high-quality animation frames. The resulting animations exhibit temporal
consistency and visual plausibility, demonstrating the effectiveness of our
method in creating dynamic anime-style animations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Deep-Learning-based Biomechanical Deformable Image
  Registration with MOREA <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Andreadis, Eduard Ruiz Munné, Thomas H. W. Bäck, Peter A. N. Bosman, Tanja Alderliesten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When choosing a deformable image registration (DIR) approach for images with
large deformations and content mismatch, the realism of found transformations
often needs to be traded off against the required runtime. DIR approaches using
deep learning (DL) techniques have shown remarkable promise in instantly
predicting a transformation. However, on difficult registration problems, the
realism of these transformations can fall short. DIR approaches using
biomechanical, finite element modeling (FEM) techniques can find more realistic
transformations, but tend to require much longer runtimes. This work proposes
the first hybrid approach to combine them, with the aim of getting the best of
both worlds. This hybrid approach, called DL-MOREA, combines a recently
introduced multi-objective DL-based DIR approach which leverages the VoxelMorph
framework, called DL-MODIR, with MOREA, an evolutionary algorithm-based,
multi-objective DIR approach in which a FEM-like biomechanical mesh
transformation model is used. In our proposed hybrid approach, the DL results
are used to smartly initialize MOREA, with the aim of more efficiently
optimizing its mesh transformation model. We empirically compare DL-MOREA
against its components, DL-MODIR and MOREA, on CT scan pairs capturing large
bladder filling differences of 15 cervical cancer patients. While MOREA
requires a median runtime of 45 minutes, DL-MOREA can already find high-quality
transformations after 5 minutes. Compared to the DL-MODIR transformations, the
transformations found by DL-MOREA exhibit far less folding and improve or
preserve the bladder contour distance error.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print for the SPIE Medical Imaging: Image Processing Conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClearSight: Human Vision-Inspired Solutions for Event-Based Motion
  Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaopeng Lin, Yulong Huang, Hongwei Ren, Zunchang Liu, Yue Zhou, Haotian Fu, Bojun Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion deblurring addresses the challenge of image blur caused by camera or
scene movement. Event cameras provide motion information that is encoded in the
asynchronous event streams. To efficiently leverage the temporal information of
event streams, we employ Spiking Neural Networks (SNNs) for motion feature
extraction and Artificial Neural Networks (ANNs) for color information
processing. Due to the non-uniform distribution and inherent redundancy of
event data, existing cross-modal feature fusion methods exhibit certain
limitations. Inspired by the visual attention mechanism in the human visual
system, this study introduces a bioinspired dual-drive hybrid network (BDHNet).
Specifically, the Neuron Configurator Module (NCM) is designed to dynamically
adjusts neuron configurations based on cross-modal features, thereby focusing
the spikes in blurry regions and adapting to varying blurry scenarios
dynamically. Additionally, the Region of Blurry Attention Module (RBAM) is
introduced to generate a blurry mask in an unsupervised manner, effectively
extracting motion clues from the event features and guiding more accurate
cross-modal feature fusion. Extensive subjective and objective evaluations
demonstrate that our method outperforms current state-of-the-art methods on
both synthetic and real-world <span class="highlight-title">dataset</span>s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Cross-Space Total Variation Regularization Model for Color Image
  Restoration with Quaternion Blur Operator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12114v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12114v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhigang Jia, Yuelian Xiang, Meixiang Zhao, Tingting Wu, Michael K. Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cross-channel deblurring problem in color image processing is difficult
to solve due to the complex coupling and structural blurring of color pixels.
Until now, there are few efficient algorithms that can reduce color artifacts
in deblurring process. To solve this challenging problem, we present a novel
cross-space total variation (CSTV) regularization model for color image
deblurring by introducing a quaternion blur operator and a cross-color space
regularization functional. The existence and uniqueness of the solution are
proved and a new L-curve method is proposed to find a balance of regularization
terms on different color spaces. The Euler-Lagrange equation is derived to show
that CSTV has taken into account the coupling of all color channels and the
local smoothing within each color channel. A quaternion operator splitting
method is firstly proposed to enhance the ability of color artifacts reduction
of the CSTV regularization model. This strategy also applies to the well-known
color deblurring models. Numerical experiments on color image databases
illustrate the efficiency and effectiveness of the new model and algorithms.
The color images restored by them successfully maintain the color and spatial
information and are of higher quality in terms of PSNR, SSIM, MSE and CIEde2000
than the restorations of the-state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15pages,14figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-26T00:00:00Z">2025-01-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StochSync: Stochastic <span class="highlight-title">Diffusion</span> Synchronization for Image Generation in
  Arbitrary Spaces <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyeongmin Yeo, Jaihoon Kim, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a zero-shot method for generating images in arbitrary spaces
(e.g., a sphere for 360{\deg} panoramas and a mesh surface for texture) using a
<span class="highlight-title">pretrain</span>ed image <span class="highlight-title">diffusion</span> model. The zero-shot generation of various visual
content using a <span class="highlight-title">pretrain</span>ed image <span class="highlight-title">diffusion</span> model has been explored mainly in
two directions. First, <span class="highlight-title">Diffusion</span> Synchronization-performing reverse <span class="highlight-title">diffusion</span>
processes jointly across different projected spaces while synchronizing them in
the target space-generates high-quality outputs when enough conditioning is
provided, but it struggles in its absence. Second, Score Distillation
Sampling-gradually updating the target space data through gradient
descent-results in better coherence but often lacks detail. In this paper, we
reveal for the first time the interconnection between these two methods while
highlighting their differences. To this end, we propose StochSync, a novel
approach that combines the strengths of both, enabling effective performance
with weak conditioning. Our experiments demonstrate that StochSync provides the
best performance in 360{\deg} panorama generation (where image conditioning is
not given), outperforming previous finetuning-based methods, and also delivers
comparable results in 3D mesh texturing (where depth conditioning is provided)
with previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://stochsync.github.io/ (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2C-GAN: Restore-to-Classify Generative Adversarial Networks for Blind
  X-Ray Restoration and COVID-19 Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ahishali, Aysen Degerli, Serkan Kiranyaz, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Restoration of poor quality images with a blended set of artifacts plays a
vital role for a reliable diagnosis. Existing studies have focused on specific
restoration problems such as image deblurring, denoising, and exposure
correction where there is usually a strong assumption on the artifact type and
severity. As a pioneer study in blind X-ray restoration, we propose a joint
model for generic image restoration and classification: Restore-to-Classify
Generative Adversarial Networks (R2C-GANs). Such a jointly optimized model
keeps any disease intact after the restoration. Therefore, this will naturally
lead to a higher diagnosis performance thanks to the improved X-ray image
quality. To accomplish this crucial objective, we define the restoration task
as an Image-to-Image translation problem from poor quality having noisy,
blurry, or over/under-exposed images to high quality image domain. The proposed
R2C-GAN model is able to learn forward and inverse transforms between the two
domains using unpaired training samples. Simultaneously, the joint
classification preserves the disease label during restoration. Moreover, the
R2C-GANs are equipped with operational layers/neurons reducing the network
depth and further boosting both restoration and classification performances.
The proposed joint model is extensively evaluated over the QaTa-COV19 <span class="highlight-title">dataset</span>
for Coronavirus Disease 2019 (COVID-19) classification. The proposed
restoration approach achieves over 90% F1-Score which is significantly higher
than the performance of any deep model. Moreover, in the qualitative analysis,
the restoration performance of R2C-GANs is approved by a group of medical
doctors. We share the software implementation at
https://github.com/meteahishali/R2C-GAN.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-25T00:00:00Z">2025-01-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Mesh Watermark Removal Attack and Mitigation: A Novel Perspective of
  Function Space <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12059v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12059v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing<span class="highlight-author">yu Zhu</span>, Guanhui Ye, Chengdong Dong, Xiapu Luo, Shiyao Zhang, Xuetao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mesh watermark embeds secret messages in 3D meshes and decodes the message
from watermarked meshes for ownership verification. Current watermarking
methods directly hide secret messages in vertex and face sets of meshes.
However, mesh is a discrete representation that uses vertex and face sets to
describe a continuous signal, which can be discretized in other discrete
representations with different vertex and face sets. This raises the question
of whether the watermark can still be verified on the different discrete
representations of the watermarked mesh. We conduct this research in an
attack-then-defense manner by proposing a novel function space mesh watermark
removal attack FuncEvade and then mitigating it through function space mesh
watermarking FuncMark. In detail, FuncEvade generates a different discrete
representation of a watermarked mesh by extracting it from the signed distance
function of the watermarked mesh. We observe that the generated mesh can evade
ALL previous watermarking methods. FuncMark mitigates FuncEvade by watermarking
signed distance function through message-guided deformation. Such deformation
can survive isosurfacing and thus be inherited by the extracted meshes for
further watermark decoding. Extensive experiments demonstrate that FuncEvade
achieves 100% evasion rate among all previous watermarking methods while
achieving only 0.3% evasion rate on FuncMark. Besides, our FuncMark performs
similarly on other metrics compared to state-of-the-art mesh watermarking
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-24T00:00:00Z">2025-01-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relightable Full-Body Gaussian Codec Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaofei Wang, Tomas Simon, Igor Santesteban, Timur Bagautdinov, Junxuan Li, Vasu Agrawal, Fabian Prada, Shoou-I Yu, Pace Nalbone, Matt Gramlich, Roman Lubachersky, Chenglei Wu, Javier Romero, Jason Saragih, Michael Zollhoefer, Andreas Geiger, Siyu Tang, Shunsuke Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Relightable Full-Body Gaussian Codec Avatars, a new approach for
modeling relightable full-body avatars with fine-grained details including face
and hands. The unique challenge for relighting full-body avatars lies in the
large deformations caused by body articulation and the resulting impact on
appearance caused by light transport. Changes in body pose can dramatically
change the orientation of body surfaces with respect to lights, resulting in
both local appearance changes due to changes in local light transport
functions, as well as non-local changes due to occlusion between body parts. To
address this, we decompose the light transport into local and non-local
effects. Local appearance changes are modeled using learnable zonal harmonics
for diffuse radiance transfer. Unlike spherical harmonics, zonal harmonics are
highly efficient to rotate under articulation. This allows us to learn diffuse
radiance transfer in a local coordinate frame, which disentangles the local
radiance transfer from the articulation of the body. To account for non-local
appearance changes, we introduce a shadow network that predicts shadows given
precomputed incoming irradiance on a base mesh. This facilitates the learning
of non-local shadowing between the body parts. Finally, we use a deferred
shading approach to model specular radiance transfer and better capture
reflections and highlights such as eye glints. We demonstrate that our approach
successfully models both the local and non-local light transport required for
relightable full-body avatars, with a superior generalization ability under
novel illumination conditions and unseen poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures. Project page:
  https://neuralbodies.github.io/RFGCA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhyDeformer: High-Quality Non-Rigid Garment Registration with
  Physics-Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10455v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10455v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Yu, Frederic Cordier, Hyewon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PhyDeformer, a new deformation method for high-quality garment
mesh registration. It operates in two phases: In the first phase, a garment
grading is performed to achieve a coarse 3D alignment between the mesh template
and the target mesh, accounting for proportional scaling and fit (e.g. length,
size). Then, the graded mesh is refined to align with the fine-grained details
of the 3D target through an optimization coupled with the Jacobian-based
deformation framework. Both quantitative and qualitative evaluations on
synthetic and real garments highlight the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human
  Pose and Talking Head Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Liu, Shidang Xu, Jing Guo, Dingbin Wang, Zairan Wang, Xianfeng Tan, Xiaoli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating talking avatar driven by audio remains a significant challenge.
Existing methods typically require high computational costs and often lack
sufficient facial detail and realism, making them unsuitable for applications
that demand high real-time performance and visual quality. Additionally, while
some methods can synchronize lip movement, they still face issues with
consistency between facial expressions and upper body movement, particularly
during silent periods. In this paper, we introduce SyncAnimation, the first
<span class="highlight-title">NeRF</span>-based method that achieves audio-driven, stable, and real-time generation
of speaking avatar by combining generalized audio-to-pose matching and
audio-to-expression synchronization. By integrating AudioPose Syncer and
AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression
generation, progressively producing audio-synchronized upper body, head, and
lip shapes. Furthermore, the High-Synchronization Human Renderer ensures
seamless integration of the head and upper body, and achieves audio-sync lip.
The project page can be found at https://syncanimation.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-<span class="highlight-title">NeRF</span>: Multimodal-Guided 3D Multi-Style Transfer of Neural Radiance
  Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13607v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13607v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijiang Yang, Zhongwei Qiu, Chang Xu, Dongmei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D style transfer aims to generate stylized views of 3D scenes with specified
styles, which requires high-quality generating and keeping multi-view
consistency. Existing methods still suffer the challenges of high-quality
stylization with texture details and stylization with multimodal guidance. In
this paper, we reveal that the common training method of stylization with <span class="highlight-title">NeRF</span>,
which generates stylized multi-view supervision by 2D style transfer models,
causes the same object in supervision to show various states (color tone,
details, etc.) in different views, leading <span class="highlight-title">NeRF</span> to tend to smooth the texture
details, further resulting in low-quality <span class="highlight-title">rendering</span> for 3D multi-style
transfer. To tackle these problems, we propose a novel Multimodal-guided 3D
Multi-style transfer of <span class="highlight-title">NeRF</span>, termed MM-<span class="highlight-title">NeRF</span>. First, MM-<span class="highlight-title">NeRF</span> projects
multimodal guidance into a unified space to keep the multimodal styles
consistency and extracts multimodal features to guide the 3D stylization.
Second, a novel multi-head learning scheme is proposed to relieve the
difficulty of learning multi-style transfer, and a multi-view style consistent
loss is proposed to track the inconsistency of multi-view supervision data.
Finally, a novel incremental learning mechanism is proposed to generalize
MM-<span class="highlight-title">NeRF</span> to any new style with small costs. Extensive experiments on several
real-world <span class="highlight-title">dataset</span>s show that MM-<span class="highlight-title">NeRF</span> achieves high-quality 3D multi-style
stylization with multimodal guidance, and keeps multi-view consistency and
style consistency between multimodal guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in: IEEE Transactions on Visualization and Computer
  Graphics</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDI: Blind Image Restoration Fidelity Evaluation based on Consistency
  with Degraded Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojun Tang, Jingru Wang, Guangwei Huang, Guannan Chen, Rui Zheng, Lian Huai, Yuyu Liu, Xingqun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Blind Image Restoration (BIR) methods, based on
Generative Adversarial Networks and <span class="highlight-title">Diffusion</span> Models, have significantly
improved visual quality. However, they present significant challenges for Image
Quality Assessment (IQA), as the existing Full-Reference IQA methods often rate
images with high perceptual quality poorly. In this paper, we reassess the
Solution Non-Uniqueness and Degradation Indeterminacy issues of BIR, and
propose constructing a specific BIR IQA system. In stead of directly comparing
a restored image with a reference image, the BIR IQA evaluates fidelity by
calculating the Consistency with Degraded Image (CDI). Specifically, we propose
a wavelet domain Reference Guided CDI algorithm, which can acquire the
consistency with a degraded image for various types without requiring knowledge
of degradation parameters. The supported degradation types include down
sampling, blur, noise, JPEG and complex combined degradations etc. In addition,
we propose a Reference Agnostic CDI, enabling BIR fidelity evaluation without
reference images. Finally, in order to validate the rationality of CDI, we
create a new Degraded Images Switch Display Comparison <span class="highlight-title">Dataset</span> (DISDCD) for
subjective evaluation of BIR fidelity. Experiments conducted on DISDCD verify
that CDI is markedly superior to common Full Reference IQA methods for BIR
fidelity evaluation. The source code and the DISDCD <span class="highlight-title">dataset</span> will be publicly
available shortly.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exposure Bracketing Is All You Need For A High-Quality Image <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00766v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00766v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilu Zhang, Shuohao Zhang, Renlong Wu, Zifei Yan, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is highly desired but challenging to acquire high-quality photos with
clear content in low-light environments. Although multi-image processing
methods (using burst, dual-exposure, or multi-exposure images) have made
significant progress in addressing this issue, they typically focus on specific
restoration or enhancement problems, and do not fully explore the potential of
utilizing multiple images. Motivated by the fact that multi-exposure images are
complementary in denoising, deblurring, high dynamic range imaging, and
super-resolution, we propose to utilize exposure bracketing photography to get
a high-quality image by combining these tasks in this work. Due to the
difficulty in collecting real-world pairs, we suggest a solution that first
<span class="highlight-title">pre-train</span>s the model with synthetic paired data and then adapts it to
real-world unlabeled images. In particular, a temporally modulated recurrent
network (TMRNet) and <span class="highlight-title">self-supervised</span> adaptation method are proposed. Moreover,
we construct a data simulation pipeline to synthesize pairs and collect
real-world images from 200 nighttime scenarios. Experiments on both <span class="highlight-title">dataset</span>s
show that our method performs favorably against the state-of-the-art
multi-image processing ones. Code and <span class="highlight-title">dataset</span>s are available at
https://github.com/cszhilu1998/BracketIRE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-23T00:00:00Z">2025-01-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an implicit surface reconstruction method with 3D
Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D
reconstruction with intricate details while inheriting the high efficiency and
<span class="highlight-title">rendering</span> quality of 3DGS. The key insight is incorporating an implicit signed
distance field (SDF) within 3D Gaussians to enable them to be aligned and
jointly optimized. First, we introduce a differentiable SDF-to-opacity
transformation function that converts SDF values into corresponding Gaussians'
opacities. This function connects the SDF and 3D Gaussians, allowing for
unified optimization and enforcing surface constraints on the 3D Gaussians.
During learning, optimizing the 3D Gaussians provides supervisory signals for
SDF learning, enabling the reconstruction of intricate details. However, this
only provides sparse supervisory signals to the SDF at locations occupied by
Gaussians, which is insufficient for learning a continuous SDF. Then, to
address this limitation, we incorporate volumetric <span class="highlight-title">rendering</span> and align the
rendered geometric attributes (depth, normal) with those derived from 3D
Gaussians. This consistency regularization introduces supervisory signals to
locations not covered by discrete 3D Gaussians, effectively eliminating
redundant surfaces outside the Gaussian sampling range. Our extensive
experimental results demonstrate that our 3DGSR method enables high-quality 3D
surface reconstruction while preserving the efficiency and <span class="highlight-title">rendering</span> quality of
3DGS. Besides, our method competes favorably with leading surface
reconstruction techniques while offering a more efficient learning process and
much better <span class="highlight-title">rendering</span> qualities. The code will be available at
https://github.com/CVMI-Lab/3DGSR.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POLAR-Sim: Augmenting NASA's POLAR <span class="highlight-title">Dataset</span> for Data-Driven Lunar
  Perception and Rover Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Hsun Chen, Peter Negrut, Thomas Liang, Nevindu Batagoda, Harry Zhang, Dan Negrut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NASA's POLAR <span class="highlight-title">dataset</span> contains approximately 2,600 pairs of high dynamic range
stereo photos captured across 13 varied terrain scenarios, including areas with
sparse or dense rock distributions, craters, and rocks of different sizes. The
purpose of these photos is to spur development in robotics, AI-based
perception, and autonomous navigation. Acknowledging a scarcity of lunar images
from around the lunar poles, NASA Ames produced on Earth but in controlled
conditions images that resemble rover operating conditions from these regions
of the Moon. We report on the outcomes of an effort aimed at accomplishing two
tasks. In Task 1, we provided bounding boxes and semantic segmentation
information for all the images in NASA's POLAR <span class="highlight-title">dataset</span>. This effort resulted in
23,000 labels and semantic segmentation annotations pertaining to rocks,
shadows, and craters. In Task 2, we generated the digital twins of the 13
scenarios that have been used to produce all the photos in the POLAR <span class="highlight-title">dataset</span>.
Specifically, for each of these scenarios, we produced individual meshes,
texture information, and material properties associated with the ground and the
rocks in each scenario. This allows anyone with a camera model to synthesize
images associated with any of the 13 scenarios of the POLAR <span class="highlight-title">dataset</span>.
Effectively, one can generate as many semantically labeled synthetic images as
desired -- with different locations and exposure values in the scene, for
different positions of the sun, with or without the presence of active
illumination, etc. The benefit of this work is twofold. Using outcomes of Task
1, one can train and/or test perception algorithms that deal with Moon images.
For Task 2, one can produce as much data as desired to train and test AI
algorithms that are anticipated to work in lunar conditions. All the outcomes
of this work are available in a public repository for unfettered use and
distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Hemodynamic Scalar Fields on Coronary Artery Meshes: A
  Benchmark of Geometric Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Nannini, Julian Suk, Patryk Rygiel, Simone Saitta, Luca Mariani, Riccardo Maragna, Andrea Baggiano, Gianluca Pontone, Jelmer M. Wolterink, Alberto Redaelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coronary artery disease, caused by the narrowing of coronary vessels due to
atherosclerosis, is the leading cause of death worldwide. The diagnostic gold
standard, fractional flow reserve (FFR), measures the trans-stenotic pressure
ratio during maximal vasodilation but is invasive and costly. This has driven
the development of virtual FFR (vFFR) using computational fluid dynamics (CFD)
to simulate coronary flow. Geometric deep learning algorithms have shown
promise for learning features on meshes, including cardiovascular research
applications. This study empirically analyzes various backends for predicting
vFFR fields in coronary arteries as CFD surrogates, comparing six backends for
learning hemodynamics on meshes using CFD solutions as ground truth.
  The study has two parts: i) Using 1,500 synthetic left coronary artery
bifurcations, models were trained to predict pressure-related fields for vFFR
reconstruction, comparing different learning variables. ii) Using 427
patient-specific CFD simulations, experiments were repeated focusing on the
best-performing learning variable from the synthetic <span class="highlight-title">dataset</span>.
  Most backends performed well on the synthetic <span class="highlight-title">dataset</span>, especially when
predicting pressure drop over the manifold. <span class="highlight-title">Transformer</span>-based backends
outperformed others when predicting pressure and vFFR fields and were the only
models achieving strong performance on patient-specific data, excelling in both
average per-point error and vFFR accuracy in stenotic lesions.
  These results suggest geometric deep learning backends can effectively
replace CFD for simple geometries, while <span class="highlight-title">transformer</span>-based networks are
superior for complex, heterogeneous <span class="highlight-title">dataset</span>s. Pressure drop was identified as
the optimal network output for learning pressure-related fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid
  Prototyping in Virtual Reality Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09600v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09600v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Augusto Pinheiro de Sousa, Heiko Hamann, Oliver Deussen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLAM is a foundational technique with broad applications in robotics and
AR/VR. SLAM simulations evaluate new concepts, but testing on
resource-constrained devices, such as VR HMDs, faces challenges: high
computational cost and restricted sensor data access. This work proposes a
sparse framework using mesh geometry projections as features, which improves
efficiency and circumvents direct sensor data access, advancing SLAM research
as we demonstrate in VR and through numerical evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ENPT XR at IEEE VR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeshLRM: Large Reconstruction Model for High-Quality Meshes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, Zexiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MeshLRM, a novel LRM-based approach that can reconstruct a
high-quality mesh from merely four input images in less than one second.
Different from previous large reconstruction models (LRMs) that focus on
<span class="highlight-title">NeRF</span>-based reconstruction, MeshLRM incorporates differentiable mesh extraction
and <span class="highlight-title">rendering</span> within the LRM framework. This allows for end-to-end mesh
reconstruction by fine-tuning a <span class="highlight-title">pre-train</span>ed <span class="highlight-title">NeRF</span> LRM with mesh <span class="highlight-title">rendering</span>.
Moreover, we improve the LRM architecture by simplifying several complex
designs in previous LRMs. MeshLRM's <span class="highlight-title">NeRF</span> initialization is sequentially trained
with low- and high-resolution images; this new LRM training strategy enables
significantly faster convergence and thereby leads to better quality with less
compute. Our approach achieves state-of-the-art mesh reconstruction from
sparse-view inputs and also allows for many downstream applications, including
text-to-3D and single-image-to-3D generation. Project page:
https://sarahweiii.github.io/meshlrm/
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VIGS SLAM: IMU-based Large-Scale 3D Gaussian Splatting SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyuhyeon Pak, Euntai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, map representations based on radiance fields such as 3D Gaussian
Splatting and <span class="highlight-title">NeRF</span>, which excellent for realistic depiction, have attracted
considerable attention, leading to attempts to combine them with SLAM. While
these approaches can build highly realistic maps, large-scale SLAM still
remains a challenge because they require a large number of Gaussian images for
mapping and adjacent images as keyframes for tracking. We propose a novel 3D
Gaussian Splatting SLAM method, VIGS SLAM, that utilizes sensor fusion of RGB-D
and IMU sensors for large-scale indoor environments. To reduce the
computational load of 3DGS-based tracking, we adopt an ICP-based tracking
framework that combines IMU preintegration to provide a good initial guess for
accurate pose estimation. Our proposed method is the first to propose that
Gaussian Splatting-based SLAM can be effectively performed in large-scale
environments by integrating IMU sensor measurements. This proposal not only
enhances the performance of Gaussian Splatting SLAM beyond room-scale scenarios
but also achieves SLAM performance comparable to state-of-the-art methods in
large-scale indoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeshLRM: Large Reconstruction Model for High-Quality Meshes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, Zexiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MeshLRM, a novel LRM-based approach that can reconstruct a
high-quality mesh from merely four input images in less than one second.
Different from previous large reconstruction models (LRMs) that focus on
<span class="highlight-title">NeRF</span>-based reconstruction, MeshLRM incorporates differentiable mesh extraction
and <span class="highlight-title">rendering</span> within the LRM framework. This allows for end-to-end mesh
reconstruction by fine-tuning a <span class="highlight-title">pre-train</span>ed <span class="highlight-title">NeRF</span> LRM with mesh <span class="highlight-title">rendering</span>.
Moreover, we improve the LRM architecture by simplifying several complex
designs in previous LRMs. MeshLRM's <span class="highlight-title">NeRF</span> initialization is sequentially trained
with low- and high-resolution images; this new LRM training strategy enables
significantly faster convergence and thereby leads to better quality with less
compute. Our approach achieves state-of-the-art mesh reconstruction from
sparse-view inputs and also allows for many downstream applications, including
text-to-3D and single-image-to-3D generation. Project page:
https://sarahweiii.github.io/meshlrm/
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianrui Luo, Juewen Peng, Zhongang Cai, Lei Yang, Fan Yang, Zhiguo Cao, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Deblur-Avatar, a novel framework for modeling high-fidelity,
animatable 3D human avatars from motion-blurred monocular video inputs. Motion
blur is prevalent in real-world dynamic video capture, especially due to human
movements in 3D human avatar modeling. Existing methods either (1) assume sharp
image inputs, failing to address the detail loss introduced by motion blur, or
(2) mainly consider blur by camera movements, neglecting the human motion blur
which is more common in animatable avatars. Our proposed approach integrates a
human movement-based motion blur model into 3D Gaussian Splatting (3DGS). By
explicitly modeling human motion trajectories during exposure time, we jointly
optimize the trajectories and 3D Gaussians to reconstruct sharp, high-quality
human avatars. We employ a pose-dependent fusion mechanism to distinguish
moving body regions, optimizing both blurred and sharp areas effectively.
Extensive experiments on synthetic and real-world <span class="highlight-title">dataset</span>s demonstrate that
Deblur-Avatar significantly outperforms existing methods in <span class="highlight-title">rendering</span> quality
and quantitative metrics, producing sharp avatar reconstructions and enabling
real-time <span class="highlight-title">rendering</span> under challenging motion blur conditions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-22T00:00:00Z">2025-01-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hunyuan3D 2.0: Scaling <span class="highlight-title">Diffusion</span> Models for High Resolution Textured 3D
  Assets Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo, Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu, Changrong Hu, Tianyu Huang, Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu, Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing He, Tian Liu, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Jingwei Huang, Chunchao Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for
generating high-resolution textured 3D assets. This system includes two
foundation components: a large-scale shape generation model -- Hunyuan3D-DiT,
and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape
generative model, built on a scalable flow-based <span class="highlight-title">diffusion</span> <span class="highlight-title">transformer</span>, aims to
create geometry that properly aligns with a given condition image, laying a
solid foundation for downstream applications. The texture synthesis model,
benefiting from strong geometric and <span class="highlight-title">diffusion</span> priors, produces high-resolution
and vibrant texture maps for either generated or hand-crafted meshes.
Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production
platform that simplifies the re-creation process of 3D assets. It allows both
professional and amateur users to manipulate or even animate their meshes
efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0
outperforms previous state-of-the-art models, including the open-source models
and closed-source models in geometry details, condition alignment, texture
quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps
in the open-source 3D community for large-scale foundation generative models.
The code and <span class="highlight-title">pre-train</span>ed weights of our models are available at:
https://github.com/Tencent/Hunyuan3D-2
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub link: https://github.com/Tencent/Hunyuan3D-2</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Radiance Fields for the Real World: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhui Xiao, Remi Chierchia, Rodrigo Santa Cruz, Xuesong Li, David Ahmedt-Aristizabal, Olivier Salvado, Clinton Fookes, Leo Lebrat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) have remodeled 3D scene representation since
release. <span class="highlight-title">NeRF</span>s can effectively reconstruct complex 3D scenes from 2D images,
advancing different fields and applications such as scene understanding, 3D
content generation, and robotics. Despite significant research progress, a
thorough <span class="highlight-title">review</span> of recent innovations, applications, and challenges is lacking.
This <span class="highlight-title">survey</span> compiles key theoretical advancements and alternative
representations and investigates emerging challenges. It further explores
applications on reconstruction, highlights <span class="highlight-title">NeRF</span>s' impact on computer vision and
robotics, and <span class="highlight-title">review</span>s essential <span class="highlight-title">dataset</span>s and toolkits. By identifying gaps in
the literature, this <span class="highlight-title">survey</span> discusses open challenges and offers directions for
future research.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DocTTT: Test-Time Training for Handwritten Document Recognition Using
  Meta-Auxiliary Learning <span class="chip">WACV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Gu, Li Gu, Ziqiang Wang, Ching Yee Suen, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent significant advancements in Handwritten Document Recognition
(HDR), the efficient and accurate recognition of text against complex
backgrounds, diverse handwriting styles, and varying document layouts remains a
practical challenge. Moreover, this issue is seldom addressed in academic
research, particularly in scenarios with minimal annotated data available. In
this paper, we introduce the DocTTT framework to address these challenges. The
key innovation of our approach is that it uses test-time training to adapt the
model to each specific input during testing. We propose a novel Meta-Auxiliary
learning approach that combines Meta-learning and <span class="highlight-title">self-supervised</span> Masked
Autoencoder~(<span class="highlight-title">MAE</span>). During testing, we adapt the visual representation
parameters using a <span class="highlight-title">self-supervised</span> <span class="highlight-title">MAE</span> loss. During training, we learn the
model parameters using a meta-learning framework, so that the model parameters
are learned to adapt to a new input effectively. Experimental results show that
our proposed method significantly outperforms existing state-of-the-art
approaches on benchmark <span class="highlight-title">dataset</span>s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV2025, camera ready with updated reference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Motion Blur Removal in the Temporal Dimension with Video <span class="highlight-title">Diffusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Pang, Zhihao Zhan, Xiang Zhu, Yechao Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most motion deblurring algorithms rely on spatial-domain convolution models,
which struggle with the complex, non-linear blur arising from camera shake and
object motion. In contrast, we propose a novel single-image deblurring approach
that treats motion blur as a temporal averaging phenomenon. Our core innovation
lies in leveraging a <span class="highlight-title">pre-train</span>ed video <span class="highlight-title">diffusion</span> <span class="highlight-title">transformer</span> model to capture
diverse motion dynamics within a latent space. It sidesteps explicit kernel
estimation and effectively accommodates diverse motion patterns. We implement
the algorithm within a <span class="highlight-title">diffusion</span>-based inverse problem framework. Empirical
results on synthetic and real-world <span class="highlight-title">dataset</span>s demonstrate that our method
outperforms existing techniques in deblurring complex motion blur scenarios.
This work paves the way for utilizing powerful video <span class="highlight-title">diffusion</span> models to
address single-image deblurring challenges.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-21T00:00:00Z">2025-01-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNRSelect: Active Best View Selection for Deferred Neural <span class="highlight-title">Rendering</span> <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongli Wu, Haochen Li, Xiaobao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deferred neural <span class="highlight-title">rendering</span> (DNR) is an emerging computer graphics pipeline
designed for high-fidelity <span class="highlight-title">rendering</span> and robotic perception. However, DNR
heavily relies on <span class="highlight-title">dataset</span>s composed of numerous ray-traced images and demands
substantial computational resources. It remains under-explored how to reduce
the reliance on high-quality ray-traced images while maintaining the <span class="highlight-title">rendering</span>
fidelity. In this paper, we propose DNRSelect, which integrates a reinforcement
learning-based view selector and a 3D texture aggregator for deferred neural
<span class="highlight-title">rendering</span>. We first propose a novel view selector for deferred neural <span class="highlight-title">rendering</span>
based on reinforcement learning, which is trained on easily obtained rasterized
images to identify the optimal views. By acquiring only a few ray-traced images
for these selected views, the selector enables DNR to achieve high-quality
<span class="highlight-title">rendering</span>. To further enhance spatial awareness and geometric consistency in
DNR, we introduce a 3D texture aggregator that fuses pyramid features from
depth maps and normal maps with UV maps. Given that acquiring ray-traced images
is more time-consuming than generating rasterized images, DNRSelect minimizes
the need for ray-traced data by using only a few selected views while still
achieving high-fidelity <span class="highlight-title">rendering</span> results. We conduct detailed experiments and
ablation studies on the <span class="highlight-title">NeRF</span>-Synthetic <span class="highlight-title">dataset</span> to demonstrate the effectiveness
of DNRSelect. The code will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures, submitted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Underwater Scene Reconstruction using Multi-View Stereo and
  Physical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Hu, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater scene reconstruction poses a substantial challenge because of the
intricate interplay between light and the medium, resulting in scattering and
absorption effects that make both depth estimation and <span class="highlight-title">rendering</span> more complex.
While recent Neural Radiance Fields (<span class="highlight-title">NeRF</span>) based methods for underwater scenes
achieve high-quality results by modeling and separating the scattering medium,
they still suffer from slow training and <span class="highlight-title">rendering</span> speeds. To address these
limitations, we propose a novel method that integrates Multi-View Stereo (MVS)
with a physics-based underwater image formation model. Our approach consists of
two branches: one for depth estimation using the traditional cost volume
pipeline of MVS, and the other for <span class="highlight-title">rendering</span> based on the physics-based image
formation model. The depth branch improves scene geometry, while the medium
branch determines the scattering parameters to achieve precise scene <span class="highlight-title">rendering</span>.
Unlike traditional MVSNet methods that rely on ground-truth depth, our method
does not necessitate the use of depth truth, thus allowing for expedited
training and <span class="highlight-title">rendering</span> processes. By leveraging the medium subnet to estimate
the medium parameters and combining this with a color <span class="highlight-title">MLP</span> for <span class="highlight-title">rendering</span>, we
restore the true colors of underwater scenes and achieve higher-fidelity
geometric representations. Experimental results show that our method enables
high-quality synthesis of novel views in scattering media, clear views
restoration by removing the medium, and outperforms existing methods in
<span class="highlight-title">rendering</span> quality and training efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DehazeGS: Seeing Through Fog with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03659v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03659v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current novel view synthesis tasks primarily rely on high-quality and clear
images. However, in foggy scenes, scattering and attenuation can significantly
degrade the reconstruction and <span class="highlight-title">rendering</span> quality. Although <span class="highlight-title">NeRF</span>-based dehazing
reconstruction algorithms have been developed, their use of deep fully
connected neural networks and per-ray sampling strategies leads to high
computational costs. Moreover, <span class="highlight-title">NeRF</span>'s implicit representation struggles to
recover fine details from hazy scenes. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly
modeling point clouds into 3D Gaussians. In this paper, we propose leveraging
the explicit Gaussian representation to explain the foggy image formation
process through a physically accurate forward <span class="highlight-title">rendering</span> process. We introduce
DehazeGS, a method capable of decomposing and <span class="highlight-title">rendering</span> a fog-free background
from participating media using only muti-view foggy images as input. We model
the transmission within each Gaussian distribution to simulate the formation of
fog. During this process, we jointly learn the atmospheric light and scattering
coefficient while optimizing the Gaussian representation of the hazy scene. In
the inference stage, we eliminate the effects of scattering and attenuation on
the Gaussians and directly project them onto a 2D plane to obtain a clear view.
Experiments on both synthetic and real-world foggy <span class="highlight-title">dataset</span>s demonstrate that
DehazeGS achieves state-of-the-art performance in terms of both <span class="highlight-title">rendering</span>
quality and computational efficiency. visualizations are available at
https://dehazegs.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,4 figures. visualizations are available at
  https://dehazegs.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metric for Evaluating Performance of Reference-Free Demorphing Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Shukla, Arun Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A facial morph is an image created by combining two (or more) face images
pertaining to two (or more) distinct identities. Reference-free face demorphing
inverts the process and tries to recover the face images constituting a facial
morph without using any other information. However, there is no consensus on
the evaluation metrics to be used to evaluate and compare such demorphing
techniques. In this paper, we first analyze the shortcomings of the demorphing
metrics currently used in the literature. We then propose a new metric called
biometrically cross-weighted IQA that overcomes these issues and extensively
benchmark current methods on the proposed metric to show its efficacy.
Experiments on three existing demorphing methods and six <span class="highlight-title">dataset</span>s on two
commonly used face matchers validate the efficacy of our proposed metric.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Deblurring by Sharpness Prior Detection and Edge Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Tian, Fabio Brau, Giulio Rossolini, Giorgio Buttazzo, Hao Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video deblurring is essential task for autonomous driving, facial
recognition, and security surveillance. Traditional methods directly estimate
motion blur kernels, often introducing artifacts and leading to poor results.
Recent approaches utilize the detection of sharp frames within video sequences
to enhance deblurring. However, existing <span class="highlight-title">dataset</span>s rely on fixed number of sharp
frames, which may be too restrictive for some applications and may introduce a
bias during model training. To address these limitations and enhance domain
adaptability, this work first introduces GoPro Random Sharp (GoProRS), a new
<span class="highlight-title">dataset</span> where the the frequency of sharp frames within the sequence is
customizable, allowing more diverse training and testing scenarios.
Furthermore, it presents a novel video deblurring model, called SPEINet, that
integrates sharp frame features into blurry frame reconstruction through an
attention-based encoder-decoder architecture, a lightweight yet robust sharp
frame detection and an edge extraction phase. Extensive experimental results
demonstrate that SPEINet outperforms state-of-the-art methods across multiple
<span class="highlight-title">dataset</span>s, achieving an average of +3.2% PSNR improvement over recent
techniques. Given such promising results, we believe that both the proposed
model and <span class="highlight-title">dataset</span> pave the way for future advancements in video deblurring
based on the detection of sharp frames.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in Pattern Recognition</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-20T00:00:00Z">2025-01-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Chen, Yujin Wang, Xin Cai, Zhiyuan You, Zheming Lu, Fan Zhang, Shi Guo, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing high dynamic range (HDR) scenes is one of the most important issues
in camera design. Majority of cameras use exposure fusion technique, which
fuses images captured by different exposure levels, to increase dynamic range.
However, this approach can only handle images with limited exposure difference,
normally 3-4 stops. When applying to very high dynamic scenes where a large
exposure difference is required, this approach often fails due to incorrect
alignment or inconsistent lighting between inputs, or tone mapping artifacts.
In this work, we propose UltraFusion, the first exposure fusion technique that
can merge input with 9 stops differences. The key idea is that we model the
exposure fusion as a guided inpainting problem, where the under-exposed image
is used as a guidance to fill the missing information of over-exposed highlight
in the over-exposed region. Using under-exposed image as a soft guidance,
instead of a hard constrain, our model is robust to potential alignment issue
or lighting variations. Moreover, utilizing the image prior of the generative
model, our model also generates natural tone mapping, even for very
high-dynamic range scene. Our approach outperforms HDR-<span class="highlight-title">Transformer</span> on latest
HDR benchmarks. Moreover, to test its performance in ultra high dynamic range
scene, we capture a new real-world exposure fusion benchmark, UltraFusion
<span class="highlight-title">Dataset</span>, with exposure difference up to 9 stops, and experiments show that
\model~can generate beautiful and high-quality fusion results under various
scenarios. An online demo is provided at
https://openimaginglab.github.io/UltraFusion/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Large Language Models to Regress Accurate Image Quality Scores
  using Score Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of Multi-modal Large Language Models (MLLMs),
MLLM-based Image Quality Assessment (IQA) methods have shown promising
performance in linguistic quality description. However, current methods still
fall short in accurately scoring image quality. In this work, we aim to
leverage MLLMs to regress accurate quality scores. A key challenge is that the
quality score is inherently continuous, typically modeled as a Gaussian
distribution, whereas MLLMs generate discrete token outputs. This mismatch
necessitates score discretization. Previous approaches discretize the mean
score into a one-hot label, resulting in information loss and failing to
capture inter-image relationships. We propose a distribution-based approach
that discretizes the score distribution into a soft label. This method
preserves the characteristics of the score distribution, achieving high
accuracy and maintaining inter-image relationships. Moreover, to address
<span class="highlight-title">dataset</span> variation, where different IQA <span class="highlight-title">dataset</span>s exhibit various distributions,
we introduce a fidelity loss based on Thurstone's model. This loss captures
intra-<span class="highlight-title">dataset</span> relationships, facilitating co-training across multiple IQA
<span class="highlight-title">dataset</span>s. With these designs, we develop the distribution-based Depicted image
Quality Assessment model for Score regression (DeQA-Score). Experiments across
multiple benchmarks show that DeQA-Score stably outperforms baselines in score
regression. Also, DeQA-Score can predict the score distribution that closely
aligns with human annotations. Codes and model weights have been released in
https://depictqa.github.io/deqa-score/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundus Image Quality Assessment and Enhancement: a Systematic <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Li, Haojin Li, Mingyang Ou, Xiangyang Yu, Xiaoqing Zhang, Ke Niu, Huazhu Fu, Jiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an affordable and convenient eye scan, fundus photography holds the
potential for preventing vision impairment, especially in resource-limited
regions. However, fundus image degradation is common under intricate imaging
environments, impacting following diagnosis and treatment. Consequently, image
quality assessment (IQA) and enhancement (IQE) are essential for ensuring the
clinical value and reliability of fundus images. While existing <span class="highlight-title">review</span>s offer
some <span class="highlight-title">overview</span> of this field, a comprehensive analysis of the interplay between
IQA and IQE, along with their clinical deployment challenges, is lacking. This
paper addresses this gap by providing a thorough <span class="highlight-title">review</span> of fundus IQA and IQE
algorithms, research advancements, and practical applications. We outline the
fundamentals of the fundus photography imaging system and the associated
interferences, and then systematically summarize the paradigms in fundus IQA
and IQE. Furthermore, we discuss the practical challenges and solutions in
deploying IQA and IQE, as well as offer insights into potential future research
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subjective and Objective Quality Assessment of Non-Uniformly Distorted
  Omnidirectional Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiebin Yan, Jiale Rao, Xuelin Liu, Yuming Fang, Yifan Zuo, Weide Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Omnidirectional image quality assessment (OIQA) has been one of the hot
topics in IQA with the continuous development of VR techniques, and achieved
much success in the past few years. However, most studies devote themselves to
the uniform distortion issue, i.e., all regions of an omnidirectional image are
perturbed by the ``same amount'' of noise, while ignoring the non-uniform
distortion issue, i.e., partial regions undergo ``different amount'' of
perturbation with the other regions in the same omnidirectional image.
Additionally, nearly all OIQA models are verified on the platforms containing a
limited number of samples, which largely increases the over-fitting risk and
therefore impedes the development of OIQA. To alleviate these issues, we
elaborately explore this topic from both subjective and objective perspectives.
Specifically, we construct a large OIQA database containing 10,320
non-uniformly distorted omnidirectional images, each of which is generated by
considering quality impairments on one or two camera len(s). Then we
meticulously conduct psychophysical experiments and delve into the influence of
both holistic and individual factors (i.e., distortion range and viewing
condition) on omnidirectional image quality. Furthermore, we propose a
perception-guided OIQA model for non-uniform distortion by adaptively
simulating users' viewing behavior. Experimental results demonstrate that the
proposed model outperforms state-of-the-art methods. The source code is
available at https://github.com/RJL2000/OIQAND.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-17T00:00:00Z">2025-01-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surface-SOS: <span class="highlight-title">Self-Supervised</span> Object Segmentation via Neural Surface
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyun Zheng, Liwei Liao, Jianbo Jiao, Feng Gao, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Self-supervised</span> Object Segmentation (SOS) aims to segment objects without any
annotations. Under conditions of multi-camera inputs, the structural, textural
and geometrical consistency among each view can be leveraged to achieve
fine-grained object segmentation. To make better use of the above information,
we propose Surface representation based <span class="highlight-title">Self-supervised</span> Object Segmentation
(Surface-SOS), a new framework to segment objects for each view by 3D surface
representation from multi-view images of a scene. To model high-quality
geometry surfaces for complex scenes, we design a novel scene representation
scheme, which decomposes the scene into two complementary neural representation
modules respectively with a Signed Distance Function (SDF). Moreover,
Surface-SOS is able to refine single-view segmentation with multi-view
unlabeled images, by introducing coarse segmentation masks as additional input.
To the best of our knowledge, Surface-SOS is the first <span class="highlight-title">self-supervised</span> approach
that leverages neural surface representation to break the dependence on large
amounts of annotated data and strong constraints. These constraints typically
involve observing target objects against a static background or relying on
temporal supervision in videos. Extensive experiments on standard benchmarks
including LLFF, CO3D, BlendedMVS, TUM and several real-world scenes show that
Surface-SOS always yields finer object masks than its <span class="highlight-title">NeRF</span>-based counterparts
and surpasses supervised single-view baselines remarkably. Code is available
at: https://github.com/zhengxyun/Surface-SOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TIP</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surface-SOS: <span class="highlight-title">Self-Supervised</span> Object Segmentation via Neural Surface
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyun Zheng, Liwei Liao, Jianbo Jiao, Feng Gao, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Self-supervised</span> Object Segmentation (SOS) aims to segment objects without any
annotations. Under conditions of multi-camera inputs, the structural, textural
and geometrical consistency among each view can be leveraged to achieve
fine-grained object segmentation. To make better use of the above information,
we propose Surface representation based <span class="highlight-title">Self-supervised</span> Object Segmentation
(Surface-SOS), a new framework to segment objects for each view by 3D surface
representation from multi-view images of a scene. To model high-quality
geometry surfaces for complex scenes, we design a novel scene representation
scheme, which decomposes the scene into two complementary neural representation
modules respectively with a Signed Distance Function (SDF). Moreover,
Surface-SOS is able to refine single-view segmentation with multi-view
unlabeled images, by introducing coarse segmentation masks as additional input.
To the best of our knowledge, Surface-SOS is the first <span class="highlight-title">self-supervised</span> approach
that leverages neural surface representation to break the dependence on large
amounts of annotated data and strong constraints. These constraints typically
involve observing target objects against a static background or relying on
temporal supervision in videos. Extensive experiments on standard benchmarks
including LLFF, CO3D, BlendedMVS, TUM and several real-world scenes show that
Surface-SOS always yields finer object masks than its <span class="highlight-title">NeRF</span>-based counterparts
and surpasses supervised single-view baselines remarkably. Code is available
at: https://github.com/zhengxyun/Surface-SOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Super<span class="highlight-title">NeRF</span>-GAN: A Universal 3D-Consistent Super-Resolution Framework for
  Efficient and Enhanced 3D-Aware Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Zheng, Linzhi Huang, Yizhou Yu, Yi Chang, Yilin Wang, Rui Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural volume <span class="highlight-title">rendering</span> techniques, such as <span class="highlight-title">NeRF</span>, have revolutionized
3D-aware image synthesis by enabling the generation of images of a single scene
or object from various camera poses. However, the high computational cost of
<span class="highlight-title">NeRF</span> presents challenges for synthesizing high-resolution (HR) images. Most
existing methods address this issue by leveraging 2D super-resolution, which
compromise 3D-consistency. Other methods propose radiance manifolds or
two-stage generation to achieve 3D-consistent HR synthesis, yet they are
limited to specific synthesis tasks, reducing their universality. To tackle
these challenges, we propose Super<span class="highlight-title">NeRF</span>-GAN, a universal framework for
3D-consistent super-resolution. A key highlight of Super<span class="highlight-title">NeRF</span>-GAN is its
seamless integration with <span class="highlight-title">NeRF</span>-based 3D-aware image synthesis methods and it
can simultaneously enhance the resolution of generated images while preserving
3D-consistency and reducing computational cost. Specifically, given a
<span class="highlight-title">pre-train</span>ed generator capable of producing a <span class="highlight-title">NeRF</span> representation such as
tri-plane, we first perform volume <span class="highlight-title">rendering</span> to obtain a low-resolution image
with corresponding depth and normal map. Then, we employ a <span class="highlight-title">NeRF</span>
Super-Resolution module which learns a network to obtain a high-resolution
<span class="highlight-title">NeRF</span>. Next, we propose a novel Depth-Guided <span class="highlight-title">Rendering</span> process which contains
three simple yet effective steps, including the construction of a
boundary-correct multi-depth map through depth aggregation, a normal-guided
depth super-resolution and a depth-guided <span class="highlight-title">NeRF</span> <span class="highlight-title">rendering</span>. Experimental results
demonstrate the superior efficiency, 3D-consistency, and quality of our
approach. Additionally, ablation studies confirm the effectiveness of our
proposed components.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IE-Bench: Advancing the Measurement of Text-Driven Image Editing for
  Human Perception Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangkun Sun, Bowen Qu, Xiaoyu Liang, Songlin Fan, Wei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-driven image editing have been significant, yet the
task of accurately evaluating these edited images continues to pose a
considerable challenge. Different from the assessment of text-driven image
generation, text-driven image editing is characterized by simultaneously
conditioning on both text and a source image. The edited images often retain an
intrinsic connection to the original image, which dynamically change with the
semantics of the text. However, previous methods tend to solely focus on
text-image alignment or have not aligned with human perception. In this work,
we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to
enhance the assessment of text-driven edited images. IE-Bench includes a
database contains diverse source images, various editing <span class="highlight-title">prompt</span>s and the
corresponding results different editing methods, and total 3,010 Mean Opinion
Scores (MOS) provided by 25 human subjects. Furthermore, we introduce IE-QA, a
multi-modality source-aware quality assessment method for text-driven image
editing. To the best of our knowledge, IE-Bench offers the first IQA <span class="highlight-title">dataset</span>
and model tailored for text-driven image editing. Extensive experiments
demonstrate IE-QA's superior subjective-alignments on the text-driven image
editing task compared with previous metrics. We will make all related data and
code available to the public.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffStereo: High-Frequency Aware <span class="highlight-title">Diffusion</span> Model for Stereo Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyun Cao, Yuan Shi, Bin Xia, Xiaoyu Jin, Wenming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Diffusion</span> models (DMs) have achieved promising performance in image
restoration but haven't been explored for stereo images. The application of DM
in stereo image restoration is confronted with a series of challenges. The need
to reconstruct two images exacerbates DM's computational cost. Additionally,
existing latent DMs usually focus on semantic information and remove
high-frequency details as redundancy during latent compression, which is
precisely what matters for image restoration. To address the above problems, we
propose a high-frequency aware <span class="highlight-title">diffusion</span> model, DiffStereo for stereo image
restoration as the first attempt at DM in this domain. Specifically, DiffStereo
first learns latent high-frequency representations (LHFR) of HQ images. DM is
then trained in the learned space to estimate LHFR for stereo images, which are
fused into a <span class="highlight-title">transformer</span>-based stereo image restoration network providing
beneficial high-frequency information of corresponding HQ images. The
resolution of LHFR is kept the same as input images, which preserves the
inherent texture from distortion. And the compression in channels alleviates
the computational burden of DM. Furthermore, we devise a position encoding
scheme when integrating the LHFR into the restoration network, enabling
distinctive guidance in different depths of the restoration network.
Comprehensive experiments verify that by combining generative DM and
<span class="highlight-title">transformer</span>, DiffStereo achieves both higher reconstruction accuracy and better
perceptual quality on stereo super-resolution, deblurring, and low-light
enhancement compared with state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-16T00:00:00Z">2025-01-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Poxel: Voxel Reconstruction for 3D Printing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Cao, Satoshi Yagi, Satoshi Yamamori, Jun Morimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D reconstruction, especially through neural <span class="highlight-title">rendering</span>
approaches like Neural Radiance Fields (<span class="highlight-title">NeRF</span>) and Plenoxel, have led to
high-quality 3D visualizations. However, these methods are optimized for
digital environments and employ view-dependent color models (RGB) and 2D
splatting techniques, which do not translate well to physical 3D printing. This
paper introduces "Poxel", which stands for Printable-Voxel, a voxel-based 3D
reconstruction framework optimized for photopolymer jetting 3D printing, which
allows for high-resolution, full-color 3D models using a CMYKWCl color model.
Our framework directly outputs printable voxel grids by removing
view-dependency and converting the digital RGB color space to a physical
CMYKWCl color space suitable for multi-material jetting. The proposed system
achieves better fidelity and quality in printed models, aligning with the
requirements of physical 3D objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normal-<span class="highlight-title">NeRF</span>: Ambiguity-Robust Normal Estimation for Highly Reflective
  Scenes <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, Wenzhen Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>) often struggle with reconstructing and
<span class="highlight-title">rendering</span> highly reflective scenes. Recent advancements have developed various
reflection-aware appearance models to enhance <span class="highlight-title">NeRF</span>'s capability to render
specular reflections. However, the robust reconstruction of highly reflective
scenes is still hindered by the inherent shape ambiguity on specular surfaces.
Existing methods typically rely on additional geometry priors to regularize the
shape prediction, but this can lead to oversmoothed geometry in complex scenes.
Observing the critical role of surface normals in parameterizing reflections,
we introduce a transmittance-gradient-based normal estimation technique that
remains robust even under ambiguous shape conditions. Furthermore, we propose a
dual activated densities module that effectively bridges the gap between smooth
surface normals and sharp object boundaries. Combined with a reflection-aware
appearance model, our proposed method achieves robust reconstruction and
high-fidelity <span class="highlight-title">rendering</span> of scenes featuring both highly specular reflections
and intricate geometric structures. Extensive experiments demonstrate that our
method outperforms existing state-of-the-art methods on various <span class="highlight-title">dataset</span>s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025, code available at https://github.com/sjj118/Normal-NeRF</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Transmission and Deblurring: A Semantic Communication Approach
  Using Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pujing Yang, Guangyi Zhang, Yunlong Cai, Lei Yu, Guanding Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based joint source-channel coding (JSCC) is emerging as a
promising technology for effective image transmission. However, most existing
approaches focus on transmitting clear images, overlooking real-world
challenges such as motion blur caused by camera shaking or fast-moving objects.
Motion blur often degrades image quality, making transmission and
reconstruction more challenging. Event cameras, which asynchronously record
pixel intensity changes with extremely low latency, have shown great potential
for motion deblurring tasks. However, the efficient transmission of the
abundant data generated by event cameras remains a significant challenge. In
this work, we propose a novel JSCC framework for the joint transmission of
blurry images and events, aimed at achieving high-quality reconstructions under
limited channel bandwidth. This approach is designed as a deblurring
task-oriented JSCC system. Since RGB cameras and event cameras capture the same
scene through different modalities, their outputs contain both shared and
domain-specific information. To avoid repeatedly transmitting the shared
information, we extract and transmit their shared information and
domain-specific information, respectively. At the receiver, the received
signals are processed by a deblurring decoder to generate clear images.
Additionally, we introduce a multi-stage training strategy to train the
proposed model. Simulation results demonstrate that our method significantly
outperforms existing JSCC-based image transmission schemes, addressing motion
blur effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention
  for Image Restoration Models Compression <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongheng Zhang, Danfeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Transformer</span>-based encoder-decoder models have achieved remarkable success in
image-to-image transfer tasks, particularly in image restoration. However,
their high computational complexity-manifested in elevated FLOPs and parameter
counts-limits their application in real-world scenarios. Existing knowledge
distillation methods in image restoration typically employ lightweight student
models that directly mimic the intermediate features and reconstruction results
of the teacher, overlooking the implicit attention relationships between them.
To address this, we propose a Soft Knowledge Distillation (SKD) strategy that
incorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for
compressing image restoration models. This mechanism facilitates interaction
between the student and teacher across both channel and spatial dimensions,
enabling the student to implicitly learn the attention matrices. Additionally,
we employ a Gaussian kernel function to measure the distance between student
and teacher features in kernel space, ensuring stable and efficient feature
learning. To further enhance the quality of reconstructed images, we replace
the commonly used L1 or KL divergence loss with a contrastive learning loss at
the image level. Experiments on three tasks-image deraining, deblurring, and
denoising-demonstrate that our SKD strategy significantly reduces computational
complexity while maintaining strong image restoration capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-15T00:00:00Z">2025-01-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and High-Quality Neural Implicit Representation for 3D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyuan Yang, Bailin Deng, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various SDF-based neural implicit surface reconstruction methods have been
proposed recently, and have demonstrated remarkable modeling capabilities.
However, due to the global nature and limited representation ability of a
single network, existing methods still suffer from many drawbacks, such as
limited accuracy and scale of the reconstruction. In this paper, we propose a
versatile, scalable and high-quality neural implicit representation to address
these issues. We integrate a divide-and-conquer approach into the neural
SDF-based reconstruction. Specifically, we model the object or scene as a
fusion of multiple independent local neural SDFs with overlapping regions. The
construction of our representation involves three key steps: (1) constructing
the distribution and overlap relationship of the local radiance fields based on
object structure or data distribution, (2) relative pose registration for
adjacent local SDFs, and (3) SDF blending. Thanks to the independent
representation of each local region, our approach can not only achieve
high-fidelity surface reconstruction, but also enable scalable scene
reconstruction. Extensive experimental results demonstrate the effectiveness
and practicality of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View
  Synthesis <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method that achieves state-of-the-art <span class="highlight-title">rendering</span> quality and
efficiency on monocular dynamic scene reconstruction using deformable 3D
Gaussians. Implicit deformable representations commonly model motion with a
canonical space and time-dependent backward-warping deformation field. Our
method, GauFRe, uses a forward-warping deformation to explicitly model
non-rigid transformations of scene geometry. Specifically, we propose a
template set of 3D Gaussians residing in a canonical space, and a
time-dependent forward-warping deformation field to model dynamic objects.
Additionally, we tailor a 3D Gaussian-specific static component supported by an
inductive bias-aware initialization approach which allows the deformation field
to focus on moving scene regions, improving the <span class="highlight-title">rendering</span> of complex real-world
motion. The differentiable pipeline is optimized end-to-end with a
<span class="highlight-title">self-supervised</span> <span class="highlight-title">rendering</span> loss. Experiments show our method achieves
competitive results and higher efficiency than both previous state-of-the-art
<span class="highlight-title">NeRF</span> and Gaussian-based methods. For real-world scenes, GauFRe can train in ~20
mins and offer 96 FPS real-time <span class="highlight-title">rendering</span> on an RTX 3090 GPU. Project website:
https://lynl7130.github.io/gaufre/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025. 11 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When No-Reference Image Quality Models Meet MAP Estimation in <span class="highlight-title">Diffusion</span>
  Latents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixia Zhang, Dingquan Li, Guangtao Zhai, Xiaokang Yang, Kede Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary no-reference image quality assessment (NR-IQA) models can
effectively quantify perceived image quality, often achieving strong
correlations with human perceptual scores on standard IQA benchmarks. Yet,
limited efforts have been devoted to treating NR-IQA models as natural image
priors for real-world image enhancement, and consequently comparing them from a
perceptual optimization standpoint. In this work, we show -- for the first time
-- that NR-IQA models can be plugged into the maximum a posteriori (MAP)
estimation framework for image enhancement. This is achieved by performing
gradient ascent in the <span class="highlight-title">diffusion</span> latent space rather than in the raw pixel
domain, leveraging a <span class="highlight-title">pretrain</span>ed differentiable and bijective <span class="highlight-title">diffusion</span> process.
Likely, different NR-IQA models lead to different enhanced outputs, which in
turn provides a new computational means of comparing them. Unlike conventional
correlation-based measures, our comparison method offers complementary insights
into the respective strengths and weaknesses of the competing NR-IQA models in
perceptual optimization scenarios. Additionally, we aim to improve the
best-performing NR-IQA model in <span class="highlight-title">diffusion</span> latent MAP estimation by
incorporating the advantages of other top-performing methods. The resulting
model delivers noticeably better results in enhancing real-world images
afflicted by unknown and complex distortions, all preserving a high degree of
image fidelity.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeblurDiNAT: A Compact Model with Exceptional Generalization and Visual
  Fidelity on Unseen Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13163v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13163v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhou Liu, Binghan Li, Chengkai Liu, Mi Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deblurring networks have effectively restored clear images from the
blurred ones. However, they often struggle with generalization to unknown
domains. Moreover, these models typically focus on distortion metrics such as
PSNR and SSIM, neglecting the critical aspect of metrics aligned with human
perception. To address these limitations, we propose DeblurDiNAT, a deblurring
<span class="highlight-title">Transformer</span> based on Dilated Neighborhood Attention. First, DeblurDiNAT employs
an alternating dilation factor paradigm to capture both local and global
blurred patterns, enhancing generalization and perceptual clarity. Second, a
local cross-channel learner aids the <span class="highlight-title">Transformer</span> block to understand the
short-range relationships between adjacent channels. Additionally, we present a
linear feed-forward network with a simple while effective design. Finally, a
dual-stage feature fusion module is introduced as an alternative to the
existing approach, which efficiently process multi-scale visual information
across network levels. Compared to state-of-the-art models, our compact
DeblurDiNAT demonstrates superior generalization capabilities and achieves
remarkable performance in perceptual metrics, while maintaining a favorable
model size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ultra-High-Definition Image Deblurring via Multi-scale Cubic-Mixer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchi Chen, Xiuyi Jia, Zhuoran Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, <span class="highlight-title">transformer</span>-based algorithms are making a splash in the domain of
image deblurring. Their achievement depends on the self-attention mechanism
with CNN stem to model long range dependencies between tokens. Unfortunately,
this ear-pleasing pipeline introduces high computational complexity and makes
it difficult to run an ultra-high-definition image on a single GPU in real
time. To trade-off accuracy and efficiency, the input degraded image is
computed cyclically over three dimensional ($C$, $W$, and $H$) signals without
a self-attention mechanism. We term this deep network as Multi-scale
Cubic-Mixer, which is acted on both the real and imaginary components after
fast Fourier transform to estimate the Fourier coefficients and thus obtain a
deblurred image. Furthermore, we combine the multi-scale cubic-mixer with a
slicing strategy to generate high-quality results at a much lower computational
cost. Experimental results demonstrate that the proposed algorithm performs
favorably against the state-of-the-art deblurring approaches on the several
benchmarks and a new ultra-high-definition <span class="highlight-title">dataset</span> in terms of accuracy and
speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-14T00:00:00Z">2025-01-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wu, Zicheng Zhang, Muer Tie, Ziqing Ai, Zhongxue Gan, Wenchao Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework
designed for large scenes. The framework comprises four main components: VIO
Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO
Front End, RGB frames are processed through dense bundle adjustment and
uncertainty estimation to extract scene geometry and poses. Based on this
output, the mapping module incrementally constructs and maintains a 2D Gaussian
map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,
Score Manager, and Pose Refinement, which collectively improve mapping speed
and localization accuracy. This enables the SLAM system to handle large-scale
urban environments with up to 50 million Gaussian ellipsoids. To ensure global
consistency in large-scale scenes, we design a Loop Closure module, which
innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian
Splatting for loop closure detection and correction of the Gaussian map.
Additionally, we propose a Dynamic Eraser to address the inevitable presence of
dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor
and outdoor environments demonstrate that our approach achieves localization
performance on par with Visual-Inertial Odometry while surpassing recent
GS/<span class="highlight-title">NeRF</span> SLAM methods. It also significantly outperforms all existing methods in
terms of mapping and <span class="highlight-title">rendering</span> quality. Furthermore, we developed a mobile app
and verified that our framework can generate high-quality Gaussian maps in real
time using only a smartphone camera and a low-frequency IMU sensor. To the best
of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method
capable of operating in outdoor environments and supporting kilometer-scale
large scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UrbanIR: Large-Scale Urban Scene Inverse <span class="highlight-title">Rendering</span> from a Single Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09349v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09349v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Hao Lin, Bohan Liu, Yi-Ting Chen, Kuan-Sheng Chen, David Forsyth, Jia-Bin Huang, Anand Bhattad, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present UrbanIR (Urban Scene Inverse <span class="highlight-title">Rendering</span>), a new inverse graphics
model that enables realistic, free-viewpoint <span class="highlight-title">rendering</span>s of scenes under various
lighting conditions with a single video. It accurately infers shape, albedo,
visibility, and sun and sky illumination from wide-baseline videos, such as
those from car-mounted cameras, differing from <span class="highlight-title">NeRF</span>'s dense view settings. In
this context, standard methods often yield subpar geometry and material
estimates, such as inaccurate roof representations and numerous 'floaters'.
UrbanIR addresses these issues with novel losses that reduce errors in inverse
graphics inference and <span class="highlight-title">rendering</span> artifacts. Its techniques allow for precise
shadow volume estimation in the original scene. The model's outputs support
controllable editing, enabling photorealistic free-viewpoint <span class="highlight-title">rendering</span>s of
night simulations, relit scenes, and inserted objects, marking a significant
improvement over existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://urbaninverserendering.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Hu, Rong Liu, Meida Chen, Peter Beerel, Andrew Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-fidelity 3D reconstruction from monocular video remains
challenging due to the inherent limitations of traditional methods like
Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene
details. While differentiable <span class="highlight-title">rendering</span> techniques such as Neural Radiance
Fields (<span class="highlight-title">NeRF</span>) address some of these challenges, their high computational costs
make them unsuitable for real-time applications. Additionally, existing 3D
Gaussian Splatting (3DGS) methods often focus on photometric consistency,
neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and
pose updates for scene refinement. We propose a framework integrating dense
SLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach
introduces SLAM-Informed Adaptive Densification, which dynamically updates and
densifies the Gaussian model by leveraging dense point clouds from SLAM.
Additionally, we incorporate Geometry-Guided Optimization, which combines
edge-aware geometric constraints and photometric consistency to jointly
optimize the appearance and geometry of the 3DGS scene representation, enabling
detailed and accurate SLAM mapping reconstruction. Experiments on the Replica
and TUM-RGBD <span class="highlight-title">dataset</span>s demonstrate the effectiveness of our approach, achieving
state-of-the-art results among monocular systems. Specifically, our method
achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,
representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the
previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by
10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the
potential of our framework in bridging the gap between photometric and
geometric dense 3D scene representations, paving the way for practical and
efficient monocular dense reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Gotin, Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have revealed that modern image and video quality assessment
(IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can
manipulate a video through preprocessing to artificially increase its quality
score according to a certain metric, despite no actual improvement in visual
quality. Most of the attacks studied in the literature are white-box attacks,
while black-box attacks in the context of VQA have received less attention.
Moreover, some research indicates a lack of transferability of adversarial
examples generated for one model to another when applied to VQA. In this paper,
we propose a cross-modal attack method, IC2VQA, aimed at exploring the
vulnerabilities of modern VQA models. This approach is motivated by the
observation that the low-level feature spaces of images and videos are similar.
We investigate the transferability of adversarial perturbations across
different modalities; specifically, we analyze how adversarial perturbations
generated on a white-box IQA model with an additional <span class="highlight-title">CLIP</span> module can
effectively target a VQA model. The addition of the <span class="highlight-title">CLIP</span> module serves as a
valuable aid in increasing transferability, as the <span class="highlight-title">CLIP</span> model is known for its
effective capture of low-level semantics. Extensive experiments demonstrate
that IC2VQA achieves a high success rate in attacking three black-box VQA
models. We compare our method with existing black-box attack strategies,
highlighting its superiority in terms of attack success within the same number
of iterations and levels of attack strength. We believe that the proposed
method will contribute to the deeper analysis of robust VQA metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for VISAPP 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-02-14T01:04:59.772625506Z">
            2025-02-14 01:04:59 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
