{"2025-02-10T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2502.06608v1","updated":"2025-02-10T16:07:54Z","published":"2025-02-10T16:07:54Z","title":"TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models","summary":"  Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprece- dented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data process- ing, and insufficient exploration of\nadvanced tech- niques in the 3D domain. Current approaches to 3D shape\ngeneration face substantial challenges in terms of output quality,\ngeneralization capa- bility, and alignment with input conditions. We present\nTripoSG, a new streamlined shape diffu- sion paradigm capable of generating\nhigh-fidelity 3D meshes with precise correspondence to input images.\nSpecifically, we propose: 1) A large-scale rectified flow transformer for 3D\nshape generation, achieving state-of-the-art fidelity through training on\nextensive, high-quality data. 2) A hybrid supervised training strategy\ncombining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality\n3D reconstruction performance. 3) A data processing pipeline to generate 2\nmillion high- quality 3D samples, highlighting the crucial rules for data\nquality and quantity in training 3D gen- erative models. Through comprehensive\nexperi- ments, we have validated the effectiveness of each component in our new\nframework. The seamless integration of these parts has enabled TripoSG to\nachieve state-of-the-art performance in 3D shape generation. The resulting 3D\nshapes exhibit en- hanced detail due to high-resolution capabilities and\ndemonstrate exceptional fidelity to input im- ages. Moreover, TripoSG\ndemonstrates improved versatility in generating 3D models from diverse image\nstyles and contents, showcasing strong gen- eralization capabilities. To foster\nprogress and innovation in the field of 3D generation, we will make our model\npublicly available.\n","authors":["Yangguang Li","Zi-Xin Zou","Zexiang Liu","Dehu Wang","Yuan Liang","Zhipeng Yu","Xingchao Liu","Yuan-Chen Guo","Ding Liang","Wanli Ouyang","Yan-Pei Cao"],"pdf_url":"https://arxiv.org/pdf/2502.06608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18917v7","updated":"2025-02-10T14:14:12Z","published":"2023-10-29T06:10:46Z","title":"TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural\n  Radiance Fields","summary":"  Previous attempts to integrate Neural Radiance Fields (NeRF) into the\nSimultaneous Localization and Mapping (SLAM) framework either rely on the\nassumption of static scenes or require the ground truth camera poses, which\nimpedes their application in real-world scenarios. This paper proposes a\ntime-varying representation to track and reconstruct the dynamic scenes.\nFirstly, two processes, a tracking process and a mapping process, are\nmaintained simultaneously in our framework. In the tracking process, all input\nimages are uniformly sampled and then progressively trained in a\nself-supervised paradigm. In the mapping process, we leverage motion masks to\ndistinguish dynamic objects from the static background, and sample more pixels\nfrom dynamic areas. Secondly, the parameter optimization for both processes is\ncomprised of two stages: the first stage associates time with 3D positions to\nconvert the deformation field to the canonical field. The second stage\nassociates time with the embeddings of the canonical field to obtain colors and\na Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection\nstrategy based on the overlapping rate. Our approach is evaluated on two\nsynthetic datasets and one real-world dataset, and the experiments validate\nthat our method achieves competitive results in both tracking and mapping when\ncompared to existing state-of-the-art NeRF-based dynamic SLAM systems.\n","authors":["Chengyao Duan","Zhiliu Yang"],"pdf_url":"https://arxiv.org/pdf/2310.18917v7.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2408.14899v3","updated":"2025-02-10T22:30:45Z","published":"2024-08-27T09:23:18Z","title":"MeshUp: Multi-Target Mesh Deformation via Blended Score Distillation","summary":"  We propose MeshUp, a technique that deforms a 3D mesh towards multiple target\nconcepts, and intuitively controls the region where each concept is expressed.\nConveniently, the concepts can be defined as either text queries, e.g., \"a dog\"\nand \"a turtle,\" or inspirational images, and the local regions can be selected\nas any number of vertices on the mesh. We can effectively control the influence\nof the concepts and mix them together using a novel score distillation\napproach, referred to as the Blended Score Distillation (BSD). BSD operates on\neach attention layer of the denoising U-Net of a diffusion model as it extracts\nand injects the per-objective activations into a unified denoising pipeline\nfrom which the deformation gradients are calculated. To localize the expression\nof these activations, we create a probabilistic Region of Interest (ROI) map on\nthe surface of the mesh, and turn it into 3D-consistent masks that we use to\ncontrol the expression of these activations. We demonstrate the effectiveness\nof BSD empirically and show that it can deform various meshes towards multiple\nobjectives. Our project page is at https://threedle.github.io/MeshUp.\n","authors":["Hyunwoo Kim","Itai Lang","Noam Aigerman","Thibault Groueix","Vladimir G. Kim","Rana Hanocka"],"pdf_url":"https://arxiv.org/pdf/2408.14899v3.pdf","comment":"Project page: https://threedle.github.io/MeshUp"},{"id":"http://arxiv.org/abs/2502.07030v1","updated":"2025-02-10T20:50:12Z","published":"2025-02-10T20:50:12Z","title":"PrismAvatar: Real-time animated 3D neural head avatars on edge devices","summary":"  We present PrismAvatar: a 3D head avatar model which is designed specifically\nto enable real-time animation and rendering on resource-constrained edge\ndevices, while still enjoying the benefits of neural volumetric rendering at\ntraining time. By integrating a rigged prism lattice with a 3D morphable head\nmodel, we use a hybrid rendering model to simultaneously reconstruct a\nmesh-based head and a deformable NeRF model for regions not represented by the\n3DMM. We then distill the deformable NeRF into a rigged mesh and neural\ntextures, which can be animated and rendered efficiently within the constraints\nof the traditional triangle rendering pipeline. In addition to running at 60\nfps with low memory usage on mobile devices, we find that our trained models\nhave comparable quality to state-of-the-art 3D avatar models on desktop\ndevices.\n","authors":["Prashant Raina","Felix Taubner","Mathieu Tuli","Eu Wern Teh","Kevin Ferreira"],"pdf_url":"https://arxiv.org/pdf/2502.07030v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.06608v1","updated":"2025-02-10T16:07:54Z","published":"2025-02-10T16:07:54Z","title":"TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models","summary":"  Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprece- dented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data process- ing, and insufficient exploration of\nadvanced tech- niques in the 3D domain. Current approaches to 3D shape\ngeneration face substantial challenges in terms of output quality,\ngeneralization capa- bility, and alignment with input conditions. We present\nTripoSG, a new streamlined shape diffu- sion paradigm capable of generating\nhigh-fidelity 3D meshes with precise correspondence to input images.\nSpecifically, we propose: 1) A large-scale rectified flow transformer for 3D\nshape generation, achieving state-of-the-art fidelity through training on\nextensive, high-quality data. 2) A hybrid supervised training strategy\ncombining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality\n3D reconstruction performance. 3) A data processing pipeline to generate 2\nmillion high- quality 3D samples, highlighting the crucial rules for data\nquality and quantity in training 3D gen- erative models. Through comprehensive\nexperi- ments, we have validated the effectiveness of each component in our new\nframework. The seamless integration of these parts has enabled TripoSG to\nachieve state-of-the-art performance in 3D shape generation. The resulting 3D\nshapes exhibit en- hanced detail due to high-resolution capabilities and\ndemonstrate exceptional fidelity to input im- ages. Moreover, TripoSG\ndemonstrates improved versatility in generating 3D models from diverse image\nstyles and contents, showcasing strong gen- eralization capabilities. To foster\nprogress and innovation in the field of 3D generation, we will make our model\npublicly available.\n","authors":["Yangguang Li","Zi-Xin Zou","Zexiang Liu","Dehu Wang","Yuan Liang","Zhipeng Yu","Xingchao Liu","Yuan-Chen Guo","Ding Liang","Wanli Ouyang","Yan-Pei Cao"],"pdf_url":"https://arxiv.org/pdf/2502.06608v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2502.07030v1","updated":"2025-02-10T20:50:12Z","published":"2025-02-10T20:50:12Z","title":"PrismAvatar: Real-time animated 3D neural head avatars on edge devices","summary":"  We present PrismAvatar: a 3D head avatar model which is designed specifically\nto enable real-time animation and rendering on resource-constrained edge\ndevices, while still enjoying the benefits of neural volumetric rendering at\ntraining time. By integrating a rigged prism lattice with a 3D morphable head\nmodel, we use a hybrid rendering model to simultaneously reconstruct a\nmesh-based head and a deformable NeRF model for regions not represented by the\n3DMM. We then distill the deformable NeRF into a rigged mesh and neural\ntextures, which can be animated and rendered efficiently within the constraints\nof the traditional triangle rendering pipeline. In addition to running at 60\nfps with low memory usage on mobile devices, we find that our trained models\nhave comparable quality to state-of-the-art 3D avatar models on desktop\ndevices.\n","authors":["Prashant Raina","Felix Taubner","Mathieu Tuli","Eu Wern Teh","Kevin Ferreira"],"pdf_url":"https://arxiv.org/pdf/2502.07030v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.07007v1","updated":"2025-02-10T20:13:16Z","published":"2025-02-10T20:13:16Z","title":"Grounding Creativity in Physics: A Brief Survey of Physical Priors in\n  AIGC","summary":"  Recent advancements in AI-generated content have significantly improved the\nrealism of 3D and 4D generation. However, most existing methods prioritize\nappearance consistency while neglecting underlying physical principles, leading\nto artifacts such as unrealistic deformations, unstable dynamics, and\nimplausible objects interactions. Incorporating physics priors into generative\nmodels has become a crucial research direction to enhance structural integrity\nand motion realism. This survey provides a review of physics-aware generative\nmethods, systematically analyzing how physical constraints are integrated into\n3D and 4D generation. First, we examine recent works in incorporating physical\npriors into static and dynamic 3D generation, categorizing methods based on\nrepresentation types, including vision-based, NeRF-based, and Gaussian\nSplatting-based approaches. Second, we explore emerging techniques in 4D\ngeneration, focusing on methods that model temporal dynamics with physical\nsimulations. Finally, we conduct a comparative analysis of major methods,\nhighlighting their strengths, limitations, and suitability for different\nmaterials and motion dynamics. By presenting an in-depth analysis of\nphysics-grounded AIGC, this survey aims to bridge the gap between generative\nmodels and physical realism, providing insights that inspire future research in\nphysically consistent content generation.\n","authors":["Siwei Meng","Yawei Luo","Ping Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18917v7","updated":"2025-02-10T14:14:12Z","published":"2023-10-29T06:10:46Z","title":"TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural\n  Radiance Fields","summary":"  Previous attempts to integrate Neural Radiance Fields (NeRF) into the\nSimultaneous Localization and Mapping (SLAM) framework either rely on the\nassumption of static scenes or require the ground truth camera poses, which\nimpedes their application in real-world scenarios. This paper proposes a\ntime-varying representation to track and reconstruct the dynamic scenes.\nFirstly, two processes, a tracking process and a mapping process, are\nmaintained simultaneously in our framework. In the tracking process, all input\nimages are uniformly sampled and then progressively trained in a\nself-supervised paradigm. In the mapping process, we leverage motion masks to\ndistinguish dynamic objects from the static background, and sample more pixels\nfrom dynamic areas. Secondly, the parameter optimization for both processes is\ncomprised of two stages: the first stage associates time with 3D positions to\nconvert the deformation field to the canonical field. The second stage\nassociates time with the embeddings of the canonical field to obtain colors and\na Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection\nstrategy based on the overlapping rate. Our approach is evaluated on two\nsynthetic datasets and one real-world dataset, and the experiments validate\nthat our method achieves competitive results in both tracking and mapping when\ncompared to existing state-of-the-art NeRF-based dynamic SLAM systems.\n","authors":["Chengyao Duan","Zhiliu Yang"],"pdf_url":"https://arxiv.org/pdf/2310.18917v7.pdf","comment":null}],"HDR":[{"id":"http://arxiv.org/abs/2502.06973v1","updated":"2025-02-10T19:12:41Z","published":"2025-02-10T19:12:41Z","title":"Indoor Light and Heat Estimation from a Single Panorama","summary":"  This paper presents a novel application for directly estimating indoor light\nand heat maps from captured indoor-outdoor High Dynamic Range (HDR) panoramas.\nIn our image-based rendering method, the indoor panorama is used to estimate\nthe 3D room layout, while the corresponding outdoor panorama serves as an\nenvironment map to infer spatially-varying light and material properties. We\nestablish a connection between indoor light transport and heat transport and\nimplement transient heat simulation to generate indoor heat panoramas. The\nsensitivity analysis of various thermal parameters is conducted, and the\nresulting heat maps are compared with the images captured by the thermal camera\nin real-world scenarios. This digital application enables automatic indoor\nlight and heat estimation without manual inputs and cumbersome field\nmeasurements.\n","authors":["Guanzhou Ji","Sriram Narayanan","Azadeh Sawyer","Srinivasa Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2502.06973v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2502.06476v1","updated":"2025-02-10T13:54:55Z","published":"2025-02-10T13:54:55Z","title":"Image Intrinsic Scale Assessment: Bridging the Gap Between Quality and\n  Resolution","summary":"  Image Quality Assessment (IQA) measures and predicts perceived image quality\nby human observers. Although recent studies have highlighted the critical\ninfluence that variations in the scale of an image have on its perceived\nquality, this relationship has not been systematically quantified. To bridge\nthis gap, we introduce the Image Intrinsic Scale (IIS), defined as the largest\nscale where an image exhibits its highest perceived quality. We also present\nthe Image Intrinsic Scale Assessment (IISA) task, which involves subjectively\nmeasuring and predicting the IIS based on human judgments. We develop a\nsubjective annotation methodology and create the IISA-DB dataset, comprising\n785 image-IIS pairs annotated by experts in a rigorously controlled\ncrowdsourcing study. Furthermore, we propose WIISA (Weak-labeling for Image\nIntrinsic Scale Assessment), a strategy that leverages how the IIS of an image\nvaries with downscaling to generate weak labels. Experiments show that applying\nWIISA during the training of several IQA methods adapted for IISA consistently\nimproves the performance compared to using only ground-truth labels. We will\nrelease the code, dataset, and pre-trained models upon acceptance.\n","authors":["Vlad Hosu","Lorenzo Agnolucci","Daisuke Iso","Dietmar Saupe"],"pdf_url":"https://arxiv.org/pdf/2502.06476v1.pdf","comment":null}]},"2025-02-07T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2410.01202v2","updated":"2025-02-07T06:52:01Z","published":"2024-10-02T03:10:38Z","title":"AniSDF: Fused-Granularity Neural Surfaces with Anisotropic Encoding for\n  High-Fidelity 3D Reconstruction","summary":"  Neural radiance fields have recently revolutionized novel-view synthesis and\nachieved high-fidelity renderings. However, these methods sacrifice the\ngeometry for the rendering quality, limiting their further applications\nincluding relighting and deformation. How to synthesize photo-realistic\nrendering while reconstructing accurate geometry remains an unsolved problem.\nIn this work, we present AniSDF, a novel approach that learns fused-granularity\nneural surfaces with physics-based encoding for high-fidelity 3D\nreconstruction. Different from previous neural surfaces, our fused-granularity\ngeometry structure balances the overall structures and fine geometric details,\nproducing accurate geometry reconstruction. To disambiguate geometry from\nreflective appearance, we introduce blended radiance fields to model diffuse\nand specularity following the anisotropic spherical Gaussian encoding, a\nphysics-based rendering pipeline. With these designs, AniSDF can reconstruct\nobjects with complex structures and produce high-quality renderings.\nFurthermore, our method is a unified model that does not require complex\nhyperparameter tuning for specific objects. Extensive experiments demonstrate\nthat our method boosts the quality of SDF-based methods by a great scale in\nboth geometry reconstruction and novel-view synthesis.\n","authors":["Jingnan Gao","Zhuo Chen","Xiaokang Yang","Yichao Yan"],"pdf_url":"https://arxiv.org/pdf/2410.01202v2.pdf","comment":"Accepted by ICLR2025, Project Page:\n  https://g-1nonly.github.io/AniSDF_Website/"}],"Mesh":[{"id":"http://arxiv.org/abs/2411.18311v2","updated":"2025-02-07T23:21:58Z","published":"2024-11-27T13:06:37Z","title":"Neural Surface Priors for Editable Gaussian Splatting","summary":"  In computer graphics and vision, recovering easily modifiable scene\nappearance from image data is crucial for applications such as content\ncreation. We introduce a novel method that integrates 3D Gaussian Splatting\nwith an implicit surface representation, enabling intuitive editing of\nrecovered scenes through mesh manipulation. Starting with a set of input images\nand camera poses, our approach reconstructs the scene surface using a neural\nsigned distance field. This neural surface acts as a geometric prior guiding\nthe training of Gaussian Splatting components, ensuring their alignment with\nthe scene geometry. To facilitate editing, we encode the visual and geometric\ninformation into a lightweight triangle soup proxy. Edits applied to the mesh\nextracted from the neural surface propagate seamlessly through this\nintermediate structure to update the recovered appearance. Unlike previous\nmethods relying on the triangle soup proxy representation, our approach\nsupports a wider range of modifications and fully leverages the mesh topology,\nenabling a more flexible and intuitive editing process. The complete source\ncode for this project can be accessed at:\nhttps://github.com/WJakubowska/NeuralSurfacePriors.\n","authors":["Jakub Szymkowiak","Weronika Jakubowska","Dawid Malarz","Weronika Smolak-Dyżewska","Maciej Zięba","Przemyslaw Musialski","Wojtek Pałubicki","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2411.18311v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.05378v1","updated":"2025-02-07T23:18:08Z","published":"2025-02-07T23:18:08Z","title":"NextBestPath: Efficient 3D Mapping of Unseen Environments","summary":"  This work addresses the problem of active 3D mapping, where an agent must\nfind an efficient trajectory to exhaustively reconstruct a new scene. Previous\napproaches mainly predict the next best view near the agent's location, which\nis prone to getting stuck in local areas. Additionally, existing indoor\ndatasets are insufficient due to limited geometric complexity and inaccurate\nground truth meshes. To overcome these limitations, we introduce a novel\ndataset AiMDoom with a map generator for the Doom video game, enabling to\nbetter benchmark active 3D mapping in diverse indoor environments. Moreover, we\npropose a new method we call next-best-path (NBP), which predicts long-term\ngoals rather than focusing solely on short-sighted views. The model jointly\npredicts accumulated surface coverage gains for long-term goals and obstacle\nmaps, allowing it to efficiently plan optimal paths with a unified model. By\nleveraging online data collection, data augmentation and curriculum learning,\nNBP significantly outperforms state-of-the-art methods on both the existing\nMP3D dataset and our AiMDoom dataset, achieving more efficient mapping in\nindoor environments of varying complexity.\n","authors":["Shiyao Li","Antoine Guédon","Clémentin Boittiaux","Shizhe Chen","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2502.05378v1.pdf","comment":"To appear at ICLR 2025. Project webpage:\n  https://shiyao-li.github.io/nbp/"},{"id":"http://arxiv.org/abs/2502.04615v1","updated":"2025-02-07T02:24:48Z","published":"2025-02-07T02:24:48Z","title":"Neural Clustering for Prefractured Mesh Generation in Real-time Object\n  Destruction","summary":"  Prefracture method is a practical implementation for real-time object\ndestruction that is hardly achievable within performance constraints, but can\nproduce unrealistic results due to its heuristic nature. To mitigate it, we\napproach the clustering of prefractured mesh generation as an unordered\nsegmentation on point cloud data, and propose leveraging the deep neural\nnetwork trained on a physics-based dataset. Our novel paradigm successfully\npredicts the structural weakness of object that have been limited, exhibiting\nready-to-use results with remarkable quality.\n","authors":["Seunghwan Kim","Sunha Park","Seungkyu Lee"],"pdf_url":"https://arxiv.org/pdf/2502.04615v1.pdf","comment":null}],"HDR":[{"id":"http://arxiv.org/abs/2502.05055v1","updated":"2025-02-07T16:24:56Z","published":"2025-02-07T16:24:56Z","title":"Differentiable Mobile Display Photometric Stereo","summary":"  Display photometric stereo uses a display as a programmable light source to\nilluminate a scene with diverse illumination conditions. Recently,\ndifferentiable display photometric stereo (DDPS) demonstrated improved normal\nreconstruction accuracy by using learned display patterns. However, DDPS faced\nlimitations in practicality, requiring a fixed desktop imaging setup using a\npolarization camera and a desktop-scale monitor. In this paper, we propose a\nmore practical physics-based photometric stereo, differentiable mobile display\nphotometric stereo (DMDPS), that leverages a mobile phone consisting of a\ndisplay and a camera. We overcome the limitations of using a mobile device by\ndeveloping a mobile app and method that simultaneously displays patterns and\ncaptures high-quality HDR images. Using this technique, we capture real-world\n3D-printed objects and learn display patterns via a differentiable learning\nprocess. We demonstrate the effectiveness of DMDPS on both a 3D printed dataset\nand a first dataset of fallen leaves. The leaf dataset contains reconstructed\nsurface normals and albedos of fallen leaves that may enable future research\nbeyond computer graphics and vision. We believe that DMDPS takes a step forward\nfor practical physics-based photometric stereo.\n","authors":["Gawoon Ban","Hyeongjun Kim","Seokjun Choi","Seungwoo Yoon","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2502.05055v1.pdf","comment":"9 pages"}],"Deblur":[{"id":"http://arxiv.org/abs/2502.05127v1","updated":"2025-02-07T18:00:36Z","published":"2025-02-07T18:00:36Z","title":"Self-supervised Conformal Prediction for Uncertainty Quantification in\n  Imaging Problems","summary":"  Most image restoration problems are ill-conditioned or ill-posed and hence\ninvolve significant uncertainty. Quantifying this uncertainty is crucial for\nreliably interpreting experimental results, particularly when reconstructed\nimages inform critical decisions and science. However, most existing image\nrestoration methods either fail to quantify uncertainty or provide estimates\nthat are highly inaccurate. Conformal prediction has recently emerged as a\nflexible framework to equip any estimator with uncertainty quantification\ncapabilities that, by construction, have nearly exact marginal coverage. To\nachieve this, conformal prediction relies on abundant ground truth data for\ncalibration. However, in image restoration problems, reliable ground truth data\nis often expensive or not possible to acquire. Also, reliance on ground truth\ndata can introduce large biases in situations of distribution shift between\ncalibration and deployment. This paper seeks to develop a more robust approach\nto conformal prediction for image restoration problems by proposing a\nself-supervised conformal prediction method that leverages Stein's Unbiased\nRisk Estimator (SURE) to self-calibrate itself directly from the observed noisy\nmeasurements, bypassing the need for ground truth. The method is suitable for\nany linear imaging inverse problem that is ill-conditioned, and it is\nespecially powerful when used with modern self-supervised image restoration\ntechniques that can also be trained directly from measurement data. The\nproposed approach is demonstrated through numerical experiments on image\ndenoising and deblurring, where it delivers results that are remarkably\naccurate and comparable to those obtained by supervised conformal prediction\nwith ground truth data.\n","authors":["Jasper M. Everink","Bernardin Tamo Amougou","Marcelo Pereyra"],"pdf_url":"https://arxiv.org/pdf/2502.05127v1.pdf","comment":null}]},"2025-02-05T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2309.16110v2","updated":"2025-02-05T18:05:56Z","published":"2023-09-28T02:23:46Z","title":"Learning Effective NeRFs and SDFs Representations with 3D Generative\n  Adversarial Networks for 3D Object Generation","summary":"  We present a solution for 3D object generation of ICCV 2023 OmniObject3D\nChallenge. In recent years, 3D object generation has made great process and\nachieved promising results, but it remains a challenging task due to the\ndifficulty of generating complex, textured, and high-fidelity results. To\nresolve this problem, we study learning effective NeRFs and SDFs\nrepresentations with 3D Generative Adversarial Networks (GANs) for 3D object\ngeneration. Specifically, inspired by recent works, we use the efficient\ngeometry-aware 3D GANs as the backbone incorporating with label embedding and\ncolor mapping, which enables to train the model on different taxonomies\nsimultaneously. Then, through a decoder, we aggregate the resulting features to\ngenerate Neural Radiance Fields (NeRFs) based representations for rendering\nhigh-fidelity synthetic images. Meanwhile, we optimize Signed Distance\nFunctions (SDFs) to effectively represent objects with 3D meshes. Besides, we\nobserve that this model can be effectively trained with only a few images of\neach object from a variety of classes, instead of using a great number of\nimages per object or training one model per class. With this pipeline, we can\noptimize an effective model for 3D object generation. This solution is among\nthe top 3 in the ICCV 2023 OmniObject3D Challenge.\n","authors":["Zheyuan Yang","Yibo Liu","Guile Wu","Tongtong Cao","Yuan Ren","Yang Liu","Bingbing Liu"],"pdf_url":"https://arxiv.org/pdf/2309.16110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18087v2","updated":"2025-02-05T16:35:14Z","published":"2024-05-28T11:47:12Z","title":"FlowSDF: Flow Matching for Medical Image Segmentation Using Distance\n  Transforms","summary":"  Medical image segmentation plays an important role in accurately identifying\nand isolating regions of interest within medical images. Generative approaches\nare particularly effective in modeling the statistical properties of\nsegmentation masks that are closely related to the respective structures. In\nthis work we introduce FlowSDF, an image-guided conditional flow matching\nframework, designed to represent the signed distance function (SDF), and, in\nturn, to represent an implicit distribution of segmentation masks. The\nadvantage of leveraging the SDF is a more natural distortion when compared to\nthat of binary masks. Through the learning of a vector field associated with\nthe probability path of conditional SDF distributions, our framework enables\naccurate sampling of segmentation masks and the computation of relevant\nstatistical measures. This probabilistic approach also facilitates the\ngeneration of uncertainty maps represented by the variance, thereby supporting\nenhanced robustness in prediction and further analysis. We qualitatively and\nquantitatively illustrate competitive performance of the proposed method on a\npublic nuclei and gland segmentation data set, highlighting its utility in\nmedical image segmentation applications.\n","authors":["Lea Bogensperger","Dominik Narnhofer","Alexander Falk","Konrad Schindler","Thomas Pock"],"pdf_url":"https://arxiv.org/pdf/2405.18087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02891v1","updated":"2025-02-05T05:25:15Z","published":"2025-02-05T05:25:15Z","title":"INST-Sculpt: Interactive Stroke-based Neural SDF Sculpting","summary":"  Recent advances in implicit neural representations have made them a popular\nchoice for modeling 3D geometry, achieving impressive results in tasks such as\nshape representation, reconstruction, and learning priors. However, directly\nediting these representations poses challenges due to the complex relationship\nbetween model weights and surface regions they influence. Among such editing\ntools, sculpting, which allows users to interactively carve or extrude the\nsurface, is a valuable editing operation to the graphics and modeling\ncommunity. While traditional mesh-based tools like ZBrush facilitate fast and\nintuitive edits, a comparable toolkit for sculpting neural SDFs is currently\nlacking. We introduce a framework that enables interactive surface sculpting\nedits directly on neural implicit representations. Unlike previous works\nlimited to spot edits, our approach allows users to perform stroke-based\nmodifications on the fly, ensuring intuitive shape manipulation without\nswitching representations. By employing tubular neighborhoods to sample strokes\nand custom brush profiles, we achieve smooth deformations along user-defined\ncurves, providing precise control over the sculpting process. Our method\ndemonstrates that intricate and versatile edits can be made while preserving\nthe smooth nature of implicit representations.\n","authors":["Fizza Rubab","Yiying Tong"],"pdf_url":"https://arxiv.org/pdf/2502.02891v1.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2309.16110v2","updated":"2025-02-05T18:05:56Z","published":"2023-09-28T02:23:46Z","title":"Learning Effective NeRFs and SDFs Representations with 3D Generative\n  Adversarial Networks for 3D Object Generation","summary":"  We present a solution for 3D object generation of ICCV 2023 OmniObject3D\nChallenge. In recent years, 3D object generation has made great process and\nachieved promising results, but it remains a challenging task due to the\ndifficulty of generating complex, textured, and high-fidelity results. To\nresolve this problem, we study learning effective NeRFs and SDFs\nrepresentations with 3D Generative Adversarial Networks (GANs) for 3D object\ngeneration. Specifically, inspired by recent works, we use the efficient\ngeometry-aware 3D GANs as the backbone incorporating with label embedding and\ncolor mapping, which enables to train the model on different taxonomies\nsimultaneously. Then, through a decoder, we aggregate the resulting features to\ngenerate Neural Radiance Fields (NeRFs) based representations for rendering\nhigh-fidelity synthetic images. Meanwhile, we optimize Signed Distance\nFunctions (SDFs) to effectively represent objects with 3D meshes. Besides, we\nobserve that this model can be effectively trained with only a few images of\neach object from a variety of classes, instead of using a great number of\nimages per object or training one model per class. With this pipeline, we can\noptimize an effective model for 3D object generation. This solution is among\nthe top 3 in the ICCV 2023 OmniObject3D Challenge.\n","authors":["Zheyuan Yang","Yibo Liu","Guile Wu","Tongtong Cao","Yuan Ren","Yang Liu","Bingbing Liu"],"pdf_url":"https://arxiv.org/pdf/2309.16110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02891v1","updated":"2025-02-05T05:25:15Z","published":"2025-02-05T05:25:15Z","title":"INST-Sculpt: Interactive Stroke-based Neural SDF Sculpting","summary":"  Recent advances in implicit neural representations have made them a popular\nchoice for modeling 3D geometry, achieving impressive results in tasks such as\nshape representation, reconstruction, and learning priors. However, directly\nediting these representations poses challenges due to the complex relationship\nbetween model weights and surface regions they influence. Among such editing\ntools, sculpting, which allows users to interactively carve or extrude the\nsurface, is a valuable editing operation to the graphics and modeling\ncommunity. While traditional mesh-based tools like ZBrush facilitate fast and\nintuitive edits, a comparable toolkit for sculpting neural SDFs is currently\nlacking. We introduce a framework that enables interactive surface sculpting\nedits directly on neural implicit representations. Unlike previous works\nlimited to spot edits, our approach allows users to perform stroke-based\nmodifications on the fly, ensuring intuitive shape manipulation without\nswitching representations. By employing tubular neighborhoods to sample strokes\nand custom brush profiles, we achieve smooth deformations along user-defined\ncurves, providing precise control over the sculpting process. Our method\ndemonstrates that intricate and versatile edits can be made while preserving\nthe smooth nature of implicit representations.\n","authors":["Fizza Rubab","Yiying Tong"],"pdf_url":"https://arxiv.org/pdf/2502.02891v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2502.05222v1","updated":"2025-02-05T20:45:00Z","published":"2025-02-05T20:45:00Z","title":"VistaFlow: Photorealistic Volumetric Reconstruction with Dynamic\n  Resolution Management via Q-Learning","summary":"  We introduce VistaFlow, a scalable three-dimensional imaging technique\ncapable of reconstructing fully interactive 3D volumetric images from a set of\n2D photographs. Our model synthesizes novel viewpoints through a differentiable\nrendering system capable of dynamic resolution management on photorealistic 3D\nscenes. We achieve this through the introduction of QuiQ, a novel intermediate\nvideo controller trained through Q-learning to maintain a consistently high\nframerate by adjusting render resolution with millisecond precision. Notably,\nVistaFlow runs natively on integrated CPU graphics, making it viable for mobile\nand entry-level devices while still delivering high-performance rendering.\nVistaFlow bypasses Neural Radiance Fields (NeRFs), using the PlenOctree data\nstructure to render complex light interactions such as reflection and\nsubsurface scattering with minimal hardware requirements. Our model is capable\nof outperforming state-of-the-art methods with novel view synthesis at a\nresolution of 1080p at over 100 frames per second on consumer hardware. By\ntailoring render quality to the capabilities of each device, VistaFlow has the\npotential to improve the efficiency and accessibility of photorealistic 3D\nscene rendering across a wide spectrum of hardware, from high-end workstations\nto inexpensive microcontrollers.\n","authors":["Jayram Palamadai","William Yu"],"pdf_url":"https://arxiv.org/pdf/2502.05222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16110v2","updated":"2025-02-05T18:05:56Z","published":"2023-09-28T02:23:46Z","title":"Learning Effective NeRFs and SDFs Representations with 3D Generative\n  Adversarial Networks for 3D Object Generation","summary":"  We present a solution for 3D object generation of ICCV 2023 OmniObject3D\nChallenge. In recent years, 3D object generation has made great process and\nachieved promising results, but it remains a challenging task due to the\ndifficulty of generating complex, textured, and high-fidelity results. To\nresolve this problem, we study learning effective NeRFs and SDFs\nrepresentations with 3D Generative Adversarial Networks (GANs) for 3D object\ngeneration. Specifically, inspired by recent works, we use the efficient\ngeometry-aware 3D GANs as the backbone incorporating with label embedding and\ncolor mapping, which enables to train the model on different taxonomies\nsimultaneously. Then, through a decoder, we aggregate the resulting features to\ngenerate Neural Radiance Fields (NeRFs) based representations for rendering\nhigh-fidelity synthetic images. Meanwhile, we optimize Signed Distance\nFunctions (SDFs) to effectively represent objects with 3D meshes. Besides, we\nobserve that this model can be effectively trained with only a few images of\neach object from a variety of classes, instead of using a great number of\nimages per object or training one model per class. With this pipeline, we can\noptimize an effective model for 3D object generation. This solution is among\nthe top 3 in the ICCV 2023 OmniObject3D Challenge.\n","authors":["Zheyuan Yang","Yibo Liu","Guile Wu","Tongtong Cao","Yuan Ren","Yang Liu","Bingbing Liu"],"pdf_url":"https://arxiv.org/pdf/2309.16110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11085v3","updated":"2025-02-05T10:25:44Z","published":"2024-08-20T17:58:23Z","title":"GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting","summary":"  We leverage 3D Gaussian Splatting (3DGS) as a scene representation and\npropose a novel test-time camera pose refinement (CPR) framework, GS-CPR. This\nframework enhances the localization accuracy of state-of-the-art absolute pose\nregression and scene coordinate regression methods. The 3DGS model renders\nhigh-quality synthetic images and depth maps to facilitate the establishment of\n2D-3D correspondences. GS-CPR obviates the need for training feature extractors\nor descriptors by operating directly on RGB images, utilizing the 3D foundation\nmodel, MASt3R, for precise 2D matching. To improve the robustness of our model\nin challenging outdoor environments, we incorporate an exposure-adaptive module\nwithin the 3DGS framework. Consequently, GS-CPR enables efficient one-shot pose\nrefinement given a single RGB query and a coarse initial pose estimation. Our\nproposed approach surpasses leading NeRF-based optimization methods in both\naccuracy and runtime across indoor and outdoor visual localization benchmarks,\nachieving new state-of-the-art accuracy on two indoor datasets. The project\npage is available at https://gsloc.active.vision.\n","authors":["Changkun Liu","Shuai Chen","Yash Bhalgat","Siyan Hu","Ming Cheng","Zirui Wang","Victor Adrian Prisacariu","Tristan Braud"],"pdf_url":"https://arxiv.org/pdf/2408.11085v3.pdf","comment":"Accepted to ICLR2025. During the ICLR review process, we changed the\n  name of our framework from GSLoc to GS-CPR (Camera Pose Refinement) according\n  to the comments of reviewers. The project page is available at\n  https://gsloc.active.vision"},{"id":"http://arxiv.org/abs/2501.13971v2","updated":"2025-02-05T09:12:56Z","published":"2025-01-22T11:21:20Z","title":"GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic\n  Gaussian Splatting","summary":"  LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR\nsimulation, offering valuable simulated point cloud data from novel viewpoints\nto aid in autonomous driving systems. However, existing LiDAR NVS methods\ntypically rely on neural radiance fields (NeRF) as their 3D representation,\nwhich incurs significant computational costs in both training and rendering.\nMoreover, NeRF and its variants are designed for symmetrical scenes, making\nthem ill-suited for driving scenarios. To address these challenges, we propose\nGS-LiDAR, a novel framework for generating realistic LiDAR point clouds with\npanoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with\nperiodic vibration properties, allowing for precise geometric reconstruction of\nboth static and dynamic elements in driving scenarios. We further introduce a\nnovel panoramic rendering technique with explicit ray-splat intersection,\nguided by panoramic LiDAR supervision. By incorporating intensity and ray-drop\nspherical harmonic (SH) coefficients into the Gaussian primitives, we enhance\nthe realism of the rendered point clouds. Extensive experiments on KITTI-360\nand nuScenes demonstrate the superiority of our method in terms of quantitative\nmetrics, visual quality, as well as training and rendering efficiency.\n","authors":["Junzhe Jiang","Chun Gu","Yurui Chen","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.13971v2.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2405.19097v3","updated":"2025-02-05T15:01:02Z","published":"2024-05-29T14:01:40Z","title":"A study of why we need to reassess full reference image quality\n  assessment with medical images","summary":"  Image quality assessment (IQA) is indispensable in clinical practice to\nensure high standards, as well as in the development stage of machine learning\nalgorithms that operate on medical images. The popular full reference (FR) IQA\nmeasures PSNR and SSIM are known and tested for working successfully in many\nnatural imaging tasks, but discrepancies in medical scenarios have been\nreported in the literature, highlighting the gap between development and actual\nclinical application. Such inconsistencies are not surprising, as medical\nimages have very different properties than natural images, and PSNR and SSIM\nhave neither been targeted nor properly tested for medical images. This may\ncause unforeseen problems in clinical applications due to wrong judgment of\nnovel methods. This paper provides a structured and comprehensive overview of\nexamples where PSNR and SSIM prove to be unsuitable for the assessment of novel\nalgorithms using different kinds of medical images, including real-world MRI,\nCT, OCT, X-Ray, digital pathology and photoacoustic imaging data. Therefore,\nimprovement is urgently needed in particular in this era of AI to increase\nreliability and explainability in machine learning for medical imaging and\nbeyond. Lastly, we will provide ideas for future research as well as suggesting\nguidelines for the usage of FR-IQA measures applied to medical images.\n","authors":["Anna Breger","Ander Biguri","Malena Sabaté Landman","Ian Selby","Nicole Amberg","Elisabeth Brunner","Janek Gröhl","Sepideh Hatamikia","Clemens Karner","Lipeng Ning","Sören Dittmer","Michael Roberts","AIX-COVNET Collaboration","Carola-Bibiane Schönlieb"],"pdf_url":"https://arxiv.org/pdf/2405.19097v3.pdf","comment":null}]},"2025-02-04T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2502.02187v1","updated":"2025-02-04T10:02:40Z","published":"2025-02-04T10:02:40Z","title":"ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel\n  Diffusion","summary":"  This paper proposes ShapeShifter, a new 3D generative model that learns to\nsynthesize shape variations based on a single reference model. While generative\nmethods for 3D objects have recently attracted much attention, current\ntechniques often lack geometric details and/or require long training times and\nlarge resources. Our approach remedies these issues by combining sparse voxel\ngrids and point, normal, and color sampling within a multiscale neural\narchitecture that can be trained efficiently and in parallel. We show that our\nresulting variations better capture the fine details of their original input\nand can handle more general types of surfaces than previous SDF-based methods.\nMoreover, we offer interactive generation of 3D shape variants, allowing more\nhuman control in the design loop if needed.\n","authors":["Nissim Maruani","Wang Yifan","Matthew Fisher","Pierre Alliez","Mathieu Desbrun"],"pdf_url":"https://arxiv.org/pdf/2502.02187v1.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2502.02590v1","updated":"2025-02-04T18:59:55Z","published":"2025-02-04T18:59:55Z","title":"Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling","summary":"  3D articulated objects modeling has long been a challenging problem, since it\nrequires to capture both accurate surface geometries and semantically\nmeaningful and spatially precise structures, parts, and joints. Existing\nmethods heavily depend on training data from a limited set of handcrafted\narticulated object categories (e.g., cabinets and drawers), which restricts\ntheir ability to model a wide range of articulated objects in an\nopen-vocabulary context. To address these limitations, we propose Articulate\nAnymesh, an automated framework that is able to convert any rigid 3D mesh into\nits articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our\nframework utilizes advanced Vision-Language Models and visual prompting\ntechniques to extract semantic information, allowing for both the segmentation\nof object parts and the construction of functional joints. Our experiments show\nthat Articulate Anymesh can generate large-scale, high-quality 3D articulated\nobjects, including tools, toys, mechanical devices, and vehicles, significantly\nexpanding the coverage of existing 3D articulated object datasets.\nAdditionally, we show that these generated assets can facilitate the\nacquisition of new articulated object manipulation skills in simulation, which\ncan then be transferred to a real robotic system. Our Github website is\nhttps://articulate-anymesh.github.io.\n","authors":["Xiaowen Qiu","Jincheng Yang","Yian Wang","Zhehuan Chen","Yufei Wang","Tsun-Hsuan Wang","Zhou Xian","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2502.02590v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2408.03356v2","updated":"2025-02-04T22:56:58Z","published":"2024-08-06T10:59:58Z","title":"RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel\n  View Synthesis","summary":"  Differentiable volumetric rendering-based methods made significant progress\nin novel view synthesis. On one hand, innovative methods have replaced the\nNeural Radiance Fields (NeRF) network with locally parameterized structures,\nenabling high-quality renderings in a reasonable time. On the other hand,\napproaches have used differentiable splatting instead of NeRF's ray casting to\noptimize radiance fields rapidly using Gaussian kernels, allowing for fine\nadaptation to the scene. However, differentiable ray casting of irregularly\nspaced kernels has been scarcely explored, while splatting, despite enabling\nfast rendering times, is susceptible to clearly visible artifacts.\n  Our work closes this gap by providing a physically consistent formulation of\nthe emitted radiance c and density {\\sigma}, decomposed with Gaussian functions\nassociated with Spherical Gaussians/Harmonics for all-frequency colorimetric\nrepresentation. We also introduce a method enabling differentiable ray casting\nof irregularly distributed Gaussians using an algorithm that integrates\nradiance fields slab by slab and leverages a BVH structure. This allows our\napproach to finely adapt to the scene while avoiding splatting artifacts. As a\nresult, we achieve superior rendering quality compared to the state-of-the-art\nwhile maintaining reasonable training times and achieving inference speeds of\n25 FPS on the Blender dataset. Project page with videos and code:\nhttps://raygauss.github.io/\n","authors":["Hugo Blanc","Jean-Emmanuel Deschaud","Alexis Paljic"],"pdf_url":"https://arxiv.org/pdf/2408.03356v2.pdf","comment":"Project page with videos and code: https://raygauss.github.io/"},{"id":"http://arxiv.org/abs/2502.02657v1","updated":"2025-02-04T19:00:49Z","published":"2025-02-04T19:00:49Z","title":"SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with\n  Uncertainty Quantification","summary":"  We present a neural radiance field (NeRF) based large-scale reconstruction\nsystem that fuses lidar and vision data to generate high-quality\nreconstructions that are geometrically accurate and capture photorealistic\ntexture. Our system adopts the state-of-the-art NeRF representation to\nadditionally incorporate lidar. Adding lidar data adds strong geometric\nconstraints on the depth and surface normals, which is particularly useful when\nmodelling uniform texture surfaces which contain ambiguous visual\nreconstruction cues. Furthermore, we estimate the epistemic uncertainty of the\nreconstruction as the spatial variance of each point location in the radiance\nfield given the sensor observations from camera and lidar. This enables the\nidentification of areas that are reliably reconstructed by each sensor\nmodality, allowing the map to be filtered according to the estimated\nuncertainty. Our system can also exploit the trajectory produced by a real-time\npose-graph lidar SLAM system during online mapping to bootstrap a\n(post-processed) Structure-from-Motion (SfM) reconstruction procedure reducing\nSfM training time by up to 70%. It also helps to properly constrain the overall\nmetric scale which is essential for the lidar depth loss. The\nglobally-consistent trajectory can then be divided into submaps using Spectral\nClustering to group sets of co-visible images together. This submapping\napproach is more suitable for visual reconstruction than distance-based\npartitioning. Each submap is filtered according to point-wise uncertainty\nestimates and merged to obtain the final large-scale 3D reconstruction. We\ndemonstrate the reconstruction system using a multi-camera, lidar sensor suite\nin experiments involving both robot-mounted and handheld scanning. Our test\ndatasets cover a total area of more than 20,000 square metres, including\nmultiple university buildings and an aerial survey of a multi-storey.\n","authors":["Yifu Tao","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2502.02657v1.pdf","comment":"webpage: https://dynamic.robots.ox.ac.uk/projects/silvr/"},{"id":"http://arxiv.org/abs/2502.02372v1","updated":"2025-02-04T14:52:34Z","published":"2025-02-04T14:52:34Z","title":"MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by\n  Continual Learning","summary":"  The generation of a virtual digital avatar is a crucial research topic in the\nfield of computer vision. Many existing works utilize Neural Radiance Fields\n(NeRF) to address this issue and have achieved impressive results. However,\nprevious works assume the images of the training person are available and fixed\nwhile the appearances and poses of a subject could constantly change and\nincrease in real-world scenarios. How to update the human avatar but also\nmaintain the ability to render the old appearance of the person is a practical\nchallenge. One trivial solution is to combine the existing virtual avatar\nmodels based on NeRF with continual learning methods. However, there are some\ncritical issues in this approach: learning new appearances and poses can cause\nthe model to forget past information, which in turn leads to a degradation in\nthe rendering quality of past appearances, especially color bleeding issues,\nand incorrect human body poses. In this work, we propose a maintainable avatar\n(MaintaAvatar) based on neural radiance fields by continual learning, which\nresolves the issues by utilizing a Global-Local Joint Storage Module and a Pose\nDistillation Module. Overall, our model requires only limited data collection\nto quickly fine-tune the model while avoiding catastrophic forgetting, thus\nachieving a maintainable virtual avatar. The experimental results validate the\neffectiveness of our MaintaAvatar model.\n","authors":["Shengbo Gu","Yu-Kun Qiu","Yu-Ming Tang","Ancong Wu","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.02372v1.pdf","comment":"AAAI 2025. 9 pages"}]},"2025-02-01T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2502.00360v1","updated":"2025-02-01T07:51:59Z","published":"2025-02-01T07:51:59Z","title":"Shape from Semantics: 3D Shape Generation from Multi-View Semantics","summary":"  We propose ``Shape from Semantics'', which is able to create 3D models whose\ngeometry and appearance match given semantics when observed from different\nviews. Traditional ``Shape from X'' tasks usually use visual input (e.g., RGB\nimages or depth maps) to reconstruct geometry, imposing strict constraints that\nlimit creative explorations. As applications, works like Shadow Art and Wire\nArt often struggle to grasp the embedded semantics of their design through\ndirect observation and rely heavily on specific setups for proper display. To\naddress these limitations, our framework uses semantics as input, greatly\nexpanding the design space to create objects that integrate multiple semantic\nelements and are easily discernible by observers. Considering that this task\nrequires a rich imagination, we adopt various generative models and\nstructure-to-detail pipelines. Specifically, we adopt multi-semantics Score\nDistillation Sampling (SDS) to distill 3D geometry and appearance from 2D\ndiffusion models, ensuring that the initial shape is consistent with the\nsemantic input. We then use image restoration and video generation models to\nadd more details as supervision. Finally, we introduce neural signed distance\nfield (SDF) representation to achieve detailed shape reconstruction. Our\nframework generates meshes with complex details, well-structured geometry,\ncoherent textures, and smooth transitions, resulting in visually appealing and\neye-catching designs. Project page: https://shapefromsemantics.github.io\n","authors":["Liangchen Li","Caoliwen Wang","Yuqi Zhou","Bailin Deng","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00360v1.pdf","comment":"Project page: https://shapefromsemantics.github.io"}],"Mesh":[{"id":"http://arxiv.org/abs/2502.00360v1","updated":"2025-02-01T07:51:59Z","published":"2025-02-01T07:51:59Z","title":"Shape from Semantics: 3D Shape Generation from Multi-View Semantics","summary":"  We propose ``Shape from Semantics'', which is able to create 3D models whose\ngeometry and appearance match given semantics when observed from different\nviews. Traditional ``Shape from X'' tasks usually use visual input (e.g., RGB\nimages or depth maps) to reconstruct geometry, imposing strict constraints that\nlimit creative explorations. As applications, works like Shadow Art and Wire\nArt often struggle to grasp the embedded semantics of their design through\ndirect observation and rely heavily on specific setups for proper display. To\naddress these limitations, our framework uses semantics as input, greatly\nexpanding the design space to create objects that integrate multiple semantic\nelements and are easily discernible by observers. Considering that this task\nrequires a rich imagination, we adopt various generative models and\nstructure-to-detail pipelines. Specifically, we adopt multi-semantics Score\nDistillation Sampling (SDS) to distill 3D geometry and appearance from 2D\ndiffusion models, ensuring that the initial shape is consistent with the\nsemantic input. We then use image restoration and video generation models to\nadd more details as supervision. Finally, we introduce neural signed distance\nfield (SDF) representation to achieve detailed shape reconstruction. Our\nframework generates meshes with complex details, well-structured geometry,\ncoherent textures, and smooth transitions, resulting in visually appealing and\neye-catching designs. Project page: https://shapefromsemantics.github.io\n","authors":["Liangchen Li","Caoliwen Wang","Yuqi Zhou","Bailin Deng","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00360v1.pdf","comment":"Project page: https://shapefromsemantics.github.io"}],"Deblur":[{"id":"http://arxiv.org/abs/2412.10338v2","updated":"2025-02-01T02:31:31Z","published":"2024-12-13T18:33:18Z","title":"XYScanNet: An Interpretable State Space Model for Perceptual Image\n  Deblurring","summary":"  Deep state-space models (SSMs), like recent Mamba architectures, are emerging\nas a promising alternative to CNN and Transformer networks. Existing\nMamba-based restoration methods process the visual data by leveraging a\nflatten-and-scan strategy that converts image patches into a 1D sequence before\nscanning. However, this scanning paradigm ignores local pixel dependencies and\nintroduces spatial misalignment by positioning distant pixels incorrectly\nadjacent, which reduces local noise-awareness and degrades image sharpness in\nlow-level vision tasks. To overcome these issues, we propose a novel\nslice-and-scan strategy that alternates scanning along intra- and inter-slices.\nWe further design a new Vision State Space Module (VSSM) for image deblurring,\nand tackle the inefficiency challenges of the current Mamba-based vision\nmodule. Building upon this, we develop XYScanNet, an SSM architecture\nintegrated with a lightweight feature fusion module for enhanced image\ndeblurring. XYScanNet, maintains competitive distortion metrics and\nsignificantly improves perceptual performance. Experimental results show that\nXYScanNet enhances KID by $17\\%$ compared to the nearest competitor. Our code\nwill be released soon.\n","authors":["Hanzhou Liu","Chengkai Liu","Jiacong Xu","Peng Jiang","Mi Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10338v2.pdf","comment":null}]},"2025-01-31T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2312.14140v2","updated":"2025-01-31T17:44:14Z","published":"2023-12-21T18:57:52Z","title":"HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs","summary":"  Current advances in human head modeling allow the generation of\nplausible-looking 3D head models via neural representations, such as NeRFs and\nSDFs. Nevertheless, constructing complete high-fidelity head models with\nexplicitly controlled animation remains an issue. Furthermore, completing the\nhead geometry based on a partial observation, e.g., coming from a depth sensor,\nwhile preserving a high level of detail is often problematic for the existing\nmethods. We introduce a generative model for detailed 3D head meshes on top of\nan articulated 3DMM, simultaneously allowing explicit animation and high-detail\npreservation. Our method is trained in two stages. First, we register a\nparametric head model with vertex displacements to each mesh of the recently\nintroduced NPHM dataset of accurate 3D head scans. The estimated displacements\nare baked into a hand-crafted UV layout. Second, we train a StyleGAN model to\ngeneralize over the UV maps of displacements, which we later refer to as\nHeadCraft. The decomposition of the parametric model and high-quality vertex\ndisplacements allows us to animate the model and modify the regions\nsemantically. We demonstrate the results of unconditional sampling, fitting to\na scan and editing. The project page is available at\nhttps://seva100.github.io/headcraft.\n","authors":["Artem Sevastopolsky","Philip-William Grassal","Simon Giebenhain","ShahRukh Athar","Luisa Verdoliva","Matthias Niessner"],"pdf_url":"https://arxiv.org/pdf/2312.14140v2.pdf","comment":"2nd version includes updated method and results. Project page:\n  https://seva100.github.io/headcraft. Video: https://youtu.be/uBeBT2f1CL0. 24\n  pages, 21 figures, 3 tables"}],"Mesh":[{"id":"http://arxiv.org/abs/2312.14140v2","updated":"2025-01-31T17:44:14Z","published":"2023-12-21T18:57:52Z","title":"HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs","summary":"  Current advances in human head modeling allow the generation of\nplausible-looking 3D head models via neural representations, such as NeRFs and\nSDFs. Nevertheless, constructing complete high-fidelity head models with\nexplicitly controlled animation remains an issue. Furthermore, completing the\nhead geometry based on a partial observation, e.g., coming from a depth sensor,\nwhile preserving a high level of detail is often problematic for the existing\nmethods. We introduce a generative model for detailed 3D head meshes on top of\nan articulated 3DMM, simultaneously allowing explicit animation and high-detail\npreservation. Our method is trained in two stages. First, we register a\nparametric head model with vertex displacements to each mesh of the recently\nintroduced NPHM dataset of accurate 3D head scans. The estimated displacements\nare baked into a hand-crafted UV layout. Second, we train a StyleGAN model to\ngeneralize over the UV maps of displacements, which we later refer to as\nHeadCraft. The decomposition of the parametric model and high-quality vertex\ndisplacements allows us to animate the model and modify the regions\nsemantically. We demonstrate the results of unconditional sampling, fitting to\na scan and editing. The project page is available at\nhttps://seva100.github.io/headcraft.\n","authors":["Artem Sevastopolsky","Philip-William Grassal","Simon Giebenhain","ShahRukh Athar","Luisa Verdoliva","Matthias Niessner"],"pdf_url":"https://arxiv.org/pdf/2312.14140v2.pdf","comment":"2nd version includes updated method and results. Project page:\n  https://seva100.github.io/headcraft. Video: https://youtu.be/uBeBT2f1CL0. 24\n  pages, 21 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.19196v1","updated":"2025-01-31T15:05:06Z","published":"2025-01-31T15:05:06Z","title":"RaySplats: Ray Tracing based Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) is a process that enables the direct creation of\n3D objects from 2D images. This representation offers numerous advantages,\nincluding rapid training and rendering. However, a significant limitation of\n3DGS is the challenge of incorporating light and shadow reflections, primarily\ndue to the utilization of rasterization rather than ray tracing for rendering.\nThis paper introduces RaySplats, a model that employs ray-tracing based\nGaussian Splatting. Rather than utilizing the projection of Gaussians, our\nmethod employs a ray-tracing mechanism, operating directly on Gaussian\nprimitives represented by confidence ellipses with RGB colors. In practice, we\ncompute the intersection between ellipses and rays to construct ray-tracing\nalgorithms, facilitating the incorporation of meshes with Gaussian Splatting\nmodels and the addition of lights, shadows, and other related effects.\n","authors":["Krzysztof Byrski","Marcin Mazur","Jacek Tabor","Tadeusz Dziarmaga","Marcin Kądziołka","Dawid Baran","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2501.19196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19088v1","updated":"2025-01-31T12:33:24Z","published":"2025-01-31T12:33:24Z","title":"JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting","summary":"  Since hands are the primary interface in daily interactions, modeling\nhigh-quality digital human hands and rendering realistic images is a critical\nresearch problem. Furthermore, considering the requirements of interactive and\nrendering applications, it is essential to achieve real-time rendering and\ndriveability of the digital model without compromising rendering quality. Thus,\nwe propose Jointly 3D Gaussian Hand (JGHand), a novel joint-driven 3D Gaussian\nSplatting (3DGS)-based hand representation that renders high-fidelity hand\nimages in real-time for various poses and characters. Distinct from existing\narticulated neural rendering techniques, we introduce a differentiable process\nfor spatial transformations based on 3D key points. This process supports\ndeformations from the canonical template to a mesh with arbitrary bone lengths\nand poses. Additionally, we propose a real-time shadow simulation method based\non per-pixel depth to simulate self-occlusion shadows caused by finger\nmovements. Finally, we embed the hand prior and propose an animatable 3DGS\nrepresentation of the hand driven solely by 3D key points. We validate the\neffectiveness of each component of our approach through comprehensive ablation\nstudies. Experimental results on public datasets demonstrate that JGHand\nachieves real-time rendering speeds with enhanced quality, surpassing\nstate-of-the-art methods.\n","authors":["Zhoutao Sun","Xukun Shen","Yong Hu","Yuyou Zhong","Xueyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.19088v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2312.14140v2","updated":"2025-01-31T17:44:14Z","published":"2023-12-21T18:57:52Z","title":"HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs","summary":"  Current advances in human head modeling allow the generation of\nplausible-looking 3D head models via neural representations, such as NeRFs and\nSDFs. Nevertheless, constructing complete high-fidelity head models with\nexplicitly controlled animation remains an issue. Furthermore, completing the\nhead geometry based on a partial observation, e.g., coming from a depth sensor,\nwhile preserving a high level of detail is often problematic for the existing\nmethods. We introduce a generative model for detailed 3D head meshes on top of\nan articulated 3DMM, simultaneously allowing explicit animation and high-detail\npreservation. Our method is trained in two stages. First, we register a\nparametric head model with vertex displacements to each mesh of the recently\nintroduced NPHM dataset of accurate 3D head scans. The estimated displacements\nare baked into a hand-crafted UV layout. Second, we train a StyleGAN model to\ngeneralize over the UV maps of displacements, which we later refer to as\nHeadCraft. The decomposition of the parametric model and high-quality vertex\ndisplacements allows us to animate the model and modify the regions\nsemantically. We demonstrate the results of unconditional sampling, fitting to\na scan and editing. The project page is available at\nhttps://seva100.github.io/headcraft.\n","authors":["Artem Sevastopolsky","Philip-William Grassal","Simon Giebenhain","ShahRukh Athar","Luisa Verdoliva","Matthias Niessner"],"pdf_url":"https://arxiv.org/pdf/2312.14140v2.pdf","comment":"2nd version includes updated method and results. Project page:\n  https://seva100.github.io/headcraft. Video: https://youtu.be/uBeBT2f1CL0. 24\n  pages, 21 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.17978v2","updated":"2025-01-31T12:35:35Z","published":"2025-01-29T20:23:48Z","title":"VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting","summary":"  Reconstructing a 3D scene from images is challenging due to the different\nways light interacts with surfaces depending on the viewer's position and the\nsurface's material. In classical computer graphics, materials can be classified\nas diffuse or specular, interacting with light differently. The standard 3D\nGaussian Splatting model struggles to represent view-dependent content, since\nit cannot differentiate an object within the scene from the light interacting\nwith its specular surfaces, which produce highlights or reflections. In this\npaper, we propose to extend the 3D Gaussian Splatting model by introducing an\nadditional symmetric matrix to enhance the opacity representation of each 3D\nGaussian. This improvement allows certain Gaussians to be suppressed based on\nthe viewer's perspective, resulting in a more accurate representation of\nview-dependent reflections and specular highlights without compromising the\nscene's integrity. By allowing the opacity to be view dependent, our enhanced\nmodel achieves state-of-the-art performance on Mip-Nerf, Tanks&Temples, Deep\nBlending, and Nerf-Synthetic datasets without a significant loss in rendering\nspeed, achieving >60FPS, and only incurring a minimal increase in memory used.\n","authors":["Mateusz Nowak","Wojciech Jarosz","Peter Chin"],"pdf_url":"https://arxiv.org/pdf/2501.17978v2.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2412.13443v2","updated":"2025-01-31T18:52:04Z","published":"2024-12-18T02:31:37Z","title":"DarkIR: Robust Low-Light Image Restoration","summary":"  Photography during night or in dark conditions typically suffers from noise,\nlow light and blurring issues due to the dim environment and the common use of\nlong exposure. Although Deblurring and Low-light Image Enhancement (LLIE) are\nrelated under these conditions, most approaches in image restoration solve\nthese tasks separately. In this paper, we present an efficient and robust\nneural network for multi-task low-light image restoration. Instead of following\nthe current tendency of Transformer-based models, we propose new attention\nmechanisms to enhance the receptive field of efficient CNNs. Our method reduces\nthe computational costs in terms of parameters and MAC operations compared to\nprevious methods. Our model, DarkIR, achieves new state-of-the-art results on\nthe popular LOLBlur, LOLv2 and Real-LOLBlur datasets, being able to generalize\non real-world night and dark images. Code and models at\nhttps://github.com/cidautai/DarkIR\n","authors":["Daniel Feijoo","Juan C. Benito","Alvaro Garcia","Marcos V. Conde"],"pdf_url":"https://arxiv.org/pdf/2412.13443v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2501.19386v1","updated":"2025-01-31T18:39:47Z","published":"2025-01-31T18:39:47Z","title":"Multi-Frame Blind Manifold Deconvolution for Rotating Synthetic Aperture\n  Imaging","summary":"  Rotating synthetic aperture (RSA) imaging system captures images of the\ntarget scene at different rotation angles by rotating a rectangular aperture.\nDeblurring acquired RSA images plays a critical role in reconstructing a latent\nsharp image underlying the scene. In the past decade, the emergence of blind\nconvolution technology has revolutionised this field by its ability to model\ncomplex features from acquired images. Most of the existing methods attempt to\nsolve the above ill-posed inverse problem through maximising a posterior.\n  Despite this progress, researchers have paid limited attention to exploring\nlow-dimensional manifold structures of the latent image within a\nhigh-dimensional ambient-space. Here, we propose a novel method to process RSA\nimages using manifold fitting and penalisation in the content of multi-frame\nblind convolution. We develop fast algorithms for implementing the proposed\nprocedure. Simulation studies demonstrate that manifold-based deconvolution can\noutperform conventional deconvolution algorithms in the sense that it can\ngenerate a sharper estimate of the latent image in terms of estimating pixel\nintensities and preserving structural details.\n","authors":["Dao Lin","Jian Zhang","Martin Benning"],"pdf_url":"https://arxiv.org/pdf/2501.19386v1.pdf","comment":"39 pages, 9 figures"}]},"2025-01-23T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2404.00409v2","updated":"2025-01-23T16:23:37Z","published":"2024-03-30T16:35:38Z","title":"3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting","summary":"  In this paper, we present an implicit surface reconstruction method with 3D\nGaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D\nreconstruction with intricate details while inheriting the high efficiency and\nrendering quality of 3DGS. The key insight is incorporating an implicit signed\ndistance field (SDF) within 3D Gaussians to enable them to be aligned and\njointly optimized. First, we introduce a differentiable SDF-to-opacity\ntransformation function that converts SDF values into corresponding Gaussians'\nopacities. This function connects the SDF and 3D Gaussians, allowing for\nunified optimization and enforcing surface constraints on the 3D Gaussians.\nDuring learning, optimizing the 3D Gaussians provides supervisory signals for\nSDF learning, enabling the reconstruction of intricate details. However, this\nonly provides sparse supervisory signals to the SDF at locations occupied by\nGaussians, which is insufficient for learning a continuous SDF. Then, to\naddress this limitation, we incorporate volumetric rendering and align the\nrendered geometric attributes (depth, normal) with those derived from 3D\nGaussians. This consistency regularization introduces supervisory signals to\nlocations not covered by discrete 3D Gaussians, effectively eliminating\nredundant surfaces outside the Gaussian sampling range. Our extensive\nexperimental results demonstrate that our 3DGSR method enables high-quality 3D\nsurface reconstruction while preserving the efficiency and rendering quality of\n3DGS. Besides, our method competes favorably with leading surface\nreconstruction techniques while offering a more efficient learning process and\nmuch better rendering qualities. The code will be available at\nhttps://github.com/CVMI-Lab/3DGSR.\n","authors":["Xiaoyang Lyu","Yang-Tian Sun","Yi-Hua Huang","Xiuzhe Wu","Ziyi Yang","Yilun Chen","Jiangmiao Pang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2404.00409v2.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2309.12397v2","updated":"2025-01-23T22:17:17Z","published":"2023-09-21T18:00:34Z","title":"POLAR-Sim: Augmenting NASA's POLAR Dataset for Data-Driven Lunar\n  Perception and Rover Simulation","summary":"  NASA's POLAR dataset contains approximately 2,600 pairs of high dynamic range\nstereo photos captured across 13 varied terrain scenarios, including areas with\nsparse or dense rock distributions, craters, and rocks of different sizes. The\npurpose of these photos is to spur development in robotics, AI-based\nperception, and autonomous navigation. Acknowledging a scarcity of lunar images\nfrom around the lunar poles, NASA Ames produced on Earth but in controlled\nconditions images that resemble rover operating conditions from these regions\nof the Moon. We report on the outcomes of an effort aimed at accomplishing two\ntasks. In Task 1, we provided bounding boxes and semantic segmentation\ninformation for all the images in NASA's POLAR dataset. This effort resulted in\n23,000 labels and semantic segmentation annotations pertaining to rocks,\nshadows, and craters. In Task 2, we generated the digital twins of the 13\nscenarios that have been used to produce all the photos in the POLAR dataset.\nSpecifically, for each of these scenarios, we produced individual meshes,\ntexture information, and material properties associated with the ground and the\nrocks in each scenario. This allows anyone with a camera model to synthesize\nimages associated with any of the 13 scenarios of the POLAR dataset.\nEffectively, one can generate as many semantically labeled synthetic images as\ndesired -- with different locations and exposure values in the scene, for\ndifferent positions of the sun, with or without the presence of active\nillumination, etc. The benefit of this work is twofold. Using outcomes of Task\n1, one can train and/or test perception algorithms that deal with Moon images.\nFor Task 2, one can produce as much data as desired to train and test AI\nalgorithms that are anticipated to work in lunar conditions. All the outcomes\nof this work are available in a public repository for unfettered use and\ndistribution.\n","authors":["Bo-Hsun Chen","Peter Negrut","Thomas Liang","Nevindu Batagoda","Harry Zhang","Dan Negrut"],"pdf_url":"https://arxiv.org/pdf/2309.12397v2.pdf","comment":"11 pages, 9 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2501.09046v2","updated":"2025-01-23T14:18:07Z","published":"2025-01-15T09:52:40Z","title":"Learning Hemodynamic Scalar Fields on Coronary Artery Meshes: A\n  Benchmark of Geometric Deep Learning Models","summary":"  Coronary artery disease, caused by the narrowing of coronary vessels due to\natherosclerosis, is the leading cause of death worldwide. The diagnostic gold\nstandard, fractional flow reserve (FFR), measures the trans-stenotic pressure\nratio during maximal vasodilation but is invasive and costly. This has driven\nthe development of virtual FFR (vFFR) using computational fluid dynamics (CFD)\nto simulate coronary flow. Geometric deep learning algorithms have shown\npromise for learning features on meshes, including cardiovascular research\napplications. This study empirically analyzes various backends for predicting\nvFFR fields in coronary arteries as CFD surrogates, comparing six backends for\nlearning hemodynamics on meshes using CFD solutions as ground truth.\n  The study has two parts: i) Using 1,500 synthetic left coronary artery\nbifurcations, models were trained to predict pressure-related fields for vFFR\nreconstruction, comparing different learning variables. ii) Using 427\npatient-specific CFD simulations, experiments were repeated focusing on the\nbest-performing learning variable from the synthetic dataset.\n  Most backends performed well on the synthetic dataset, especially when\npredicting pressure drop over the manifold. Transformer-based backends\noutperformed others when predicting pressure and vFFR fields and were the only\nmodels achieving strong performance on patient-specific data, excelling in both\naverage per-point error and vFFR accuracy in stenotic lesions.\n  These results suggest geometric deep learning backends can effectively\nreplace CFD for simple geometries, while transformer-based networks are\nsuperior for complex, heterogeneous datasets. Pressure drop was identified as\nthe optimal network output for learning pressure-related fields.\n","authors":["Guido Nannini","Julian Suk","Patryk Rygiel","Simone Saitta","Luca Mariani","Riccardo Maragna","Andrea Baggiano","Gianluca Pontone","Jelmer M. Wolterink","Alberto Redaelli"],"pdf_url":"https://arxiv.org/pdf/2501.09046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09600v4","updated":"2025-01-23T11:25:43Z","published":"2025-01-16T15:22:06Z","title":"Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid\n  Prototyping in Virtual Reality Applications","summary":"  SLAM is a foundational technique with broad applications in robotics and\nAR/VR. SLAM simulations evaluate new concepts, but testing on\nresource-constrained devices, such as VR HMDs, faces challenges: high\ncomputational cost and restricted sensor data access. This work proposes a\nsparse framework using mesh geometry projections as features, which improves\nefficiency and circumvents direct sensor data access, advancing SLAM research\nas we demonstrate in VR and through numerical evaluation.\n","authors":["Carlos Augusto Pinheiro de Sousa","Heiko Hamann","Oliver Deussen"],"pdf_url":"https://arxiv.org/pdf/2501.09600v4.pdf","comment":"Accepted to ENPT XR at IEEE VR 2025"},{"id":"http://arxiv.org/abs/2404.12385v2","updated":"2025-01-23T01:55:24Z","published":"2024-04-18T17:59:41Z","title":"MeshLRM: Large Reconstruction Model for High-Quality Meshes","summary":"  We propose MeshLRM, a novel LRM-based approach that can reconstruct a\nhigh-quality mesh from merely four input images in less than one second.\nDifferent from previous large reconstruction models (LRMs) that focus on\nNeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction\nand rendering within the LRM framework. This allows for end-to-end mesh\nreconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.\nMoreover, we improve the LRM architecture by simplifying several complex\ndesigns in previous LRMs. MeshLRM's NeRF initialization is sequentially trained\nwith low- and high-resolution images; this new LRM training strategy enables\nsignificantly faster convergence and thereby leads to better quality with less\ncompute. Our approach achieves state-of-the-art mesh reconstruction from\nsparse-view inputs and also allows for many downstream applications, including\ntext-to-3D and single-image-to-3D generation. Project page:\nhttps://sarahweiii.github.io/meshlrm/\n","authors":["Xinyue Wei","Kai Zhang","Sai Bi","Hao Tan","Fujun Luan","Valentin Deschaintre","Kalyan Sunkavalli","Hao Su","Zexiang Xu"],"pdf_url":"https://arxiv.org/pdf/2404.12385v2.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2501.13402v1","updated":"2025-01-23T06:01:03Z","published":"2025-01-23T06:01:03Z","title":"VIGS SLAM: IMU-based Large-Scale 3D Gaussian Splatting SLAM","summary":"  Recently, map representations based on radiance fields such as 3D Gaussian\nSplatting and NeRF, which excellent for realistic depiction, have attracted\nconsiderable attention, leading to attempts to combine them with SLAM. While\nthese approaches can build highly realistic maps, large-scale SLAM still\nremains a challenge because they require a large number of Gaussian images for\nmapping and adjacent images as keyframes for tracking. We propose a novel 3D\nGaussian Splatting SLAM method, VIGS SLAM, that utilizes sensor fusion of RGB-D\nand IMU sensors for large-scale indoor environments. To reduce the\ncomputational load of 3DGS-based tracking, we adopt an ICP-based tracking\nframework that combines IMU preintegration to provide a good initial guess for\naccurate pose estimation. Our proposed method is the first to propose that\nGaussian Splatting-based SLAM can be effectively performed in large-scale\nenvironments by integrating IMU sensor measurements. This proposal not only\nenhances the performance of Gaussian Splatting SLAM beyond room-scale scenarios\nbut also achieves SLAM performance comparable to state-of-the-art methods in\nlarge-scale indoor environments.\n","authors":["Gyuhyeon Pak","Euntai Kim"],"pdf_url":"https://arxiv.org/pdf/2501.13402v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.12385v2","updated":"2025-01-23T01:55:24Z","published":"2024-04-18T17:59:41Z","title":"MeshLRM: Large Reconstruction Model for High-Quality Meshes","summary":"  We propose MeshLRM, a novel LRM-based approach that can reconstruct a\nhigh-quality mesh from merely four input images in less than one second.\nDifferent from previous large reconstruction models (LRMs) that focus on\nNeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction\nand rendering within the LRM framework. This allows for end-to-end mesh\nreconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.\nMoreover, we improve the LRM architecture by simplifying several complex\ndesigns in previous LRMs. MeshLRM's NeRF initialization is sequentially trained\nwith low- and high-resolution images; this new LRM training strategy enables\nsignificantly faster convergence and thereby leads to better quality with less\ncompute. Our approach achieves state-of-the-art mesh reconstruction from\nsparse-view inputs and also allows for many downstream applications, including\ntext-to-3D and single-image-to-3D generation. Project page:\nhttps://sarahweiii.github.io/meshlrm/\n","authors":["Xinyue Wei","Kai Zhang","Sai Bi","Hao Tan","Fujun Luan","Valentin Deschaintre","Kalyan Sunkavalli","Hao Su","Zexiang Xu"],"pdf_url":"https://arxiv.org/pdf/2404.12385v2.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2501.13335v1","updated":"2025-01-23T02:31:57Z","published":"2025-01-23T02:31:57Z","title":"Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos","summary":"  We introduce Deblur-Avatar, a novel framework for modeling high-fidelity,\nanimatable 3D human avatars from motion-blurred monocular video inputs. Motion\nblur is prevalent in real-world dynamic video capture, especially due to human\nmovements in 3D human avatar modeling. Existing methods either (1) assume sharp\nimage inputs, failing to address the detail loss introduced by motion blur, or\n(2) mainly consider blur by camera movements, neglecting the human motion blur\nwhich is more common in animatable avatars. Our proposed approach integrates a\nhuman movement-based motion blur model into 3D Gaussian Splatting (3DGS). By\nexplicitly modeling human motion trajectories during exposure time, we jointly\noptimize the trajectories and 3D Gaussians to reconstruct sharp, high-quality\nhuman avatars. We employ a pose-dependent fusion mechanism to distinguish\nmoving body regions, optimizing both blurred and sharp areas effectively.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nDeblur-Avatar significantly outperforms existing methods in rendering quality\nand quantitative metrics, producing sharp avatar reconstructions and enabling\nreal-time rendering under challenging motion blur conditions.\n","authors":["Xianrui Luo","Juewen Peng","Zhongang Cai","Lei Yang","Fan Yang","Zhiguo Cao","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2501.13335v1.pdf","comment":null}]},"2025-01-17T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2501.09947v1","updated":"2025-01-17T04:14:09Z","published":"2025-01-17T04:14:09Z","title":"Surface-SOS: Self-Supervised Object Segmentation via Neural Surface\n  Representation","summary":"  Self-supervised Object Segmentation (SOS) aims to segment objects without any\nannotations. Under conditions of multi-camera inputs, the structural, textural\nand geometrical consistency among each view can be leveraged to achieve\nfine-grained object segmentation. To make better use of the above information,\nwe propose Surface representation based Self-supervised Object Segmentation\n(Surface-SOS), a new framework to segment objects for each view by 3D surface\nrepresentation from multi-view images of a scene. To model high-quality\ngeometry surfaces for complex scenes, we design a novel scene representation\nscheme, which decomposes the scene into two complementary neural representation\nmodules respectively with a Signed Distance Function (SDF). Moreover,\nSurface-SOS is able to refine single-view segmentation with multi-view\nunlabeled images, by introducing coarse segmentation masks as additional input.\nTo the best of our knowledge, Surface-SOS is the first self-supervised approach\nthat leverages neural surface representation to break the dependence on large\namounts of annotated data and strong constraints. These constraints typically\ninvolve observing target objects against a static background or relying on\ntemporal supervision in videos. Extensive experiments on standard benchmarks\nincluding LLFF, CO3D, BlendedMVS, TUM and several real-world scenes show that\nSurface-SOS always yields finer object masks than its NeRF-based counterparts\nand surpasses supervised single-view baselines remarkably. Code is available\nat: https://github.com/zhengxyun/Surface-SOS.\n","authors":["Xiaoyun Zheng","Liwei Liao","Jianbo Jiao","Feng Gao","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2501.09947v1.pdf","comment":"Accepted by TIP"}],"NeRF":[{"id":"http://arxiv.org/abs/2501.06770v2","updated":"2025-01-17T08:02:51Z","published":"2025-01-12T10:31:33Z","title":"SuperNeRF-GAN: A Universal 3D-Consistent Super-Resolution Framework for\n  Efficient and Enhanced 3D-Aware Image Synthesis","summary":"  Neural volume rendering techniques, such as NeRF, have revolutionized\n3D-aware image synthesis by enabling the generation of images of a single scene\nor object from various camera poses. However, the high computational cost of\nNeRF presents challenges for synthesizing high-resolution (HR) images. Most\nexisting methods address this issue by leveraging 2D super-resolution, which\ncompromise 3D-consistency. Other methods propose radiance manifolds or\ntwo-stage generation to achieve 3D-consistent HR synthesis, yet they are\nlimited to specific synthesis tasks, reducing their universality. To tackle\nthese challenges, we propose SuperNeRF-GAN, a universal framework for\n3D-consistent super-resolution. A key highlight of SuperNeRF-GAN is its\nseamless integration with NeRF-based 3D-aware image synthesis methods and it\ncan simultaneously enhance the resolution of generated images while preserving\n3D-consistency and reducing computational cost. Specifically, given a\npre-trained generator capable of producing a NeRF representation such as\ntri-plane, we first perform volume rendering to obtain a low-resolution image\nwith corresponding depth and normal map. Then, we employ a NeRF\nSuper-Resolution module which learns a network to obtain a high-resolution\nNeRF. Next, we propose a novel Depth-Guided Rendering process which contains\nthree simple yet effective steps, including the construction of a\nboundary-correct multi-depth map through depth aggregation, a normal-guided\ndepth super-resolution and a depth-guided NeRF rendering. Experimental results\ndemonstrate the superior efficiency, 3D-consistency, and quality of our\napproach. Additionally, ablation studies confirm the effectiveness of our\nproposed components.\n","authors":["Peng Zheng","Linzhi Huang","Yizhou Yu","Yi Chang","Yilin Wang","Rui Ma"],"pdf_url":"https://arxiv.org/pdf/2501.06770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09947v1","updated":"2025-01-17T04:14:09Z","published":"2025-01-17T04:14:09Z","title":"Surface-SOS: Self-Supervised Object Segmentation via Neural Surface\n  Representation","summary":"  Self-supervised Object Segmentation (SOS) aims to segment objects without any\nannotations. Under conditions of multi-camera inputs, the structural, textural\nand geometrical consistency among each view can be leveraged to achieve\nfine-grained object segmentation. To make better use of the above information,\nwe propose Surface representation based Self-supervised Object Segmentation\n(Surface-SOS), a new framework to segment objects for each view by 3D surface\nrepresentation from multi-view images of a scene. To model high-quality\ngeometry surfaces for complex scenes, we design a novel scene representation\nscheme, which decomposes the scene into two complementary neural representation\nmodules respectively with a Signed Distance Function (SDF). Moreover,\nSurface-SOS is able to refine single-view segmentation with multi-view\nunlabeled images, by introducing coarse segmentation masks as additional input.\nTo the best of our knowledge, Surface-SOS is the first self-supervised approach\nthat leverages neural surface representation to break the dependence on large\namounts of annotated data and strong constraints. These constraints typically\ninvolve observing target objects against a static background or relying on\ntemporal supervision in videos. Extensive experiments on standard benchmarks\nincluding LLFF, CO3D, BlendedMVS, TUM and several real-world scenes show that\nSurface-SOS always yields finer object masks than its NeRF-based counterparts\nand surpasses supervised single-view baselines remarkably. Code is available\nat: https://github.com/zhengxyun/Surface-SOS.\n","authors":["Xiaoyun Zheng","Liwei Liao","Jianbo Jiao","Feng Gao","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2501.09947v1.pdf","comment":"Accepted by TIP"}],"IQA":[{"id":"http://arxiv.org/abs/2501.09927v1","updated":"2025-01-17T02:47:25Z","published":"2025-01-17T02:47:25Z","title":"IE-Bench: Advancing the Measurement of Text-Driven Image Editing for\n  Human Perception Alignment","summary":"  Recent advances in text-driven image editing have been significant, yet the\ntask of accurately evaluating these edited images continues to pose a\nconsiderable challenge. Different from the assessment of text-driven image\ngeneration, text-driven image editing is characterized by simultaneously\nconditioning on both text and a source image. The edited images often retain an\nintrinsic connection to the original image, which dynamically change with the\nsemantics of the text. However, previous methods tend to solely focus on\ntext-image alignment or have not aligned with human perception. In this work,\nwe introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to\nenhance the assessment of text-driven edited images. IE-Bench includes a\ndatabase contains diverse source images, various editing prompts and the\ncorresponding results different editing methods, and total 3,010 Mean Opinion\nScores (MOS) provided by 25 human subjects. Furthermore, we introduce IE-QA, a\nmulti-modality source-aware quality assessment method for text-driven image\nediting. To the best of our knowledge, IE-Bench offers the first IQA dataset\nand model tailored for text-driven image editing. Extensive experiments\ndemonstrate IE-QA's superior subjective-alignments on the text-driven image\nediting task compared with previous metrics. We will make all related data and\ncode available to the public.\n","authors":["Shangkun Sun","Bowen Qu","Xiaoyu Liang","Songlin Fan","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2501.09927v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2501.10325v1","updated":"2025-01-17T17:56:52Z","published":"2025-01-17T17:56:52Z","title":"DiffStereo: High-Frequency Aware Diffusion Model for Stereo Image\n  Restoration","summary":"  Diffusion models (DMs) have achieved promising performance in image\nrestoration but haven't been explored for stereo images. The application of DM\nin stereo image restoration is confronted with a series of challenges. The need\nto reconstruct two images exacerbates DM's computational cost. Additionally,\nexisting latent DMs usually focus on semantic information and remove\nhigh-frequency details as redundancy during latent compression, which is\nprecisely what matters for image restoration. To address the above problems, we\npropose a high-frequency aware diffusion model, DiffStereo for stereo image\nrestoration as the first attempt at DM in this domain. Specifically, DiffStereo\nfirst learns latent high-frequency representations (LHFR) of HQ images. DM is\nthen trained in the learned space to estimate LHFR for stereo images, which are\nfused into a transformer-based stereo image restoration network providing\nbeneficial high-frequency information of corresponding HQ images. The\nresolution of LHFR is kept the same as input images, which preserves the\ninherent texture from distortion. And the compression in channels alleviates\nthe computational burden of DM. Furthermore, we devise a position encoding\nscheme when integrating the LHFR into the restoration network, enabling\ndistinctive guidance in different depths of the restoration network.\nComprehensive experiments verify that by combining generative DM and\ntransformer, DiffStereo achieves both higher reconstruction accuracy and better\nperceptual quality on stereo super-resolution, deblurring, and low-light\nenhancement compared with state-of-the-art methods.\n","authors":["Huiyun Cao","Yuan Shi","Bin Xia","Xiaoyu Jin","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2501.10325v1.pdf","comment":"9 pages, 6 figures"}]},"2025-01-15T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2501.08577v1","updated":"2025-01-15T04:56:26Z","published":"2025-01-15T04:56:26Z","title":"Scalable and High-Quality Neural Implicit Representation for 3D\n  Reconstruction","summary":"  Various SDF-based neural implicit surface reconstruction methods have been\nproposed recently, and have demonstrated remarkable modeling capabilities.\nHowever, due to the global nature and limited representation ability of a\nsingle network, existing methods still suffer from many drawbacks, such as\nlimited accuracy and scale of the reconstruction. In this paper, we propose a\nversatile, scalable and high-quality neural implicit representation to address\nthese issues. We integrate a divide-and-conquer approach into the neural\nSDF-based reconstruction. Specifically, we model the object or scene as a\nfusion of multiple independent local neural SDFs with overlapping regions. The\nconstruction of our representation involves three key steps: (1) constructing\nthe distribution and overlap relationship of the local radiance fields based on\nobject structure or data distribution, (2) relative pose registration for\nadjacent local SDFs, and (3) SDF blending. Thanks to the independent\nrepresentation of each local region, our approach can not only achieve\nhigh-fidelity surface reconstruction, but also enable scalable scene\nreconstruction. Extensive experimental results demonstrate the effectiveness\nand practicality of our proposed method.\n","authors":["Leyuan Yang","Bailin Deng","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.08577v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2312.11458v3","updated":"2025-01-15T22:17:24Z","published":"2023-12-18T18:59:03Z","title":"GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View\n  Synthesis","summary":"  We propose a method that achieves state-of-the-art rendering quality and\nefficiency on monocular dynamic scene reconstruction using deformable 3D\nGaussians. Implicit deformable representations commonly model motion with a\ncanonical space and time-dependent backward-warping deformation field. Our\nmethod, GauFRe, uses a forward-warping deformation to explicitly model\nnon-rigid transformations of scene geometry. Specifically, we propose a\ntemplate set of 3D Gaussians residing in a canonical space, and a\ntime-dependent forward-warping deformation field to model dynamic objects.\nAdditionally, we tailor a 3D Gaussian-specific static component supported by an\ninductive bias-aware initialization approach which allows the deformation field\nto focus on moving scene regions, improving the rendering of complex real-world\nmotion. The differentiable pipeline is optimized end-to-end with a\nself-supervised rendering loss. Experiments show our method achieves\ncompetitive results and higher efficiency than both previous state-of-the-art\nNeRF and Gaussian-based methods. For real-world scenes, GauFRe can train in ~20\nmins and offer 96 FPS real-time rendering on an RTX 3090 GPU. Project website:\nhttps://lynl7130.github.io/gaufre/index.html\n","authors":["Yiqing Liang","Numair Khan","Zhengqin Li","Thu Nguyen-Phuoc","Douglas Lanman","James Tompkin","Lei Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.11458v3.pdf","comment":"WACV 2025. 11 pages, 8 figures, 5 tables"}],"IQA":[{"id":"http://arxiv.org/abs/2403.06406v2","updated":"2025-01-15T12:36:24Z","published":"2024-03-11T03:35:41Z","title":"When No-Reference Image Quality Models Meet MAP Estimation in Diffusion\n  Latents","summary":"  Contemporary no-reference image quality assessment (NR-IQA) models can\neffectively quantify perceived image quality, often achieving strong\ncorrelations with human perceptual scores on standard IQA benchmarks. Yet,\nlimited efforts have been devoted to treating NR-IQA models as natural image\npriors for real-world image enhancement, and consequently comparing them from a\nperceptual optimization standpoint. In this work, we show -- for the first time\n-- that NR-IQA models can be plugged into the maximum a posteriori (MAP)\nestimation framework for image enhancement. This is achieved by performing\ngradient ascent in the diffusion latent space rather than in the raw pixel\ndomain, leveraging a pretrained differentiable and bijective diffusion process.\nLikely, different NR-IQA models lead to different enhanced outputs, which in\nturn provides a new computational means of comparing them. Unlike conventional\ncorrelation-based measures, our comparison method offers complementary insights\ninto the respective strengths and weaknesses of the competing NR-IQA models in\nperceptual optimization scenarios. Additionally, we aim to improve the\nbest-performing NR-IQA model in diffusion latent MAP estimation by\nincorporating the advantages of other top-performing methods. The resulting\nmodel delivers noticeably better results in enhancing real-world images\nafflicted by unknown and complex distortions, all preserving a high degree of\nimage fidelity.\n","authors":["Weixia Zhang","Dingquan Li","Guangtao Zhai","Xiaokang Yang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2403.06406v2.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2403.13163v5","updated":"2025-01-15T18:45:15Z","published":"2024-03-19T21:31:31Z","title":"DeblurDiNAT: A Compact Model with Exceptional Generalization and Visual\n  Fidelity on Unseen Domains","summary":"  Recent deblurring networks have effectively restored clear images from the\nblurred ones. However, they often struggle with generalization to unknown\ndomains. Moreover, these models typically focus on distortion metrics such as\nPSNR and SSIM, neglecting the critical aspect of metrics aligned with human\nperception. To address these limitations, we propose DeblurDiNAT, a deblurring\nTransformer based on Dilated Neighborhood Attention. First, DeblurDiNAT employs\nan alternating dilation factor paradigm to capture both local and global\nblurred patterns, enhancing generalization and perceptual clarity. Second, a\nlocal cross-channel learner aids the Transformer block to understand the\nshort-range relationships between adjacent channels. Additionally, we present a\nlinear feed-forward network with a simple while effective design. Finally, a\ndual-stage feature fusion module is introduced as an alternative to the\nexisting approach, which efficiently process multi-scale visual information\nacross network levels. Compared to state-of-the-art models, our compact\nDeblurDiNAT demonstrates superior generalization capabilities and achieves\nremarkable performance in perceptual metrics, while maintaining a favorable\nmodel size.\n","authors":["Hanzhou Liu","Binghan Li","Chengkai Liu","Mi Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13163v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.03678v2","updated":"2025-01-15T06:32:05Z","published":"2022-06-08T05:04:43Z","title":"Ultra-High-Definition Image Deblurring via Multi-scale Cubic-Mixer","summary":"  Currently, transformer-based algorithms are making a splash in the domain of\nimage deblurring. Their achievement depends on the self-attention mechanism\nwith CNN stem to model long range dependencies between tokens. Unfortunately,\nthis ear-pleasing pipeline introduces high computational complexity and makes\nit difficult to run an ultra-high-definition image on a single GPU in real\ntime. To trade-off accuracy and efficiency, the input degraded image is\ncomputed cyclically over three dimensional ($C$, $W$, and $H$) signals without\na self-attention mechanism. We term this deep network as Multi-scale\nCubic-Mixer, which is acted on both the real and imaginary components after\nfast Fourier transform to estimate the Fourier coefficients and thus obtain a\ndeblurred image. Furthermore, we combine the multi-scale cubic-mixer with a\nslicing strategy to generate high-quality results at a much lower computational\ncost. Experimental results demonstrate that the proposed algorithm performs\nfavorably against the state-of-the-art deblurring approaches on the several\nbenchmarks and a new ultra-high-definition dataset in terms of accuracy and\nspeed.\n","authors":["Xingchi Chen","Xiuyi Jia","Zhuoran Zheng"],"pdf_url":"https://arxiv.org/pdf/2206.03678v2.pdf","comment":"9 pages"}]},"2025-02-11T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2502.07754v1","updated":"2025-02-11T18:27:39Z","published":"2025-02-11T18:27:39Z","title":"MeshSplats: Mesh-Based Rendering with Gaussian Splatting Initialization","summary":"  Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer\ngraphics. GS-based algorithms almost always bypass classical methods such as\nray tracing, which offers numerous inherent advantages for rendering. For\nexample, ray tracing is able to handle incoherent rays for advanced lighting\neffects, including shadows and reflections. To address this limitation, we\nintroduce MeshSplats, a method which converts GS to a mesh-like format.\nFollowing the completion of training, MeshSplats transforms Gaussian elements\ninto mesh faces, enabling rendering using ray tracing methods with all their\nassociated benefits. Our model can be utilized immediately following\ntransformation, yielding a mesh of slightly reduced quality without additional\ntraining. Furthermore, we can enhance the reconstruction quality through the\napplication of a dedicated optimization algorithm that operates on mesh faces\nrather than Gaussian components. The efficacy of our method is substantiated by\nexperimental results, underscoring its extensive applications in computer\ngraphics and image processing.\n","authors":["Rafał Tobiasz","Grzegorz Wilczyński","Marcin Mazur","Sławomir Tadeja","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2502.07754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20562v2","updated":"2025-02-11T17:53:46Z","published":"2024-09-30T17:59:03Z","title":"SpaceMesh: A Continuous Representation for Learning Manifold Surface\n  Meshes","summary":"  Meshes are ubiquitous in visual computing and simulation, yet most existing\nmachine learning techniques represent meshes only indirectly, e.g. as the level\nset of a scalar field or deformation of a template, or as a disordered triangle\nsoup lacking local structure. This work presents a scheme to directly generate\nmanifold, polygonal meshes of complex connectivity as the output of a neural\nnetwork. Our key innovation is to define a continuous latent connectivity space\nat each mesh vertex, which implies the discrete mesh. In particular, our vertex\nembeddings generate cyclic neighbor relationships in a halfedge mesh\nrepresentation, which gives a guarantee of edge-manifoldness and the ability to\nrepresent general polygonal meshes. This representation is well-suited to\nmachine learning and stochastic optimization, without restriction on\nconnectivity or topology. We first explore the basic properties of this\nrepresentation, then use it to fit distributions of meshes from large datasets.\nThe resulting models generate diverse meshes with tessellation structure\nlearned from the dataset population, with concise details and high-quality mesh\nelements. In applications, this approach not only yields high-quality outputs\nfrom generative models, but also enables directly learning challenging geometry\nprocessing tasks such as mesh repair.\n","authors":["Tianchang Shen","Zhaoshuo Li","Marc Law","Matan Atzmon","Sanja Fidler","James Lucas","Jun Gao","Nicholas Sharp"],"pdf_url":"https://arxiv.org/pdf/2409.20562v2.pdf","comment":"published at SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2502.07615v1","updated":"2025-02-11T15:05:26Z","published":"2025-02-11T15:05:26Z","title":"Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained\n  Matching Priors","summary":"  3D Gaussian Splatting (3DGS) has achieved excellent rendering quality with\nfast training and rendering speed. However, its optimization process lacks\nexplicit geometric constraints, leading to suboptimal geometric reconstruction\nin regions with sparse or no observational input views. In this work, we try to\nmitigate the issue by incorporating a pre-trained matching prior to the 3DGS\noptimization process. We introduce Flow Distillation Sampling (FDS), a\ntechnique that leverages pre-trained geometric knowledge to bolster the\naccuracy of the Gaussian radiance field. Our method employs a strategic\nsampling technique to target unobserved views adjacent to the input views,\nutilizing the optical flow calculated from the matching model (Prior Flow) to\nguide the flow analytically calculated from the 3DGS geometry (Radiance Flow).\nComprehensive experiments in depth rendering, mesh reconstruction, and novel\nview synthesis showcase the significant advantages of FDS over state-of-the-art\nmethods. Additionally, our interpretive experiments and analysis aim to shed\nlight on the effects of FDS on geometric accuracy and rendering quality,\npotentially providing readers with insights into its performance. Project page:\nhttps://nju-3dv.github.io/projects/fds\n","authors":["Lin-Zhuo Chen","Kangjie Liu","Youtian Lin","Siyu Zhu","Zhihao Li","Xun Cao","Yao Yao"],"pdf_url":"https://arxiv.org/pdf/2502.07615v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2411.08508v3","updated":"2025-02-11T10:38:48Z","published":"2024-11-13T10:43:39Z","title":"BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel\n  View Synthesis","summary":"  We present billboard Splatting (BBSplat) - a novel approach for 3D scene\nrepresentation based on textured geometric primitives. BBSplat represents the\nscene as a set of optimizable textured planar primitives with learnable RGB\ntextures and alpha-maps to control their shape. BBSplat primitives can be used\nin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. The\nproposed primitives close the rendering quality gap between 2D and 3D Gaussian\nSplatting (GS), preserving the accurate mesh extraction ability of 2D\nprimitives. Our novel regularization term encourages textures to have a sparser\nstructure, unlocking an efficient compression that leads to a reduction in the\nstorage space of the model. Our experiments show the efficiency of BBSplat on\nstandard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,\nand Mip-NeRF-360.\n","authors":["David Svitov","Pietro Morerio","Lourdes Agapito","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2411.08508v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05769v2","updated":"2025-02-11T03:32:10Z","published":"2025-02-09T04:06:07Z","title":"Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual\n  Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps\n  Platform","summary":"  Urban digital twins are virtual replicas of cities that use multi-source data\nand data analytics to optimize urban planning, infrastructure management, and\ndecision-making. Towards this, we propose a framework focused on the\nsingle-building scale. By connecting to cloud mapping platforms such as Google\nMap Platforms APIs, by leveraging state-of-the-art multi-agent Large Language\nModels data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our\nGaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings\nframework can retrieve a building's 3D model, visual descriptions, and achieve\ncloud-based mapping integration with large language model-based data analytics\nusing a building's address, postal code, or geographic coordinates.\n","authors":["Kyle Gao","Dening Lu","Liangzhi Li","Nan Chen","Hongjie He","Linlin Xu","Jonathan Li"],"pdf_url":"https://arxiv.org/pdf/2502.05769v2.pdf","comment":"-Fixed minor typo"},{"id":"http://arxiv.org/abs/2502.07145v1","updated":"2025-02-11T00:19:23Z","published":"2025-02-11T00:19:23Z","title":"Mesh2SSM++: A Probabilistic Framework for Unsupervised Learning of\n  Statistical Shape Model of Anatomies from Surface Meshes","summary":"  Anatomy evaluation is crucial for understanding the physiological state,\ndiagnosing abnormalities, and guiding medical interventions. Statistical shape\nmodeling (SSM) is vital in this process. By enabling the extraction of\nquantitative morphological shape descriptors from MRI and CT scans, SSM\nprovides comprehensive descriptions of anatomical variations within a\npopulation. However, the effectiveness of SSM in anatomy evaluation hinges on\nthe quality and robustness of the shape models. While deep learning techniques\nshow promise in addressing these challenges by learning complex nonlinear\nrepresentations of shapes, existing models still have limitations and often\nrequire pre-established shape models for training. To overcome these issues, we\npropose Mesh2SSM++, a novel approach that learns to estimate correspondences\nfrom meshes in an unsupervised manner. This method leverages unsupervised,\npermutation-invariant representation learning to estimate how to deform a\ntemplate point cloud into subject-specific meshes, forming a\ncorrespondence-based shape model. Additionally, our probabilistic formulation\nallows learning a population-specific template, reducing potential biases\nassociated with template selection. A key feature of Mesh2SSM++ is its ability\nto quantify aleatoric uncertainty, which captures inherent data variability and\nis essential for ensuring reliable model predictions and robust decision-making\nin clinical tasks, especially under challenging imaging conditions. Through\nextensive validation across diverse anatomies, evaluation metrics, and\ndownstream tasks, we demonstrate that Mesh2SSM++ outperforms existing methods.\nIts ability to operate directly on meshes, combined with computational\nefficiency and interpretability through its probabilistic framework, makes it\nan attractive alternative to traditional and deep learning-based SSM\napproaches.\n","authors":["Krithika Iyer","Mokshagna Sai Teja Karanam","Shireen Elhabian"],"pdf_url":"https://arxiv.org/pdf/2502.07145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07140v1","updated":"2025-02-11T00:10:58Z","published":"2025-02-11T00:10:58Z","title":"Few-Shot Multi-Human Neural Rendering Using Geometry Constraints","summary":"  We present a method for recovering the shape and radiance of a scene\nconsisting of multiple people given solely a few images. Multi-human scenes are\ncomplex due to additional occlusion and clutter. For single-human settings,\nexisting approaches using implicit neural representations have achieved\nimpressive results that deliver accurate geometry and appearance. However, it\nremains challenging to extend these methods for estimating multiple humans from\nsparse views. We propose a neural implicit reconstruction method that addresses\nthe inherent challenges of this task through the following contributions:\nFirst, we propose to use geometry constraints by exploiting pre-computed meshes\nusing a human body model (SMPL). Specifically, we regularize the signed\ndistances using the SMPL mesh and leverage bounding boxes for improved\nrendering. Second, we propose a ray regularization scheme to minimize rendering\ninconsistencies, and a saturation regularization for robust optimization in\nvariable illumination. Extensive experiments on both real and synthetic\ndatasets demonstrate the benefits of our approach and show state-of-the-art\nperformance against existing neural reconstruction methods.\n","authors":["Qian li","Victoria Fernàndez Abrevaya","Franck Multon","Adnane Boukhayma"],"pdf_url":"https://arxiv.org/pdf/2502.07140v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2502.04843v2","updated":"2025-02-11T10:48:23Z","published":"2025-02-07T11:24:23Z","title":"PoI: Pixel of Interest for Novel View Synthesis Assisted Scene\n  Coordinate Regression","summary":"  The task of estimating camera poses can be enhanced through novel view\nsynthesis techniques such as NeRF and Gaussian Splatting to increase the\ndiversity and extension of training data. However, these techniques often\nproduce rendered images with issues like blurring and ghosting, which\ncompromise their reliability. These issues become particularly pronounced for\nScene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the\npixel level. To mitigate the problems associated with unreliable rendered\nimages, we introduce a novel filtering approach, which selectively extracts\nwell-rendered pixels while discarding the inferior ones. This filter\nsimultaneously measures the SCR model's real-time reprojection loss and\ngradient during training. Building on this filtering technique, we also develop\na new strategy to improve scene coordinate regression using sparse inputs,\ndrawing on successful applications of sparse input techniques in novel view\nsynthesis. Our experimental results validate the effectiveness of our method,\ndemonstrating state-of-the-art performance on indoor and outdoor datasets.\n","authors":["Feifei Li","Qi Song","Chi Zhang","Hui Shuai","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2502.04843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08508v3","updated":"2025-02-11T10:38:48Z","published":"2024-11-13T10:43:39Z","title":"BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel\n  View Synthesis","summary":"  We present billboard Splatting (BBSplat) - a novel approach for 3D scene\nrepresentation based on textured geometric primitives. BBSplat represents the\nscene as a set of optimizable textured planar primitives with learnable RGB\ntextures and alpha-maps to control their shape. BBSplat primitives can be used\nin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. The\nproposed primitives close the rendering quality gap between 2D and 3D Gaussian\nSplatting (GS), preserving the accurate mesh extraction ability of 2D\nprimitives. Our novel regularization term encourages textures to have a sparser\nstructure, unlocking an efficient compression that leads to a reduction in the\nstorage space of the model. Our experiments show the efficiency of BBSplat on\nstandard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,\nand Mip-NeRF-360.\n","authors":["David Svitov","Pietro Morerio","Lourdes Agapito","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2411.08508v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11394v3","updated":"2025-02-11T07:34:37Z","published":"2024-07-16T05:26:14Z","title":"DreamCatalyst: Fast and High-Quality 3D Editing via Controlling\n  Editability and Identity Preservation","summary":"  Score distillation sampling (SDS) has emerged as an effective framework in\ntext-driven 3D editing tasks, leveraging diffusion models for 3D-consistent\nediting. However, existing SDS-based 3D editing methods suffer from long\ntraining times and produce low-quality results. We identify that the root cause\nof this performance degradation is \\textit{their conflict with the sampling\ndynamics of diffusion models}. Addressing this conflict allows us to treat SDS\nas a diffusion reverse process for 3D editing via sampling from data space. In\ncontrast, existing methods naively distill the score function using diffusion\nmodels. From these insights, we propose DreamCatalyst, a novel framework that\nconsiders these sampling dynamics in the SDS framework. Specifically, we devise\nthe optimization process of our DreamCatalyst to approximate the diffusion\nreverse process in editing tasks, thereby aligning with diffusion sampling\ndynamics. As a result, DreamCatalyst successfully reduces training time and\nimproves editing quality. Our method offers two modes: (1) a fast mode that\nedits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than\ncurrent state-of-the-art NeRF editing methods, and (2) a high-quality mode that\nproduces superior results about 8 times faster than these methods. Notably, our\nhigh-quality mode outperforms current state-of-the-art NeRF editing methods in\nterms of both speed and quality. DreamCatalyst also surpasses the\nstate-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing\nitself as an effective and model-agnostic 3D editing solution. See more\nextensive results on our project page: https://dream-catalyst.github.io.\n","authors":["Jiwook Kim","Seonho Lee","Jaeyo Shin","Jiho Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2407.11394v3.pdf","comment":"ICLR 2025"}],"Deblur":[{"id":"http://arxiv.org/abs/2502.07602v1","updated":"2025-02-11T14:52:11Z","published":"2025-02-11T14:52:11Z","title":"An Improved Optimal Proximal Gradient Algorithm for Non-Blind Image\n  Deblurring","summary":"  Image deblurring remains a central research area within image processing,\ncritical for its role in enhancing image quality and facilitating clearer\nvisual representations across diverse applications. This paper tackles the\noptimization problem of image deblurring, assuming a known blurring kernel. We\nintroduce an improved optimal proximal gradient algorithm (IOptISTA), which\nbuilds upon the optimal gradient method and a weighting matrix, to efficiently\naddress the non-blind image deblurring problem. Based on two regularization\ncases, namely the $l_1$ norm and total variation norm, we perform numerical\nexperiments to assess the performance of our proposed algorithm. The results\nindicate that our algorithm yields enhanced PSNR and SSIM values, as well as a\nreduced tolerance, compared to existing methods.\n","authors":["Qingsong Wang","Shengze Xu","Xiaojiao Tong","Tieyong Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.07602v1.pdf","comment":null}]},"2025-02-09T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2502.05859v1","updated":"2025-02-09T11:36:45Z","published":"2025-02-09T11:36:45Z","title":"SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion","summary":"  Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.\n","authors":["Qingsong Yan","Qiang Wang","Kaiyong Zhao","Jie Chen","Bo Li","Xiaowen Chu","Fei Deng"],"pdf_url":"https://arxiv.org/pdf/2502.05859v1.pdf","comment":"3DV 2025"},{"id":"http://arxiv.org/abs/2403.09981v3","updated":"2025-02-09T11:04:10Z","published":"2024-03-15T02:57:20Z","title":"Controllable Text-to-3D Generation via Surface-Aligned Gaussian\n  Splatting","summary":"  While text-to-3D and image-to-3D generation tasks have received considerable\nattention, one important but under-explored field between them is controllable\ntext-to-3D generation, which we mainly focus on in this work. To address this\ntask, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network\narchitecture designed to enhance existing pre-trained multi-view diffusion\nmodels by integrating additional input conditions, such as edge, depth, normal,\nand scribble maps. Our innovation lies in the introduction of a conditioning\nmodule that controls the base diffusion model using both local and global\nembeddings, which are computed from the input condition images and camera\nposes. Once trained, MVControl is able to offer 3D diffusion guidance for\noptimization-based 3D generation. And, 2) we propose an efficient multi-stage\n3D generation pipeline that leverages the benefits of recent large\nreconstruction models and score distillation algorithm. Building upon our\nMVControl architecture, we employ a unique hybrid diffusion guidance method to\ndirect the optimization process. In pursuit of efficiency, we adopt 3D\nGaussians as our representation instead of the commonly used implicit\nrepresentations. We also pioneer the use of SuGaR, a hybrid representation that\nbinds Gaussians to mesh triangle faces. This approach alleviates the issue of\npoor geometry in 3D Gaussians and enables the direct sculpting of fine-grained\ngeometry on the mesh. Extensive experiments demonstrate that our method\nachieves robust generalization and enables the controllable generation of\nhigh-quality 3D content. Project page: https://lizhiqi49.github.io/MVControl/.\n","authors":["Zhiqi Li","Yiming Chen","Lingzhe Zhao","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.09981v3.pdf","comment":"3DV-2025"},{"id":"http://arxiv.org/abs/2502.05752v1","updated":"2025-02-09T03:06:19Z","published":"2025-02-09T03:06:19Z","title":"PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based\n  Implicit Neural Map","summary":"  Robots require high-fidelity reconstructions of their environment for\neffective operation. Such scene representations should be both, geometrically\naccurate and photorealistic to support downstream tasks. While this can be\nachieved by building distance fields from range sensors and radiance fields\nfrom cameras, the scalable incremental mapping of both fields consistently and\nat the same time with high quality remains challenging. In this paper, we\npropose a novel map representation that unifies a continuous signed distance\nfield and a Gaussian splatting radiance field within an elastic and compact\npoint-based implicit neural map. By enforcing geometric consistency between\nthese fields, we achieve mutual improvements by exploiting both modalities. We\ndevise a LiDAR-visual SLAM system called PINGS using the proposed map\nrepresentation and evaluate it on several challenging large-scale datasets.\nExperimental results demonstrate that PINGS can incrementally build globally\nconsistent distance and radiance fields encoded with a compact set of neural\npoints. Compared to the state-of-the-art methods, PINGS achieves superior\nphotometric and geometric rendering at novel views by leveraging the\nconstraints from the distance field. Furthermore, by utilizing dense\nphotometric cues and multi-view consistency from the radiance field, PINGS\nproduces more accurate distance fields, leading to improved odometry estimation\nand mesh reconstruction.\n","authors":["Yue Pan","Xingguang Zhong","Liren Jin","Louis Wiesmann","Marija Popović","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2502.05752v1.pdf","comment":"14 pages, 8 figures"}]},"2025-02-06T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2502.04317v1","updated":"2025-02-06T18:57:57Z","published":"2025-02-06T18:57:57Z","title":"Factorized Implicit Global Convolution for Automotive Computational\n  Fluid Dynamics Prediction","summary":"  Computational Fluid Dynamics (CFD) is crucial for automotive design,\nrequiring the analysis of large 3D point clouds to study how vehicle geometry\naffects pressure fields and drag forces. However, existing deep learning\napproaches for CFD struggle with the computational complexity of processing\nhigh-resolution 3D data. We propose Factorized Implicit Global Convolution\n(FIGConv), a novel architecture that efficiently solves CFD problems for very\nlarge 3D meshes with arbitrary input and output geometries. FIGConv achieves\nquadratic complexity $O(N^2)$, a significant improvement over existing 3D\nneural CFD models that require cubic complexity $O(N^3)$. Our approach combines\nFactorized Implicit Grids to approximate high-resolution domains, efficient\nglobal convolutions through 2D reparameterization, and a U-shaped architecture\nfor effective information gathering and integration. We validate our approach\non the industry-standard Ahmed body dataset and the large-scale DrivAerNet\ndataset. In DrivAerNet, our model achieves an $R^2$ value of 0.95 for drag\nprediction, outperforming the previous state-of-the-art by a significant\nmargin. This represents a 40% improvement in relative mean squared error and a\n70% improvement in absolute mean squared error over previous methods.\n","authors":["Chris Choy","Alexey Kamenev","Jean Kossaifi","Max Rietmann","Jan Kautz","Kamyar Azizzadenesheli"],"pdf_url":"https://arxiv.org/pdf/2502.04317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13882v3","updated":"2025-02-06T18:15:21Z","published":"2024-10-03T19:42:16Z","title":"Articulate-Anything: Automatic Modeling of Articulated Objects via a\n  Vision-Language Foundation Model","summary":"  Interactive 3D simulated objects are crucial in AR/VR, animations, and\nrobotics, driving immersive experiences and advanced automation. However,\ncreating these articulated objects requires extensive human effort and\nexpertise, limiting their broader applications. To overcome this challenge, we\npresent Articulate-Anything, a system that automates the articulation of\ndiverse, complex objects from many input modalities, including text, images,\nand videos. Articulate-Anything leverages vision-language models (VLMs) to\ngenerate code that can be compiled into an interactable digital twin for use in\nstandard 3D simulators. Our system exploits existing 3D asset datasets via a\nmesh retrieval mechanism, along with an actor-critic system that iteratively\nproposes, evaluates, and refines solutions for articulating the objects,\nself-correcting errors to achieve a robust outcome. Qualitative evaluations\ndemonstrate Articulate-Anything's capability to articulate complex and even\nambiguous object affordances by leveraging rich grounded inputs. In extensive\nquantitative experiments on the standard PartNet-Mobility dataset,\nArticulate-Anything substantially outperforms prior work, increasing the\nsuccess rate from 8.7-11.6% to 75% and setting a new bar for state-of-the-art\nperformance. We further showcase the utility of our system by generating 3D\nassets from in-the-wild video inputs, which are then used to train robotic\npolicies for fine-grained manipulation tasks in simulation that go beyond basic\npick and place. These policies are then transferred to a real robotic system.\n","authors":["Long Le","Jason Xie","William Liang","Hung-Ju Wang","Yue Yang","Yecheng Jason Ma","Kyle Vedder","Arjun Krishna","Dinesh Jayaraman","Eric Eaton"],"pdf_url":"https://arxiv.org/pdf/2410.13882v3.pdf","comment":"ICLR 2025. Project website and open-source code:\n  https://articulate-anything.github.io/"},{"id":"http://arxiv.org/abs/2502.03836v1","updated":"2025-02-06T07:42:00Z","published":"2025-02-06T07:42:00Z","title":"Adapting Human Mesh Recovery with Vision-Language Feedback","summary":"  Human mesh recovery can be approached using either regression-based or\noptimization-based methods. Regression models achieve high pose accuracy but\nstruggle with model-to-image alignment due to the lack of explicit 2D-3D\ncorrespondences. In contrast, optimization-based methods align 3D models to 2D\nobservations but are prone to local minima and depth ambiguity. In this work,\nwe leverage large vision-language models (VLMs) to generate interactive body\npart descriptions, which serve as implicit constraints to enhance 3D perception\nand limit the optimization space. Specifically, we formulate monocular human\nmesh recovery as a distribution adaptation task by integrating both 2D\nobservations and language descriptions. To bridge the gap between text and 3D\npose signals, we first train a text encoder and a pose VQ-VAE, aligning texts\nto body poses in a shared latent space using contrastive learning.\nSubsequently, we employ a diffusion-based framework to refine the initial\nparameters guided by gradients derived from both 2D observations and text\ndescriptions. Finally, the model can produce poses with accurate 3D perception\nand image consistency. Experimental results on multiple benchmarks validate its\neffectiveness. The code will be made publicly available.\n","authors":["Chongyang Xu","Buzhen Huang","Chengfang Zhang","Ziliang Feng","Yangang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.03836v1.pdf","comment":"6 pages, 7 figures"}],"Deblur":[{"id":"http://arxiv.org/abs/2502.03810v1","updated":"2025-02-06T06:43:32Z","published":"2025-02-06T06:43:32Z","title":"DeblurDiff: Real-World Image Deblurring with Generative Diffusion Models","summary":"  Diffusion models have achieved significant progress in image generation. The\npre-trained Stable Diffusion (SD) models are helpful for image deblurring by\nproviding clear image priors. However, directly using a blurry image or\npre-deblurred one as a conditional control for SD will either hinder accurate\nstructure extraction or make the results overly dependent on the deblurring\nnetwork. In this work, we propose a Latent Kernel Prediction Network (LKPN) to\nachieve robust real-world image deblurring. Specifically, we co-train the LKPN\nin latent space with conditional diffusion. The LKPN learns a spatially variant\nkernel to guide the restoration of sharp images in the latent space. By\napplying element-wise adaptive convolution (EAC), the learned kernel is\nutilized to adaptively process the input feature, effectively preserving the\nstructural information of the input. This process thereby more effectively\nguides the generative process of Stable Diffusion (SD), enhancing both the\ndeblurring efficacy and the quality of detail reconstruction. Moreover, the\nresults at each diffusion step are utilized to iteratively estimate the kernels\nin LKPN to better restore the sharp latent by EAC. This iterative refinement\nenhances the accuracy and robustness of the deblurring process. Extensive\nexperimental results demonstrate that the proposed method outperforms\nstate-of-the-art image deblurring methods on both benchmark and real-world\nimages.\n","authors":["Lingshun Kong","Jiawei Zhang","Dongqing Zou","Jimmy Ren","Xiaohe Wu","Jiangxin Dong","Jinshan Pan"],"pdf_url":"https://arxiv.org/pdf/2502.03810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03686v1","updated":"2025-02-06T00:24:39Z","published":"2025-02-06T00:24:39Z","title":"Variational Control for Guidance in Diffusion Models","summary":"  Diffusion models exhibit excellent sample quality, but existing guidance\nmethods often require additional model training or are limited to specific\ntasks. We revisit guidance in diffusion models from the perspective of\nvariational inference and control, introducing Diffusion Trajectory Matching\n(DTM) that enables guiding pretrained diffusion trajectories to satisfy a\nterminal cost. DTM unifies a broad class of guidance methods and enables novel\ninstantiations. We introduce a new method within this framework that achieves\nstate-of-the-art results on several linear and (blind) non-linear inverse\nproblems without requiring additional model training or modifications. For\ninstance, in ImageNet non-linear deblurring, our model achieves an FID score of\n34.31, significantly improving over the best pretrained-method baseline (FID\n78.07). We will make the code available in a future update.\n","authors":["Kushagra Pandey","Farrin Marouf Sofian","Felix Draxler","Theofanis Karaletsos","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2502.03686v1.pdf","comment":"8 pages in main text. Total of 20 pages"}]},"2025-02-03T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2502.01536v1","updated":"2025-02-03T17:15:05Z","published":"2025-02-03T17:15:05Z","title":"VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and\n  Locomotion","summary":"  Recent success in legged robot locomotion is attributed to the integration of\nreinforcement learning and physical simulators. However, these policies often\nencounter challenges when deployed in real-world environments due to\nsim-to-real gaps, as simulators typically fail to replicate visual realism and\ncomplex real-world geometry. Moreover, the lack of realistic visual rendering\nlimits the ability of these policies to support high-level tasks requiring\nRGB-based perception like ego-centric navigation. This paper presents a\nReal-to-Sim-to-Real framework that generates photorealistic and physically\ninteractive \"digital twin\" simulation environments for visual navigation and\nlocomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based\nscene reconstruction from multi-view images and integrates these environments\ninto simulations that support ego-centric visual perception and mesh-based\nphysical interactions. To demonstrate its effectiveness, we train a\nreinforcement learning policy within the simulator to perform a visual\ngoal-tracking task. Extensive experiments show that our framework achieves\nRGB-only sim-to-real policy transfer. Additionally, our framework facilitates\nthe rapid adaptation of robot policies with effective exploration capability in\ncomplex new environments, highlighting its potential for applications in\nhouseholds and factories.\n","authors":["Shaoting Zhu","Linzhan Mou","Derun Li","Baijun Ye","Runhan Huang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.01536v1.pdf","comment":"Project Page: https://vr-robo.github.io/"},{"id":"http://arxiv.org/abs/2502.01157v1","updated":"2025-02-03T08:49:57Z","published":"2025-02-03T08:49:57Z","title":"Radiant Foam: Real-Time Differentiable Ray Tracing","summary":"  Research on differentiable scene representations is consistently moving\ntowards more efficient, real-time models. Recently, this has led to the\npopularization of splatting methods, which eschew the traditional ray-based\nrendering of radiance fields in favor of rasterization. This has yielded a\nsignificant improvement in rendering speeds due to the efficiency of\nrasterization algorithms and hardware, but has come at a cost: the\napproximations that make rasterization efficient also make implementation of\nlight transport phenomena like reflection and refraction much more difficult.\nWe propose a novel scene representation which avoids these approximations, but\nkeeps the efficiency and reconstruction quality of splatting by leveraging a\ndecades-old efficient volumetric mesh ray tracing algorithm which has been\nlargely overlooked in recent computer vision research. The resulting model,\nwhich we name Radiant Foam, achieves rendering speed and quality comparable to\nGaussian Splatting, without the constraints of rasterization. Unlike ray traced\nGaussian models that use hardware ray tracing acceleration, our method requires\nno special hardware or APIs beyond the standard features of a programmable GPU.\n","authors":["Shrisudhan Govindarajan","Daniel Rebain","Kwang Moo Yi","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2502.01157v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2502.01522v1","updated":"2025-02-03T17:00:40Z","published":"2025-02-03T17:00:40Z","title":"BD-Diff: Generative Diffusion Model for Image Deblurring on Unknown\n  Domains with Blur-Decoupled Learning","summary":"  Generative diffusion models trained on large-scale datasets have achieved\nremarkable progress in image synthesis. In favor of their ability to supplement\nmissing details and generate aesthetically pleasing contents, recent works have\napplied them to image deblurring tasks via training an adapter on blurry-sharp\nimage pairs to provide structural conditions for restoration. However,\nacquiring substantial amounts of realistic paired data is challenging and\ncostly in real-world scenarios. On the other hand, relying solely on synthetic\ndata often results in overfitting, leading to unsatisfactory performance when\nconfronted with unseen blur patterns. To tackle this issue, we propose BD-Diff,\na generative-diffusion-based model designed to enhance deblurring performance\non unknown domains by decoupling structural features and blur patterns through\njoint training on three specially designed tasks. We employ two Q-Formers as\nstructural representations and blur patterns extractors separately. The\nfeatures extracted by them will be used for the supervised deblurring task on\nsynthetic data and the unsupervised blur-transfer task by leveraging unpaired\nblurred images from the target domain simultaneously. Furthermore, we introduce\na reconstruction task to make the structural features and blur patterns\ncomplementary. This blur-decoupled learning process enhances the generalization\ncapabilities of BD-Diff when encountering unknown domain blur patterns.\nExperiments on real-world datasets demonstrate that BD-Diff outperforms\nexisting state-of-the-art methods in blur removal and structural preservation\nin various challenging scenarios. The codes will be released in\nhttps://github.com/donahowe/BD-Diff\n","authors":["Junhao Cheng","Wei-Ting Chen","Xi Lu","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2502.01522v1.pdf","comment":"We propose BD-Diff to integrate generative diffusion model into\n  unpaired deblurring tasks"}]},"2025-02-02T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.14317v3","updated":"2025-02-02T04:56:55Z","published":"2025-01-24T08:22:02Z","title":"Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation","summary":"  Triangle meshes are fundamental to 3D applications, enabling efficient\nmodification and rasterization while maintaining compatibility with standard\nrendering pipelines. However, current automatic mesh generation methods\ntypically rely on intermediate representations that lack the continuous surface\nquality inherent to meshes. Converting these representations into meshes\nproduces dense, suboptimal outputs. Although recent autoregressive approaches\ndemonstrate promise in directly modeling mesh vertices and faces, they are\nconstrained by the limitation in face count, scalability, and structural\nfidelity. To address these challenges, we propose Nautilus, a locality-aware\nautoencoder for artist-like mesh generation that leverages the local properties\nof manifold meshes to achieve structural fidelity and efficient representation.\nOur approach introduces a novel tokenization algorithm that preserves face\nproximity relationships and compresses sequence length through locally shared\nvertices and edges, enabling the generation of meshes with an unprecedented\nscale of up to 5,000 faces. Furthermore, we develop a Dual-stream Point\nConditioner that provides multi-scale geometric guidance, ensuring global\nconsistency and local structural fidelity by capturing fine-grained geometric\nfeatures. Extensive experiments demonstrate that Nautilus significantly\noutperforms state-of-the-art methods in both fidelity and scalability. The\nproject page is at https://nautilusmeshgen.github.io.\n","authors":["Yuxuan Wang","Xuanyu Yi","Haohan Weng","Qingshan Xu","Xiaokang Wei","Xianghui Yang","Chunchao Guo","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.14317v3.pdf","comment":"Project Page: https://nautilusmeshgen.github.io, Tencent Hunyuan, 14\n  pages"}],"NeRF":[{"id":"http://arxiv.org/abs/2304.02061v4","updated":"2025-02-02T17:40:23Z","published":"2023-04-04T18:24:22Z","title":"Generating Continual Human Motion in Diverse 3D Scenes","summary":"  We introduce a method to synthesize animator guided human motion across 3D\nscenes. Given a set of sparse (3 or 4) joint locations (such as the location of\na person's hand and two feet) and a seed motion sequence in a 3D scene, our\nmethod generates a plausible motion sequence starting from the seed motion\nwhile satisfying the constraints imposed by the provided keypoints. We\ndecompose the continual motion synthesis problem into walking along paths and\ntransitioning in and out of the actions specified by the keypoints, which\nenables long generation of motions that satisfy scene constraints without\nexplicitly incorporating scene information. Our method is trained only using\nscene agnostic mocap data. As a result, our approach is deployable across 3D\nscenes with various geometries. For achieving plausible continual motion\nsynthesis without drift, our key contribution is to generate motion in a\ngoal-centric canonical coordinate frame where the next immediate target is\nsituated at the origin. Our model can generate long sequences of diverse\nactions such as grabbing, sitting and leaning chained together in arbitrary\norder, demonstrated on scenes of varying geometry: HPS, Replica, Matterport,\nScanNet and scenes represented using NeRFs. Several experiments demonstrate\nthat our method outperforms existing methods that navigate paths in 3D scenes.\nFor more results we urge the reader to watch our supplementary video available\nat: https://www.youtube.com/watch?v=0wZgsdyCT4A&t=1s\n","authors":["Aymen Mir","Xavier Puig","Angjoo Kanazawa","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2304.02061v4.pdf","comment":"Webpage: https://virtualhumans.mpi-inf.mpg.de/origin_2/"},{"id":"http://arxiv.org/abs/2501.12637v2","updated":"2025-02-02T08:42:39Z","published":"2025-01-22T04:53:12Z","title":"DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet\n  Transform","summary":"  Neural Radiance Fields (NeRF) has achieved superior performance in novel view\nsynthesis and 3D scene representation, but its practical applications are\nhindered by slow convergence and reliance on dense training views. To this end,\nwe present DWTNeRF, a unified framework based on Instant-NGP's fast-training\nhash encoding. It is coupled with regularization terms designed for few-shot\nNeRF, which operates on sparse training views. Our DWTNeRF additionally\nincludes a novel Discrete Wavelet loss that allows explicit prioritization of\nlow frequencies directly in the training objective, reducing few-shot NeRF's\noverfitting on high frequencies in earlier training stages. We also introduce a\nmodel-based approach, based on multi-head attention, that is compatible with\nINGP, which are sensitive to architectural changes. On the 3-shot LLFF\nbenchmark, DWTNeRF outperforms Vanilla INGP by 15.07% in PSNR, 24.45% in SSIM\nand 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot\napproaches for fast-converging implicit representations like INGP or 3DGS.\n","authors":["Hung Nguyen","Blark Runfa Li","Truong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2501.12637v2.pdf","comment":"17 pages, 13 figures, 8 tables"},{"id":"http://arxiv.org/abs/2501.06927v2","updated":"2025-02-02T05:08:46Z","published":"2025-01-12T20:36:39Z","title":"CULTURE3D: Cultural Landmarks and Terrain Dataset for 3D Applications","summary":"  In this paper, we present a large-scale fine-grained dataset using\nhigh-resolution images captured from locations worldwide. Compared to existing\ndatasets, our dataset offers a significantly larger size and includes a higher\nlevel of detail, making it uniquely suited for fine-grained 3D applications.\nNotably, our dataset is built using drone-captured aerial imagery, which\nprovides a more accurate perspective for capturing real-world site layouts and\narchitectural structures. By reconstructing environments with these detailed\nimages, our dataset supports applications such as the COLMAP format for\nGaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible\nwith widely-used techniques including SLAM, Multi-View Stereo, and Neural\nRadiance Fields (NeRF), enabling accurate 3D reconstructions and point clouds.\nThis makes it a benchmark for reconstruction and segmentation tasks. The\ndataset enables seamless integration with multi-modal data, supporting a range\nof 3D applications, from architectural reconstruction to virtual tourism. Its\nflexibility promotes innovation, facilitating breakthroughs in 3D modeling and\nanalysis.\n","authors":["Xinyi Zheng","Steve Zhang","Weizhe Lin","Aaron Zhang","Walterio W. Mayol-Cuevas","Junxiao Shen"],"pdf_url":"https://arxiv.org/pdf/2501.06927v2.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2405.07346v2","updated":"2025-02-02T08:58:09Z","published":"2024-05-12T17:45:11Z","title":"Quality Assessment for AI Generated Images with Instruction Tuning","summary":"  Artificial Intelligence Generated Content (AIGC) has grown rapidly in recent\nyears, among which AI-based image generation has gained widespread attention\ndue to its efficient and imaginative image creation ability. However,\nAI-generated Images (AIGIs) may not satisfy human preferences due to their\nunique distortions, which highlights the necessity to understand and evaluate\nhuman preferences for AIGIs. To this end, in this paper, we first establish a\nnovel Image Quality Assessment (IQA) database for AIGIs, termed AIGCIQA2023+,\nwhich provides human visual preference scores and detailed preference\nexplanations from three perspectives including quality, authenticity, and\ncorrespondence. Then, based on the constructed AIGCIQA2023+ database, this\npaper presents a MINT-IQA model to evaluate and explain human preferences for\nAIGIs from Multi-perspectives with INstruction Tuning. Specifically, the\nMINT-IQA model first learn and evaluate human preferences for AI-generated\nImages from multi-perspectives, then via the vision-language instruction tuning\nstrategy, MINT-IQA attains powerful understanding and explanation ability for\nhuman visual preference on AIGIs, which can be used for feedback to further\nimprove the assessment capabilities. Extensive experimental results demonstrate\nthat the proposed MINT-IQA model achieves state-of-the-art performance in\nunderstanding and evaluating human visual preferences for AIGIs, and the\nproposed model also achieves competing results on traditional IQA tasks\ncompared with state-of-the-art IQA models. The AIGCIQA2023+ database and\nMINT-IQA model are available at: https://github.com/IntMeGroup/MINT-IQA.\n","authors":["Jiarui Wang","Huiyu Duan","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2405.07346v2.pdf","comment":null}]},"2025-01-30T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.18595v1","updated":"2025-01-30T18:59:54Z","published":"2025-01-30T18:59:54Z","title":"ROSA: Reconstructing Object Shape and Appearance Textures by Adaptive\n  Detail Transfer","summary":"  Reconstructing an object's shape and appearance in terms of a mesh textured\nby a spatially-varying bidirectional reflectance distribution function (SVBRDF)\nfrom a limited set of images captured under collocated light is an ill-posed\nproblem. Previous state-of-the-art approaches either aim to reconstruct the\nappearance directly on the geometry or additionally use texture normals as part\nof the appearance features. However, this requires detailed but inefficiently\nlarge meshes, that would have to be simplified in a post-processing step, or\nsuffers from well-known limitations of normal maps such as missing shadows or\nincorrect silhouettes. Another limiting factor is the fixed and typically low\nresolution of the texture estimation resulting in loss of important surface\ndetails. To overcome these problems, we present ROSA, an inverse rendering\nmethod that directly optimizes mesh geometry with spatially adaptive mesh\nresolution solely based on the image data. In particular, we refine the mesh\nand locally condition the surface smoothness based on the estimated normal\ntexture and mesh curvature. In addition, we enable the reconstruction of fine\nappearance details in high-resolution textures through a pioneering tile-based\nmethod that operates on a single pre-trained decoder network but is not limited\nby the network output resolution.\n","authors":["Julian Kaltheuner","Patrick Stotko","Reinhard Klein"],"pdf_url":"https://arxiv.org/pdf/2501.18595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18315v1","updated":"2025-01-30T12:49:17Z","published":"2025-01-30T12:49:17Z","title":"Surface Defect Identification using Bayesian Filtering on a 3D Mesh","summary":"  This paper presents a CAD-based approach for automated surface defect\ndetection. We leverage the a-priori knowledge embedded in a CAD model and\nintegrate it with point cloud data acquired from commercially available stereo\nand depth cameras. The proposed method first transforms the CAD model into a\nhigh-density polygonal mesh, where each vertex represents a state variable in\n3D space. Subsequently, a weighted least squares algorithm is employed to\niteratively estimate the state of the scanned workpiece based on the captured\npoint cloud measurements. This framework offers the potential to incorporate\ninformation from diverse sensors into the CAD domain, facilitating a more\ncomprehensive analysis. Preliminary results demonstrate promising performance,\nwith the algorithm achieving convergence to a sub-millimeter standard deviation\nin the region of interest using only approximately 50 point cloud samples. This\nhighlights the potential of utilising commercially available stereo cameras for\nhigh-precision quality control applications.\n","authors":["Matteo Dalle Vedove","Matteo Bonetto","Edoardo Lamon","Luigi Palopoli","Matteo Saveriano","Daniele Fontanelli"],"pdf_url":"https://arxiv.org/pdf/2501.18315v1.pdf","comment":"Presented at IMEKO2024 World Congress, Hamburg, Germany, 26-29\n  October 2024"}],"Deblur":[{"id":"http://arxiv.org/abs/2501.18403v1","updated":"2025-01-30T14:58:33Z","published":"2025-01-30T14:58:33Z","title":"Efficient Transformer for High Resolution Image Motion Deblurring","summary":"  This paper presents a comprehensive study and improvement of the Restormer\narchitecture for high-resolution image motion deblurring. We introduce\narchitectural modifications that reduce model complexity by 18.4% while\nmaintaining or improving performance through optimized attention mechanisms.\nOur enhanced training pipeline incorporates additional transformations\nincluding color jitter, Gaussian blur, and perspective transforms to improve\nmodel robustness as well as a new frequency loss term. Extensive experiments on\nthe RealBlur-R, RealBlur-J, and Ultra-High-Definition Motion blurred (UHDM)\ndatasets demonstrate the effectiveness of our approach. The improved\narchitecture shows better convergence behavior and reduced training time while\nmaintaining competitive performance across challenging scenarios. We also\nprovide detailed ablation studies analyzing the impact of our modifications on\nmodel behavior and performance. Our results suggest that thoughtful\narchitectural simplification combined with enhanced training strategies can\nyield more efficient yet equally capable models for motion deblurring tasks.\nCode and Data Available at: https://github.com/hamzafer/image-deblurring\n","authors":["Amanturdieva Akmaral","Muhammad Hamza Zafar"],"pdf_url":"https://arxiv.org/pdf/2501.18403v1.pdf","comment":"14 pages, 18 figures Submitted as a preprint, no prior\n  journal/conference submission"}]},"2025-01-29T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.17987v1","updated":"2025-01-29T20:49:59Z","published":"2025-01-29T20:49:59Z","title":"Pressure Field Reconstruction with SIREN: A Mesh-Free Approach for Image\n  Velocimetry in Complex Noisy Environments","summary":"  This work presents a novel approach for pressure field reconstruction from\nimage velocimetry data using SIREN (Sinusoidal Representation Network),\nemphasizing its effectiveness as an implicit neural representation in noisy\nenvironments and its mesh-free nature. While we briefly assess two recently\nproposed methods - one-shot matrix-omnidirectional integration (OS-MODI) and\nGreen's function integral (GFI) - the primary focus is on the advantages of the\nSIREN approach. The OS-MODI technique performs well in noise-free conditions\nand with structured meshes but struggles when applied to unstructured meshes\nwith high aspect ratio. Similarly, the GFI method encounters difficulties due\nto singularities inherent from the Newtonian kernel. In contrast, the proposed\nSIREN approach is a mesh-free method that directly reconstructs the pressure\nfield, bypassing the need for an intrinsic grid connectivity and, hence,\navoiding the challenges associated with ill-conditioned cells and unstructured\nmeshes. This provides a distinct advantage over traditional mesh-based methods.\nMoreover, it is shown that changes in the architecture of the SIREN can be used\nto filter out inherent noise from velocimetry data. This work positions SIREN\nas a robust and versatile solution for pressure reconstruction, particularly in\nnoisy environments characterized by the absence of mesh structure, opening new\navenues for innovative applications in this field.\n","authors":["Renato F. Miotto","William R. Wolf","Fernando Zigunov"],"pdf_url":"https://arxiv.org/pdf/2501.17987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17655v1","updated":"2025-01-29T13:40:25Z","published":"2025-01-29T13:40:25Z","title":"FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for\n  Geometrically Accurate and Artifact-Reduced Reconstruction","summary":"  3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene\nreconstruction using 3D Gaussians. However, neither the centers nor surfaces of\nthe Gaussians are accurately aligned to the object surface, complicating their\ndirect use in point cloud and mesh reconstruction. Additionally, 3DGS typically\nproduces floater artifacts, increasing the number of Gaussians and storage\nrequirements. To address these issues, we present FeatureGS, which incorporates\nan additional geometric loss term based on an eigenvalue-derived 3D shape\nfeature into the optimization process of 3DGS. The goal is to improve geometric\naccuracy and enhance properties of planar surfaces with reduced structural\nentropy in local 3D neighborhoods.We present four alternative formulations for\nthe geometric loss term based on 'planarity' of Gaussians, as well as\n'planarity', 'omnivariance', and 'eigenentropy' of Gaussian neighborhoods. We\nprovide quantitative and qualitative evaluations on 15 scenes of the DTU\nbenchmark dataset focusing on following key aspects: Geometric accuracy and\nartifact-reduction, measured by the Chamfer distance, and memory efficiency,\nevaluated by the total number of Gaussians. Additionally, rendering quality is\nmonitored by Peak Signal-to-Noise Ratio. FeatureGS achieves a 30 % improvement\nin geometric accuracy, reduces the number of Gaussians by 90 %, and suppresses\nfloater artifacts, while maintaining comparable photometric rendering quality.\nThe geometric loss with 'planarity' from Gaussians provides the highest\ngeometric accuracy, while 'omnivariance' in Gaussian neighborhoods reduces\nfloater artifacts and number of Gaussians the most. This makes FeatureGS a\nstrong method for geometrically accurate, artifact-reduced and memory-efficient\n3D scene reconstruction, enabling the direct use of Gaussian centers for\ngeometric representation.\n","authors":["Miriam Jäger","Markus Hillemann","Boris Jutzi"],"pdf_url":"https://arxiv.org/pdf/2501.17655v1.pdf","comment":"16 pages, 9 figures, 7 tables"}]},"2025-01-28T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.17085v1","updated":"2025-01-28T17:14:13Z","published":"2025-01-28T17:14:13Z","title":"Evaluating CrowdSplat: Perceived Level of Detail for Gaussian Crowds","summary":"  Efficient and realistic crowd rendering is an important element of many\nreal-time graphics applications such as Virtual Reality (VR) and games. To this\nend, Levels of Detail (LOD) avatar representations such as polygonal meshes,\nimage-based impostors, and point clouds have been proposed and evaluated. More\nrecently, 3D Gaussian Splatting has been explored as a potential method for\nreal-time crowd rendering. In this paper, we present a two-alternative forced\nchoice (2AFC) experiment that aims to determine the perceived quality of 3D\nGaussian avatars. Three factors were explored: Motion, LOD (i.e., #Gaussians),\nand the avatar height in Pixels (corresponding to the viewing distance).\nParticipants viewed pairs of animated 3D Gaussian avatars and were tasked with\nchoosing the most detailed one. Our findings can inform the optimization of LOD\nstrategies in Gaussian-based crowd rendering, thereby helping to achieve\nefficient rendering while maintaining visual quality in real-time applications.\n","authors":["Xiaohan Sun","Yinghan Xu","John Dingliana","Carol O'Sullivan"],"pdf_url":"https://arxiv.org/pdf/2501.17085v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.07688v2","updated":"2025-01-28T16:02:30Z","published":"2024-10-10T07:54:17Z","title":"PokeFlex: A Real-World Dataset of Volumetric Deformable Objects for\n  Robotics","summary":"  Data-driven methods have shown great potential in solving challenging\nmanipulation tasks; however, their application in the domain of deformable\nobjects has been constrained, in part, by the lack of data. To address this\nlack, we propose PokeFlex, a dataset featuring real-world multimodal data that\nis paired and annotated. The modalities include 3D textured meshes, point\nclouds, RGB images, and depth maps. Such data can be leveraged for several\ndownstream tasks, such as online 3D mesh reconstruction, and it can potentially\nenable underexplored applications such as the real-world deployment of\ntraditional control methods based on mesh simulations. To deal with the\nchallenges posed by real-world 3D mesh reconstruction, we leverage a\nprofessional volumetric capture system that allows complete 360{\\deg}\nreconstruction. PokeFlex consists of 18 deformable objects with varying\nstiffness and shapes. Deformations are generated by dropping objects onto a\nflat surface or by poking the objects with a robot arm. Interaction wrenches\nand contact locations are also reported for the latter case. Using different\ndata modalities, we demonstrated a use case for our dataset training models\nthat, given the novelty of the multimodal nature of Pokeflex, constitute the\nstate-of-the-art in multi-object online template-based mesh reconstruction from\nmultimodal data, to the best of our knowledge. We refer the reader to our\nwebsite ( https://pokeflex-dataset.github.io/ ) for further demos and examples.\n","authors":["Jan Obrist","Miguel Zamora","Hehui Zheng","Ronan Hinchet","Firat Ozdemir","Juan Zarate","Robert K. Katzschmann","Stelian Coros"],"pdf_url":"https://arxiv.org/pdf/2410.07688v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2501.15371v2","updated":"2025-01-28T15:56:14Z","published":"2025-01-26T02:52:46Z","title":"Acquiring Submillimeter-Accurate Multi-Task Vision Datasets for\n  Computer-Assisted Orthopedic Surgery","summary":"  Advances in computer vision, particularly in optical image-based 3D\nreconstruction and feature matching, enable applications like marker-less\nsurgical navigation and digitization of surgery. However, their development is\nhindered by a lack of suitable datasets with 3D ground truth. This work\nexplores an approach to generating realistic and accurate ex vivo datasets\ntailored for 3D reconstruction and feature matching in open orthopedic surgery.\nA set of posed images and an accurately registered ground truth surface mesh of\nthe scene are required to develop vision-based 3D reconstruction and matching\nmethods suitable for surgery. We propose a framework consisting of three core\nsteps and compare different methods for each step: 3D scanning, calibration of\nviewpoints for a set of high-resolution RGB images, and an optical-based method\nfor scene registration. We evaluate each step of this framework on an ex vivo\nscoliosis surgery using a pig spine, conducted under real operating room\nconditions. A mean 3D Euclidean error of 0.35 mm is achieved with respect to\nthe 3D ground truth. The proposed method results in submillimeter accurate 3D\nground truths and surgical images with a spatial resolution of 0.1 mm. This\nopens the door to acquiring future surgical datasets for high-precision\napplications.\n","authors":["Emma Most","Jonas Hein","Frédéric Giraud","Nicola A. Cavalcanti","Lukas Zingg","Baptiste Brument","Nino Louman","Fabio Carrillo","Philipp Fürnstahl","Lilian Calvet"],"pdf_url":"https://arxiv.org/pdf/2501.15371v2.pdf","comment":"18 pages, 12 figures. Submitted to the 16th International Conference\n  on Information Processing in Computer-Assisted Interventions (IPCAI 2025)"},{"id":"http://arxiv.org/abs/2501.16312v2","updated":"2025-01-28T12:52:41Z","published":"2025-01-27T18:49:38Z","title":"LinPrim: Linear Primitives for Differentiable Volumetric Rendering","summary":"  Volumetric rendering has become central to modern novel view synthesis\nmethods, which use differentiable rendering to optimize 3D scene\nrepresentations directly from observed views. While many recent works build on\nNeRF or 3D Gaussians, we explore an alternative volumetric scene\nrepresentation. More specifically, we introduce two new scene representations\nbased on linear primitives-octahedra and tetrahedra-both of which define\nhomogeneous volumes bounded by triangular faces. This formulation aligns\nnaturally with standard mesh-based tools, minimizing overhead for downstream\napplications. To optimize these primitives, we present a differentiable\nrasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based\noptimization while maintaining realtime rendering capabilities. Through\nexperiments on real-world datasets, we demonstrate comparable performance to\nstate-of-the-art volumetric methods while requiring fewer primitives to achieve\nsimilar reconstruction fidelity. Our findings provide insights into the\ngeometry of volumetric rendering and suggest that adopting explicit polyhedra\ncan expand the design space of scene representations.\n","authors":["Nicolas von Lützow","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2501.16312v2.pdf","comment":"Project page: https://nicolasvonluetzow.github.io/LinPrim ; Project\n  video: https://youtu.be/P2yeHwmGaeM"},{"id":"http://arxiv.org/abs/2408.01826v3","updated":"2025-01-28T02:31:02Z","published":"2024-08-03T17:18:26Z","title":"GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent\n  Diffusion Transformer","summary":"  Speech-driven talking head generation is a critical yet challenging task with\napplications in augmented reality and virtual human modeling. While recent\napproaches using autoregressive and diffusion-based models have achieved\nnotable progress, they often suffer from modality inconsistencies, particularly\nmisalignment between audio and mesh, leading to reduced motion diversity and\nlip-sync accuracy. To address this, we propose GLDiTalker, a novel\nspeech-driven 3D facial animation model based on a Graph Latent Diffusion\nTransformer. GLDiTalker resolves modality misalignment by diffusing signals\nwithin a quantized spatiotemporal latent space. It employs a two-stage training\npipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync\naccuracy, while the Space-Time Powered Latent Diffusion Stage enhances motion\ndiversity. Together, these stages enable GLDiTalker to generate realistic,\ntemporally stable 3D facial animations. Extensive evaluations on standard\nbenchmarks demonstrate that GLDiTalker outperforms existing methods, achieving\nsuperior results in both lip-sync accuracy and motion diversity.\n","authors":["Yihong Lin","Zhaoxin Fan","Xianjia Wu","Lingyu Xiong","Liang Peng","Xiandong Li","Wenxiong Kang","Songju Lei","Huang Xu"],"pdf_url":"https://arxiv.org/pdf/2408.01826v3.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.11518v2","updated":"2025-01-28T02:29:29Z","published":"2024-08-21T10:51:12Z","title":"EmoFace: Emotion-Content Disentangled Speech-Driven 3D Talking Face\n  Animation","summary":"  The creation of increasingly vivid 3D talking face has become a hot topic in\nrecent years. Currently, most speech-driven works focus on lip synchronisation\nbut neglect to effectively capture the correlations between emotions and facial\nmotions. To address this problem, we propose a two-stream network called\nEmoFace, which consists of an emotion branch and a content branch. EmoFace\nemploys a novel Mesh Attention mechanism to analyse and fuse the emotion\nfeatures and content features. Particularly, a newly designed spatio-temporal\ngraph-based convolution, SpiralConv3D, is used in Mesh Attention to learn\npotential temporal and spatial feature dependencies between mesh vertices. In\naddition, to the best of our knowledge, it is the first time to introduce a new\nself-growing training scheme with intermediate supervision to dynamically\nadjust the ratio of groundtruth adopted in the 3D face animation task.\nComprehensive quantitative and qualitative evaluations on our high-quality 3D\nemotional facial animation dataset, 3D-RAVDESS ($4.8863\\times 10^{-5}$mm for\nLVE and $0.9509\\times 10^{-5}$mm for EVE), together with the public dataset\nVOCASET ($2.8669\\times 10^{-5}$mm for LVE and $0.4664\\times 10^{-5}$mm for\nEVE), demonstrate that our approach achieves state-of-the-art performance.\n","authors":["Yihong Lin","Liang Peng","Xianjia Wu","Jianqiao Hu","Xiandong Li","Wenxiong Kang","Songju Lei","Huang Xu"],"pdf_url":"https://arxiv.org/pdf/2408.11518v2.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2501.16312v2","updated":"2025-01-28T12:52:41Z","published":"2025-01-27T18:49:38Z","title":"LinPrim: Linear Primitives for Differentiable Volumetric Rendering","summary":"  Volumetric rendering has become central to modern novel view synthesis\nmethods, which use differentiable rendering to optimize 3D scene\nrepresentations directly from observed views. While many recent works build on\nNeRF or 3D Gaussians, we explore an alternative volumetric scene\nrepresentation. More specifically, we introduce two new scene representations\nbased on linear primitives-octahedra and tetrahedra-both of which define\nhomogeneous volumes bounded by triangular faces. This formulation aligns\nnaturally with standard mesh-based tools, minimizing overhead for downstream\napplications. To optimize these primitives, we present a differentiable\nrasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based\noptimization while maintaining realtime rendering capabilities. Through\nexperiments on real-world datasets, we demonstrate comparable performance to\nstate-of-the-art volumetric methods while requiring fewer primitives to achieve\nsimilar reconstruction fidelity. Our findings provide insights into the\ngeometry of volumetric rendering and suggest that adopting explicit polyhedra\ncan expand the design space of scene representations.\n","authors":["Nicolas von Lützow","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2501.16312v2.pdf","comment":"Project page: https://nicolasvonluetzow.github.io/LinPrim ; Project\n  video: https://youtu.be/P2yeHwmGaeM"}]},"2025-01-27T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.16550v1","updated":"2025-01-27T22:48:36Z","published":"2025-01-27T22:48:36Z","title":"PhysAnimator: Physics-Guided Generative Cartoon Animation","summary":"  Creating hand-drawn animation sequences is labor-intensive and demands\nprofessional expertise. We introduce PhysAnimator, a novel approach for\ngenerating physically plausible meanwhile anime-stylized animation from static\nanime illustrations. Our method seamlessly integrates physics-based simulations\nwith data-driven generative models to produce dynamic and visually compelling\nanimations. To capture the fluidity and exaggeration characteristic of anime,\nwe perform image-space deformable body simulations on extracted mesh\ngeometries. We enhance artistic control by introducing customizable energy\nstrokes and incorporating rigging point support, enabling the creation of\ntailored animation effects such as wind interactions. Finally, we extract and\nwarp sketches from the simulation sequence, generating a texture-agnostic\nrepresentation, and employ a sketch-guided video diffusion model to synthesize\nhigh-quality animation frames. The resulting animations exhibit temporal\nconsistency and visual plausibility, demonstrating the effectiveness of our\nmethod in creating dynamic anime-style animations.\n","authors":["Tianyi Xie","Yiwei Zhao","Ying Jiang","Chenfanfu Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.16550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16525v1","updated":"2025-01-27T21:50:12Z","published":"2025-01-27T21:50:12Z","title":"Multi-Objective Deep-Learning-based Biomechanical Deformable Image\n  Registration with MOREA","summary":"  When choosing a deformable image registration (DIR) approach for images with\nlarge deformations and content mismatch, the realism of found transformations\noften needs to be traded off against the required runtime. DIR approaches using\ndeep learning (DL) techniques have shown remarkable promise in instantly\npredicting a transformation. However, on difficult registration problems, the\nrealism of these transformations can fall short. DIR approaches using\nbiomechanical, finite element modeling (FEM) techniques can find more realistic\ntransformations, but tend to require much longer runtimes. This work proposes\nthe first hybrid approach to combine them, with the aim of getting the best of\nboth worlds. This hybrid approach, called DL-MOREA, combines a recently\nintroduced multi-objective DL-based DIR approach which leverages the VoxelMorph\nframework, called DL-MODIR, with MOREA, an evolutionary algorithm-based,\nmulti-objective DIR approach in which a FEM-like biomechanical mesh\ntransformation model is used. In our proposed hybrid approach, the DL results\nare used to smartly initialize MOREA, with the aim of more efficiently\noptimizing its mesh transformation model. We empirically compare DL-MOREA\nagainst its components, DL-MODIR and MOREA, on CT scan pairs capturing large\nbladder filling differences of 15 cervical cancer patients. While MOREA\nrequires a median runtime of 45 minutes, DL-MOREA can already find high-quality\ntransformations after 5 minutes. Compared to the DL-MODIR transformations, the\ntransformations found by DL-MOREA exhibit far less folding and improve or\npreserve the bladder contour distance error.\n","authors":["Georgios Andreadis","Eduard Ruiz Munné","Thomas H. W. Bäck","Peter A. N. Bosman","Tanja Alderliesten"],"pdf_url":"https://arxiv.org/pdf/2501.16525v1.pdf","comment":"Pre-print for the SPIE Medical Imaging: Image Processing Conference"}],"Deblur":[{"id":"http://arxiv.org/abs/2501.15808v1","updated":"2025-01-27T06:28:45Z","published":"2025-01-27T06:28:45Z","title":"ClearSight: Human Vision-Inspired Solutions for Event-Based Motion\n  Deblurring","summary":"  Motion deblurring addresses the challenge of image blur caused by camera or\nscene movement. Event cameras provide motion information that is encoded in the\nasynchronous event streams. To efficiently leverage the temporal information of\nevent streams, we employ Spiking Neural Networks (SNNs) for motion feature\nextraction and Artificial Neural Networks (ANNs) for color information\nprocessing. Due to the non-uniform distribution and inherent redundancy of\nevent data, existing cross-modal feature fusion methods exhibit certain\nlimitations. Inspired by the visual attention mechanism in the human visual\nsystem, this study introduces a bioinspired dual-drive hybrid network (BDHNet).\nSpecifically, the Neuron Configurator Module (NCM) is designed to dynamically\nadjusts neuron configurations based on cross-modal features, thereby focusing\nthe spikes in blurry regions and adapting to varying blurry scenarios\ndynamically. Additionally, the Region of Blurry Attention Module (RBAM) is\nintroduced to generate a blurry mask in an unsupervised manner, effectively\nextracting motion clues from the event features and guiding more accurate\ncross-modal feature fusion. Extensive subjective and objective evaluations\ndemonstrate that our method outperforms current state-of-the-art methods on\nboth synthetic and real-world datasets.\n","authors":["Xiaopeng Lin","Yulong Huang","Hongwei Ren","Zunchang Liu","Yue Zhou","Haotian Fu","Bojun Cheng"],"pdf_url":"https://arxiv.org/pdf/2501.15808v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2405.12114v3","updated":"2025-01-27T04:02:25Z","published":"2024-05-20T15:29:26Z","title":"A New Cross-Space Total Variation Regularization Model for Color Image\n  Restoration with Quaternion Blur Operator","summary":"  The cross-channel deblurring problem in color image processing is difficult\nto solve due to the complex coupling and structural blurring of color pixels.\nUntil now, there are few efficient algorithms that can reduce color artifacts\nin deblurring process. To solve this challenging problem, we present a novel\ncross-space total variation (CSTV) regularization model for color image\ndeblurring by introducing a quaternion blur operator and a cross-color space\nregularization functional. The existence and uniqueness of the solution are\nproved and a new L-curve method is proposed to find a balance of regularization\nterms on different color spaces. The Euler-Lagrange equation is derived to show\nthat CSTV has taken into account the coupling of all color channels and the\nlocal smoothing within each color channel. A quaternion operator splitting\nmethod is firstly proposed to enhance the ability of color artifacts reduction\nof the CSTV regularization model. This strategy also applies to the well-known\ncolor deblurring models. Numerical experiments on color image databases\nillustrate the efficiency and effectiveness of the new model and algorithms.\nThe color images restored by them successfully maintain the color and spatial\ninformation and are of higher quality in terms of PSNR, SSIM, MSE and CIEde2000\nthan the restorations of the-state-of-the-art methods.\n","authors":["Zhigang Jia","Yuelian Xiang","Meixiang Zhao","Tingting Wu","Michael K. Ng"],"pdf_url":"https://arxiv.org/pdf/2405.12114v3.pdf","comment":"15pages,14figures"}]},"2025-01-26T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.15445v1","updated":"2025-01-26T08:22:44Z","published":"2025-01-26T08:22:44Z","title":"StochSync: Stochastic Diffusion Synchronization for Image Generation in\n  Arbitrary Spaces","summary":"  We propose a zero-shot method for generating images in arbitrary spaces\n(e.g., a sphere for 360{\\deg} panoramas and a mesh surface for texture) using a\npretrained image diffusion model. The zero-shot generation of various visual\ncontent using a pretrained image diffusion model has been explored mainly in\ntwo directions. First, Diffusion Synchronization-performing reverse diffusion\nprocesses jointly across different projected spaces while synchronizing them in\nthe target space-generates high-quality outputs when enough conditioning is\nprovided, but it struggles in its absence. Second, Score Distillation\nSampling-gradually updating the target space data through gradient\ndescent-results in better coherence but often lacks detail. In this paper, we\nreveal for the first time the interconnection between these two methods while\nhighlighting their differences. To this end, we propose StochSync, a novel\napproach that combines the strengths of both, enabling effective performance\nwith weak conditioning. Our experiments demonstrate that StochSync provides the\nbest performance in 360{\\deg} panorama generation (where image conditioning is\nnot given), outperforming previous finetuning-based methods, and also delivers\ncomparable results in 3D mesh texturing (where depth conditioning is provided)\nwith previous methods.\n","authors":["Kyeongmin Yeo","Jaihoon Kim","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2501.15445v1.pdf","comment":"Project page: https://stochsync.github.io/ (ICLR 2025)"}],"Deblur":[{"id":"http://arxiv.org/abs/2209.14770v3","updated":"2025-01-26T10:01:33Z","published":"2022-09-29T13:28:34Z","title":"R2C-GAN: Restore-to-Classify Generative Adversarial Networks for Blind\n  X-Ray Restoration and COVID-19 Classification","summary":"  Restoration of poor quality images with a blended set of artifacts plays a\nvital role for a reliable diagnosis. Existing studies have focused on specific\nrestoration problems such as image deblurring, denoising, and exposure\ncorrection where there is usually a strong assumption on the artifact type and\nseverity. As a pioneer study in blind X-ray restoration, we propose a joint\nmodel for generic image restoration and classification: Restore-to-Classify\nGenerative Adversarial Networks (R2C-GANs). Such a jointly optimized model\nkeeps any disease intact after the restoration. Therefore, this will naturally\nlead to a higher diagnosis performance thanks to the improved X-ray image\nquality. To accomplish this crucial objective, we define the restoration task\nas an Image-to-Image translation problem from poor quality having noisy,\nblurry, or over/under-exposed images to high quality image domain. The proposed\nR2C-GAN model is able to learn forward and inverse transforms between the two\ndomains using unpaired training samples. Simultaneously, the joint\nclassification preserves the disease label during restoration. Moreover, the\nR2C-GANs are equipped with operational layers/neurons reducing the network\ndepth and further boosting both restoration and classification performances.\nThe proposed joint model is extensively evaluated over the QaTa-COV19 dataset\nfor Coronavirus Disease 2019 (COVID-19) classification. The proposed\nrestoration approach achieves over 90% F1-Score which is significantly higher\nthan the performance of any deep model. Moreover, in the qualitative analysis,\nthe restoration performance of R2C-GANs is approved by a group of medical\ndoctors. We share the software implementation at\nhttps://github.com/meteahishali/R2C-GAN.\n","authors":["Mete Ahishali","Aysen Degerli","Serkan Kiranyaz","Tahir Hamid","Rashid Mazhar","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2209.14770v3.pdf","comment":null}]},"2025-01-25T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2311.12059v3","updated":"2025-01-25T07:34:46Z","published":"2023-11-18T16:14:08Z","title":"Mesh Watermark Removal Attack and Mitigation: A Novel Perspective of\n  Function Space","summary":"  Mesh watermark embeds secret messages in 3D meshes and decodes the message\nfrom watermarked meshes for ownership verification. Current watermarking\nmethods directly hide secret messages in vertex and face sets of meshes.\nHowever, mesh is a discrete representation that uses vertex and face sets to\ndescribe a continuous signal, which can be discretized in other discrete\nrepresentations with different vertex and face sets. This raises the question\nof whether the watermark can still be verified on the different discrete\nrepresentations of the watermarked mesh. We conduct this research in an\nattack-then-defense manner by proposing a novel function space mesh watermark\nremoval attack FuncEvade and then mitigating it through function space mesh\nwatermarking FuncMark. In detail, FuncEvade generates a different discrete\nrepresentation of a watermarked mesh by extracting it from the signed distance\nfunction of the watermarked mesh. We observe that the generated mesh can evade\nALL previous watermarking methods. FuncMark mitigates FuncEvade by watermarking\nsigned distance function through message-guided deformation. Such deformation\ncan survive isosurfacing and thus be inherited by the extracted meshes for\nfurther watermark decoding. Extensive experiments demonstrate that FuncEvade\nachieves 100% evasion rate among all previous watermarking methods while\nachieving only 0.3% evasion rate on FuncMark. Besides, our FuncMark performs\nsimilarly on other metrics compared to state-of-the-art mesh watermarking\nmethods.\n","authors":["Xingyu Zhu","Guanhui Ye","Chengdong Dong","Xiapu Luo","Shiyao Zhang","Xuetao Wei"],"pdf_url":"https://arxiv.org/pdf/2311.12059v3.pdf","comment":"Accepted by AAAI25"}]},"2025-01-24T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.14726v1","updated":"2025-01-24T18:59:15Z","published":"2025-01-24T18:59:15Z","title":"Relightable Full-Body Gaussian Codec Avatars","summary":"  We propose Relightable Full-Body Gaussian Codec Avatars, a new approach for\nmodeling relightable full-body avatars with fine-grained details including face\nand hands. The unique challenge for relighting full-body avatars lies in the\nlarge deformations caused by body articulation and the resulting impact on\nappearance caused by light transport. Changes in body pose can dramatically\nchange the orientation of body surfaces with respect to lights, resulting in\nboth local appearance changes due to changes in local light transport\nfunctions, as well as non-local changes due to occlusion between body parts. To\naddress this, we decompose the light transport into local and non-local\neffects. Local appearance changes are modeled using learnable zonal harmonics\nfor diffuse radiance transfer. Unlike spherical harmonics, zonal harmonics are\nhighly efficient to rotate under articulation. This allows us to learn diffuse\nradiance transfer in a local coordinate frame, which disentangles the local\nradiance transfer from the articulation of the body. To account for non-local\nappearance changes, we introduce a shadow network that predicts shadows given\nprecomputed incoming irradiance on a base mesh. This facilitates the learning\nof non-local shadowing between the body parts. Finally, we use a deferred\nshading approach to model specular radiance transfer and better capture\nreflections and highlights such as eye glints. We demonstrate that our approach\nsuccessfully models both the local and non-local light transport required for\nrelightable full-body avatars, with a superior generalization ability under\nnovel illumination conditions and unseen poses.\n","authors":["Shaofei Wang","Tomas Simon","Igor Santesteban","Timur Bagautdinov","Junxuan Li","Vasu Agrawal","Fabian Prada","Shoou-I Yu","Pace Nalbone","Matt Gramlich","Roman Lubachersky","Chenglei Wu","Javier Romero","Jason Saragih","Michael Zollhoefer","Andreas Geiger","Siyu Tang","Shunsuke Saito"],"pdf_url":"https://arxiv.org/pdf/2501.14726v1.pdf","comment":"14 pages, 9 figures. Project page:\n  https://neuralbodies.github.io/RFGCA"},{"id":"http://arxiv.org/abs/2501.10455v2","updated":"2025-01-24T13:00:28Z","published":"2025-01-14T20:02:59Z","title":"PhyDeformer: High-Quality Non-Rigid Garment Registration with\n  Physics-Awareness","summary":"  We present PhyDeformer, a new deformation method for high-quality garment\nmesh registration. It operates in two phases: In the first phase, a garment\ngrading is performed to achieve a coarse 3D alignment between the mesh template\nand the target mesh, accounting for proportional scaling and fit (e.g. length,\nsize). Then, the graded mesh is refined to align with the fine-grained details\nof the 3D target through an optimization coupled with the Jacobian-based\ndeformation framework. Both quantitative and qualitative evaluations on\nsynthetic and real garments highlight the effectiveness of our method.\n","authors":["Boyang Yu","Frederic Cordier","Hyewon Seo"],"pdf_url":"https://arxiv.org/pdf/2501.10455v2.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2501.14646v1","updated":"2025-01-24T17:14:25Z","published":"2025-01-24T17:14:25Z","title":"SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human\n  Pose and Talking Head Animation","summary":"  Generating talking avatar driven by audio remains a significant challenge.\nExisting methods typically require high computational costs and often lack\nsufficient facial detail and realism, making them unsuitable for applications\nthat demand high real-time performance and visual quality. Additionally, while\nsome methods can synchronize lip movement, they still face issues with\nconsistency between facial expressions and upper body movement, particularly\nduring silent periods. In this paper, we introduce SyncAnimation, the first\nNeRF-based method that achieves audio-driven, stable, and real-time generation\nof speaking avatar by combining generalized audio-to-pose matching and\naudio-to-expression synchronization. By integrating AudioPose Syncer and\nAudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression\ngeneration, progressively producing audio-synchronized upper body, head, and\nlip shapes. Furthermore, the High-Synchronization Human Renderer ensures\nseamless integration of the head and upper body, and achieves audio-sync lip.\nThe project page can be found at https://syncanimation.github.io\n","authors":["Yujian Liu","Shidang Xu","Jing Guo","Dingbin Wang","Zairan Wang","Xianfeng Tan","Xiaoli Liu"],"pdf_url":"https://arxiv.org/pdf/2501.14646v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2309.13607v3","updated":"2025-01-24T16:37:45Z","published":"2023-09-24T11:04:50Z","title":"MM-NeRF: Multimodal-Guided 3D Multi-Style Transfer of Neural Radiance\n  Field","summary":"  3D style transfer aims to generate stylized views of 3D scenes with specified\nstyles, which requires high-quality generating and keeping multi-view\nconsistency. Existing methods still suffer the challenges of high-quality\nstylization with texture details and stylization with multimodal guidance. In\nthis paper, we reveal that the common training method of stylization with NeRF,\nwhich generates stylized multi-view supervision by 2D style transfer models,\ncauses the same object in supervision to show various states (color tone,\ndetails, etc.) in different views, leading NeRF to tend to smooth the texture\ndetails, further resulting in low-quality rendering for 3D multi-style\ntransfer. To tackle these problems, we propose a novel Multimodal-guided 3D\nMulti-style transfer of NeRF, termed MM-NeRF. First, MM-NeRF projects\nmultimodal guidance into a unified space to keep the multimodal styles\nconsistency and extracts multimodal features to guide the 3D stylization.\nSecond, a novel multi-head learning scheme is proposed to relieve the\ndifficulty of learning multi-style transfer, and a multi-view style consistent\nloss is proposed to track the inconsistency of multi-view supervision data.\nFinally, a novel incremental learning mechanism is proposed to generalize\nMM-NeRF to any new style with small costs. Extensive experiments on several\nreal-world datasets show that MM-NeRF achieves high-quality 3D multi-style\nstylization with multimodal guidance, and keeps multi-view consistency and\nstyle consistency between multimodal guidance.\n","authors":["Zijiang Yang","Zhongwei Qiu","Chang Xu","Dongmei Fu"],"pdf_url":"https://arxiv.org/pdf/2309.13607v3.pdf","comment":"Published in: IEEE Transactions on Visualization and Computer\n  Graphics"}],"IQA":[{"id":"http://arxiv.org/abs/2501.14264v1","updated":"2025-01-24T06:05:47Z","published":"2025-01-24T06:05:47Z","title":"CDI: Blind Image Restoration Fidelity Evaluation based on Consistency\n  with Degraded Image","summary":"  Recent advancements in Blind Image Restoration (BIR) methods, based on\nGenerative Adversarial Networks and Diffusion Models, have significantly\nimproved visual quality. However, they present significant challenges for Image\nQuality Assessment (IQA), as the existing Full-Reference IQA methods often rate\nimages with high perceptual quality poorly. In this paper, we reassess the\nSolution Non-Uniqueness and Degradation Indeterminacy issues of BIR, and\npropose constructing a specific BIR IQA system. In stead of directly comparing\na restored image with a reference image, the BIR IQA evaluates fidelity by\ncalculating the Consistency with Degraded Image (CDI). Specifically, we propose\na wavelet domain Reference Guided CDI algorithm, which can acquire the\nconsistency with a degraded image for various types without requiring knowledge\nof degradation parameters. The supported degradation types include down\nsampling, blur, noise, JPEG and complex combined degradations etc. In addition,\nwe propose a Reference Agnostic CDI, enabling BIR fidelity evaluation without\nreference images. Finally, in order to validate the rationality of CDI, we\ncreate a new Degraded Images Switch Display Comparison Dataset (DISDCD) for\nsubjective evaluation of BIR fidelity. Experiments conducted on DISDCD verify\nthat CDI is markedly superior to common Full Reference IQA methods for BIR\nfidelity evaluation. The source code and the DISDCD dataset will be publicly\navailable shortly.\n","authors":["Xiaojun Tang","Jingru Wang","Guangwei Huang","Guannan Chen","Rui Zheng","Lian Huai","Yuyu Liu","Xingqun Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.14264v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2401.00766v5","updated":"2025-01-24T08:47:23Z","published":"2024-01-01T14:14:35Z","title":"Exposure Bracketing Is All You Need For A High-Quality Image","summary":"  It is highly desired but challenging to acquire high-quality photos with\nclear content in low-light environments. Although multi-image processing\nmethods (using burst, dual-exposure, or multi-exposure images) have made\nsignificant progress in addressing this issue, they typically focus on specific\nrestoration or enhancement problems, and do not fully explore the potential of\nutilizing multiple images. Motivated by the fact that multi-exposure images are\ncomplementary in denoising, deblurring, high dynamic range imaging, and\nsuper-resolution, we propose to utilize exposure bracketing photography to get\na high-quality image by combining these tasks in this work. Due to the\ndifficulty in collecting real-world pairs, we suggest a solution that first\npre-trains the model with synthetic paired data and then adapts it to\nreal-world unlabeled images. In particular, a temporally modulated recurrent\nnetwork (TMRNet) and self-supervised adaptation method are proposed. Moreover,\nwe construct a data simulation pipeline to synthesize pairs and collect\nreal-world images from 200 nighttime scenarios. Experiments on both datasets\nshow that our method performs favorably against the state-of-the-art\nmulti-image processing ones. Code and datasets are available at\nhttps://github.com/cszhilu1998/BracketIRE.\n","authors":["Zhilu Zhang","Shuohao Zhang","Renlong Wu","Zifei Yan","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2401.00766v5.pdf","comment":"ICLR 2025"}]},"2025-01-22T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.12202v2","updated":"2025-01-22T12:01:39Z","published":"2025-01-21T15:16:54Z","title":"Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D\n  Assets Generation","summary":"  We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for\ngenerating high-resolution textured 3D assets. This system includes two\nfoundation components: a large-scale shape generation model -- Hunyuan3D-DiT,\nand a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape\ngenerative model, built on a scalable flow-based diffusion transformer, aims to\ncreate geometry that properly aligns with a given condition image, laying a\nsolid foundation for downstream applications. The texture synthesis model,\nbenefiting from strong geometric and diffusion priors, produces high-resolution\nand vibrant texture maps for either generated or hand-crafted meshes.\nFurthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production\nplatform that simplifies the re-creation process of 3D assets. It allows both\nprofessional and amateur users to manipulate or even animate their meshes\nefficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0\noutperforms previous state-of-the-art models, including the open-source models\nand closed-source models in geometry details, condition alignment, texture\nquality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps\nin the open-source 3D community for large-scale foundation generative models.\nThe code and pre-trained weights of our models are available at:\nhttps://github.com/Tencent/Hunyuan3D-2\n","authors":["Zibo Zhao","Zeqiang Lai","Qingxiang Lin","Yunfei Zhao","Haolin Liu","Shuhui Yang","Yifei Feng","Mingxin Yang","Sheng Zhang","Xianghui Yang","Huiwen Shi","Sicong Liu","Junta Wu","Yihang Lian","Fan Yang","Ruining Tang","Zebin He","Xinzhou Wang","Jian Liu","Xuhui Zuo","Zhuo Chen","Biwen Lei","Haohan Weng","Jing Xu","Yiling Zhu","Xinhai Liu","Lixin Xu","Changrong Hu","Tianyu Huang","Lifu Wang","Jihong Zhang","Meng Chen","Liang Dong","Yiwen Jia","Yulin Cai","Jiaao Yu","Yixuan Tang","Hao Zhang","Zheng Ye","Peng He","Runzhou Wu","Chao Zhang","Yonghao Tan","Jie Xiao","Yangyu Tao","Jianchen Zhu","Jinbao Xue","Kai Liu","Chongqing Zhao","Xinming Wu","Zhichao Hu","Lei Qin","Jianbing Peng","Zhan Li","Minghui Chen","Xipeng Zhang","Lin Niu","Paige Wang","Yingkai Wang","Haozhao Kuang","Zhongyi Fan","Xu Zheng","Weihao Zhuang","YingPing He","Tian Liu","Yong Yang","Di Wang","Yuhong Liu","Jie Jiang","Jingwei Huang","Chunchao Guo"],"pdf_url":"https://arxiv.org/pdf/2501.12202v2.pdf","comment":"GitHub link: https://github.com/Tencent/Hunyuan3D-2"}],"NeRF":[{"id":"http://arxiv.org/abs/2501.13104v1","updated":"2025-01-22T18:59:10Z","published":"2025-01-22T18:59:10Z","title":"Neural Radiance Fields for the Real World: A Survey","summary":"  Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since\nrelease. NeRFs can effectively reconstruct complex 3D scenes from 2D images,\nadvancing different fields and applications such as scene understanding, 3D\ncontent generation, and robotics. Despite significant research progress, a\nthorough review of recent innovations, applications, and challenges is lacking.\nThis survey compiles key theoretical advancements and alternative\nrepresentations and investigates emerging challenges. It further explores\napplications on reconstruction, highlights NeRFs' impact on computer vision and\nrobotics, and reviews essential datasets and toolkits. By identifying gaps in\nthe literature, this survey discusses open challenges and offers directions for\nfuture research.\n","authors":["Wenhui Xiao","Remi Chierchia","Rodrigo Santa Cruz","Xuesong Li","David Ahmedt-Aristizabal","Olivier Salvado","Clinton Fookes","Leo Lebrat"],"pdf_url":"https://arxiv.org/pdf/2501.13104v1.pdf","comment":null}],"HDR":[{"id":"http://arxiv.org/abs/2501.12898v1","updated":"2025-01-22T14:18:47Z","published":"2025-01-22T14:18:47Z","title":"DocTTT: Test-Time Training for Handwritten Document Recognition Using\n  Meta-Auxiliary Learning","summary":"  Despite recent significant advancements in Handwritten Document Recognition\n(HDR), the efficient and accurate recognition of text against complex\nbackgrounds, diverse handwriting styles, and varying document layouts remains a\npractical challenge. Moreover, this issue is seldom addressed in academic\nresearch, particularly in scenarios with minimal annotated data available. In\nthis paper, we introduce the DocTTT framework to address these challenges. The\nkey innovation of our approach is that it uses test-time training to adapt the\nmodel to each specific input during testing. We propose a novel Meta-Auxiliary\nlearning approach that combines Meta-learning and self-supervised Masked\nAutoencoder~(MAE). During testing, we adapt the visual representation\nparameters using a self-supervised MAE loss. During training, we learn the\nmodel parameters using a meta-learning framework, so that the model parameters\nare learned to adapt to a new input effectively. Experimental results show that\nour proposed method significantly outperforms existing state-of-the-art\napproaches on benchmark datasets.\n","authors":["Wenhao Gu","Li Gu","Ziqiang Wang","Ching Yee Suen","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2501.12898v1.pdf","comment":"WACV2025, camera ready with updated reference"}],"Deblur":[{"id":"http://arxiv.org/abs/2501.12604v1","updated":"2025-01-22T03:01:54Z","published":"2025-01-22T03:01:54Z","title":"Image Motion Blur Removal in the Temporal Dimension with Video Diffusion\n  Models","summary":"  Most motion deblurring algorithms rely on spatial-domain convolution models,\nwhich struggle with the complex, non-linear blur arising from camera shake and\nobject motion. In contrast, we propose a novel single-image deblurring approach\nthat treats motion blur as a temporal averaging phenomenon. Our core innovation\nlies in leveraging a pre-trained video diffusion transformer model to capture\ndiverse motion dynamics within a latent space. It sidesteps explicit kernel\nestimation and effectively accommodates diverse motion patterns. We implement\nthe algorithm within a diffusion-based inverse problem framework. Empirical\nresults on synthetic and real-world datasets demonstrate that our method\noutperforms existing techniques in deblurring complex motion blur scenarios.\nThis work paves the way for utilizing powerful video diffusion models to\naddress single-image deblurring challenges.\n","authors":["Wang Pang","Zhihao Zhan","Xiang Zhu","Yechao Bai"],"pdf_url":"https://arxiv.org/pdf/2501.12604v1.pdf","comment":null}]},"2025-02-12T00:00:00Z":{"NeRF":[{"id":"http://arxiv.org/abs/2502.08352v1","updated":"2025-02-12T12:27:32Z","published":"2025-02-12T12:27:32Z","title":"Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images\n  with Depth and Normal Supervision","summary":"  With advancements in satellite imaging technology, acquiring high-resolution\nmulti-view satellite imagery has become increasingly accessible, enabling rapid\nand location-independent ground model reconstruction. However, traditional\nstereo matching methods struggle to capture fine details, and while neural\nradiance fields (NeRFs) achieve high-quality reconstructions, their training\ntime is prohibitively long. Moreover, challenges such as low visibility of\nbuilding facades, illumination and style differences between pixels, and weakly\ntextured regions in satellite imagery further make it hard to reconstruct\nreasonable terrain geometry and detailed building facades. To address these\nissues, we propose Sat-DN, a novel framework leveraging a progressively trained\nmulti-resolution hash grid reconstruction architecture with explicit depth\nguidance and surface normal consistency constraints to enhance reconstruction\nquality. The multi-resolution hash grid accelerates training, while the\nprogressive strategy incrementally increases the learning frequency, using\ncoarse low-frequency geometry to guide the reconstruction of fine\nhigh-frequency details. The depth and normal constraints ensure a clear\nbuilding outline and correct planar distribution. Extensive experiments on the\nDFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving\nstate-of-the-art results in both qualitative and quantitative evaluations. The\ncode is available at https://github.com/costune/SatDN.\n","authors":["Tianle Liu","Shuangming Zhao","Wanshou Jiang","Bingxuan Guo"],"pdf_url":"https://arxiv.org/pdf/2502.08352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07916v3","updated":"2025-02-12T04:19:43Z","published":"2023-10-11T22:04:33Z","title":"Dynamic Appearance Particle Neural Radiance Field","summary":"  Neural Radiance Fields (NeRFs) have shown great potential in modeling 3D\nscenes. Dynamic NeRFs extend this model by capturing time-varying elements,\ntypically using deformation fields. The existing dynamic NeRFs employ a similar\nEulerian representation for both light radiance and deformation fields. This\nleads to a close coupling of appearance and motion and lacks a physical\ninterpretation. In this work, we propose Dynamic Appearance Particle Neural\nRadiance Field (DAP-NeRF), which introduces particle-based representation to\nmodel the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists\nof the superposition of a static field and a dynamic field. The dynamic field\nis quantized as a collection of appearance particles, which carries the visual\ninformation of a small dynamic element in the scene and is equipped with a\nmotion model. All components, including the static field, the visual features\nand the motion models of particles, are learned from monocular videos without\nany prior geometric knowledge of the scene. We develop an efficient\ncomputational framework for the particle-based model. We also construct a new\ndataset to evaluate motion modeling. Experimental results show that DAP-NeRF is\nan effective technique to capture not only the appearance but also the\nphysically meaningful motions in a 3D dynamic scene. Code is available at:\nhttps://github.com/Cenbylin/DAP-NeRF.\n","authors":["Ancheng Lin","Yusheng Xiang","Jun Li","Mukesh Prasad"],"pdf_url":"https://arxiv.org/pdf/2310.07916v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19243v3","updated":"2025-02-12T00:08:30Z","published":"2024-03-28T08:58:20Z","title":"Efficient Learning With Sine-Activated Low-rank Matrices","summary":"  Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.\n","authors":["Yiping Ji","Hemanth Saratchandran","Cameron Gordon","Zeyu Zhang","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19243v3.pdf","comment":"The first two authors contributed equally. Paper accepted at ICLR\n  2025"}],"IQA":[{"id":"http://arxiv.org/abs/2502.08540v1","updated":"2025-02-12T16:24:22Z","published":"2025-02-12T16:24:22Z","title":"A Survey on Image Quality Assessment: Insights, Analysis, and Future\n  Outlook","summary":"  Image quality assessment (IQA) represents a pivotal challenge in\nimage-focused technologies, significantly influencing the advancement\ntrajectory of image processing and computer vision. Recently, IQA has witnessed\na notable surge in innovative research efforts, driven by the emergence of\nnovel architectural paradigms and sophisticated computational techniques. This\nsurvey delivers an extensive analysis of contemporary IQA methodologies,\norganized according to their application scenarios, serving as a beneficial\nreference for both beginners and experienced researchers. We analyze the\nadvantages and limitations of current approaches and suggest potential future\nresearch pathways. The survey encompasses both general and specific IQA\nmethodologies, including conventional statistical measures, machine learning\ntechniques, and cutting-edge deep learning models such as convolutional neural\nnetworks (CNNs) and Transformer models. The analysis within this survey\nhighlights the necessity for distortion-specific IQA methods tailored to\nvarious application scenarios, emphasizing the significance of practicality,\ninterpretability, and ease of implementation in future developments.\n","authors":["Chengqian Ma","Zhengyi Shi","Zhiqiang Lu","Shenghao Xie","Fei Chao","Yao Sui"],"pdf_url":"https://arxiv.org/pdf/2502.08540v1.pdf","comment":null}]},"2025-01-21T00:00:00Z":{"NeRF":[{"id":"http://arxiv.org/abs/2501.12150v1","updated":"2025-01-21T14:01:10Z","published":"2025-01-21T14:01:10Z","title":"DNRSelect: Active Best View Selection for Deferred Neural Rendering","summary":"  Deferred neural rendering (DNR) is an emerging computer graphics pipeline\ndesigned for high-fidelity rendering and robotic perception. However, DNR\nheavily relies on datasets composed of numerous ray-traced images and demands\nsubstantial computational resources. It remains under-explored how to reduce\nthe reliance on high-quality ray-traced images while maintaining the rendering\nfidelity. In this paper, we propose DNRSelect, which integrates a reinforcement\nlearning-based view selector and a 3D texture aggregator for deferred neural\nrendering. We first propose a novel view selector for deferred neural rendering\nbased on reinforcement learning, which is trained on easily obtained rasterized\nimages to identify the optimal views. By acquiring only a few ray-traced images\nfor these selected views, the selector enables DNR to achieve high-quality\nrendering. To further enhance spatial awareness and geometric consistency in\nDNR, we introduce a 3D texture aggregator that fuses pyramid features from\ndepth maps and normal maps with UV maps. Given that acquiring ray-traced images\nis more time-consuming than generating rasterized images, DNRSelect minimizes\nthe need for ray-traced data by using only a few selected views while still\nachieving high-fidelity rendering results. We conduct detailed experiments and\nablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness\nof DNRSelect. The code will be released.\n","authors":["Dongli Wu","Haochen Li","Xiaobao Wei"],"pdf_url":"https://arxiv.org/pdf/2501.12150v1.pdf","comment":"7 pages, 8 figures, submitted to ICRA 2025"},{"id":"http://arxiv.org/abs/2501.03659v4","updated":"2025-01-21T08:09:03Z","published":"2025-01-07T09:47:46Z","title":"DehazeGS: Seeing Through Fog with 3D Gaussian Splatting","summary":"  Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency. visualizations are available at\nhttps://dehazegs.github.io/\n","authors":["Jinze Yu","Yiqun Wang","Zhengda Lu","Jianwei Guo","Yong Li","Hongxing Qin","Xiaopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.03659v4.pdf","comment":"9 pages,4 figures. visualizations are available at\n  https://dehazegs.github.io/"},{"id":"http://arxiv.org/abs/2501.11884v1","updated":"2025-01-21T04:35:27Z","published":"2025-01-21T04:35:27Z","title":"Fast Underwater Scene Reconstruction using Multi-View Stereo and\n  Physical Imaging","summary":"  Underwater scene reconstruction poses a substantial challenge because of the\nintricate interplay between light and the medium, resulting in scattering and\nabsorption effects that make both depth estimation and rendering more complex.\nWhile recent Neural Radiance Fields (NeRF) based methods for underwater scenes\nachieve high-quality results by modeling and separating the scattering medium,\nthey still suffer from slow training and rendering speeds. To address these\nlimitations, we propose a novel method that integrates Multi-View Stereo (MVS)\nwith a physics-based underwater image formation model. Our approach consists of\ntwo branches: one for depth estimation using the traditional cost volume\npipeline of MVS, and the other for rendering based on the physics-based image\nformation model. The depth branch improves scene geometry, while the medium\nbranch determines the scattering parameters to achieve precise scene rendering.\nUnlike traditional MVSNet methods that rely on ground-truth depth, our method\ndoes not necessitate the use of depth truth, thus allowing for expedited\ntraining and rendering processes. By leveraging the medium subnet to estimate\nthe medium parameters and combining this with a color MLP for rendering, we\nrestore the true colors of underwater scenes and achieve higher-fidelity\ngeometric representations. Experimental results show that our method enables\nhigh-quality synthesis of novel views in scattering media, clear views\nrestoration by removing the medium, and outperforms existing methods in\nrendering quality and training efficiency.\n","authors":["Shuyi Hu","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2501.11884v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2501.12319v1","updated":"2025-01-21T17:38:55Z","published":"2025-01-21T17:38:55Z","title":"Metric for Evaluating Performance of Reference-Free Demorphing Methods","summary":"  A facial morph is an image created by combining two (or more) face images\npertaining to two (or more) distinct identities. Reference-free face demorphing\ninverts the process and tries to recover the face images constituting a facial\nmorph without using any other information. However, there is no consensus on\nthe evaluation metrics to be used to evaluate and compare such demorphing\ntechniques. In this paper, we first analyze the shortcomings of the demorphing\nmetrics currently used in the literature. We then propose a new metric called\nbiometrically cross-weighted IQA that overcomes these issues and extensively\nbenchmark current methods on the proposed metric to show its efficacy.\nExperiments on three existing demorphing methods and six datasets on two\ncommonly used face matchers validate the efficacy of our proposed metric.\n","authors":["Nitish Shukla","Arun Ross"],"pdf_url":"https://arxiv.org/pdf/2501.12319v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2501.12246v1","updated":"2025-01-21T16:07:32Z","published":"2025-01-21T16:07:32Z","title":"Video Deblurring by Sharpness Prior Detection and Edge Information","summary":"  Video deblurring is essential task for autonomous driving, facial\nrecognition, and security surveillance. Traditional methods directly estimate\nmotion blur kernels, often introducing artifacts and leading to poor results.\nRecent approaches utilize the detection of sharp frames within video sequences\nto enhance deblurring. However, existing datasets rely on fixed number of sharp\nframes, which may be too restrictive for some applications and may introduce a\nbias during model training. To address these limitations and enhance domain\nadaptability, this work first introduces GoPro Random Sharp (GoProRS), a new\ndataset where the the frequency of sharp frames within the sequence is\ncustomizable, allowing more diverse training and testing scenarios.\nFurthermore, it presents a novel video deblurring model, called SPEINet, that\nintegrates sharp frame features into blurry frame reconstruction through an\nattention-based encoder-decoder architecture, a lightweight yet robust sharp\nframe detection and an edge extraction phase. Extensive experimental results\ndemonstrate that SPEINet outperforms state-of-the-art methods across multiple\ndatasets, achieving an average of +3.2% PSNR improvement over recent\ntechniques. Given such promising results, we believe that both the proposed\nmodel and dataset pave the way for future advancements in video deblurring\nbased on the detection of sharp frames.\n","authors":["Yang Tian","Fabio Brau","Giulio Rossolini","Giorgio Buttazzo","Hao Meng"],"pdf_url":"https://arxiv.org/pdf/2501.12246v1.pdf","comment":"Under review in Pattern Recognition"}]},"2025-01-16T00:00:00Z":{"NeRF":[{"id":"http://arxiv.org/abs/2501.10474v1","updated":"2025-01-16T12:01:31Z","published":"2025-01-16T12:01:31Z","title":"Poxel: Voxel Reconstruction for 3D Printing","summary":"  Recent advancements in 3D reconstruction, especially through neural rendering\napproaches like Neural Radiance Fields (NeRF) and Plenoxel, have led to\nhigh-quality 3D visualizations. However, these methods are optimized for\ndigital environments and employ view-dependent color models (RGB) and 2D\nsplatting techniques, which do not translate well to physical 3D printing. This\npaper introduces \"Poxel\", which stands for Printable-Voxel, a voxel-based 3D\nreconstruction framework optimized for photopolymer jetting 3D printing, which\nallows for high-resolution, full-color 3D models using a CMYKWCl color model.\nOur framework directly outputs printable voxel grids by removing\nview-dependency and converting the digital RGB color space to a physical\nCMYKWCl color space suitable for multi-material jetting. The proposed system\nachieves better fidelity and quality in printed models, aligning with the\nrequirements of physical 3D objects.\n","authors":["Ruixiang Cao","Satoshi Yagi","Satoshi Yamamori","Jun Morimoto"],"pdf_url":"https://arxiv.org/pdf/2501.10474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09460v1","updated":"2025-01-16T10:42:29Z","published":"2025-01-16T10:42:29Z","title":"Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective\n  Scenes","summary":"  Neural Radiance Fields (NeRF) often struggle with reconstructing and\nrendering highly reflective scenes. Recent advancements have developed various\nreflection-aware appearance models to enhance NeRF's capability to render\nspecular reflections. However, the robust reconstruction of highly reflective\nscenes is still hindered by the inherent shape ambiguity on specular surfaces.\nExisting methods typically rely on additional geometry priors to regularize the\nshape prediction, but this can lead to oversmoothed geometry in complex scenes.\nObserving the critical role of surface normals in parameterizing reflections,\nwe introduce a transmittance-gradient-based normal estimation technique that\nremains robust even under ambiguous shape conditions. Furthermore, we propose a\ndual activated densities module that effectively bridges the gap between smooth\nsurface normals and sharp object boundaries. Combined with a reflection-aware\nappearance model, our proposed method achieves robust reconstruction and\nhigh-fidelity rendering of scenes featuring both highly specular reflections\nand intricate geometric structures. Extensive experiments demonstrate that our\nmethod outperforms existing state-of-the-art methods on various datasets.\n","authors":["Ji Shi","Xianghua Ying","Ruohao Guo","Bowei Xing","Wenzhen Yue"],"pdf_url":"https://arxiv.org/pdf/2501.09460v1.pdf","comment":"AAAI 2025, code available at https://github.com/sjj118/Normal-NeRF"}],"Deblur":[{"id":"http://arxiv.org/abs/2501.09396v1","updated":"2025-01-16T09:07:01Z","published":"2025-01-16T09:07:01Z","title":"Joint Transmission and Deblurring: A Semantic Communication Approach\n  Using Events","summary":"  Deep learning-based joint source-channel coding (JSCC) is emerging as a\npromising technology for effective image transmission. However, most existing\napproaches focus on transmitting clear images, overlooking real-world\nchallenges such as motion blur caused by camera shaking or fast-moving objects.\nMotion blur often degrades image quality, making transmission and\nreconstruction more challenging. Event cameras, which asynchronously record\npixel intensity changes with extremely low latency, have shown great potential\nfor motion deblurring tasks. However, the efficient transmission of the\nabundant data generated by event cameras remains a significant challenge. In\nthis work, we propose a novel JSCC framework for the joint transmission of\nblurry images and events, aimed at achieving high-quality reconstructions under\nlimited channel bandwidth. This approach is designed as a deblurring\ntask-oriented JSCC system. Since RGB cameras and event cameras capture the same\nscene through different modalities, their outputs contain both shared and\ndomain-specific information. To avoid repeatedly transmitting the shared\ninformation, we extract and transmit their shared information and\ndomain-specific information, respectively. At the receiver, the received\nsignals are processed by a deblurring decoder to generate clear images.\nAdditionally, we introduce a multi-stage training strategy to train the\nproposed model. Simulation results demonstrate that our method significantly\noutperforms existing JSCC-based image transmission schemes, addressing motion\nblur effectively.\n","authors":["Pujing Yang","Guangyi Zhang","Yunlong Cai","Lei Yu","Guanding Yu"],"pdf_url":"https://arxiv.org/pdf/2501.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09321v1","updated":"2025-01-16T06:25:56Z","published":"2025-01-16T06:25:56Z","title":"Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention\n  for Image Restoration Models Compression","summary":"  Transformer-based encoder-decoder models have achieved remarkable success in\nimage-to-image transfer tasks, particularly in image restoration. However,\ntheir high computational complexity-manifested in elevated FLOPs and parameter\ncounts-limits their application in real-world scenarios. Existing knowledge\ndistillation methods in image restoration typically employ lightweight student\nmodels that directly mimic the intermediate features and reconstruction results\nof the teacher, overlooking the implicit attention relationships between them.\nTo address this, we propose a Soft Knowledge Distillation (SKD) strategy that\nincorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for\ncompressing image restoration models. This mechanism facilitates interaction\nbetween the student and teacher across both channel and spatial dimensions,\nenabling the student to implicitly learn the attention matrices. Additionally,\nwe employ a Gaussian kernel function to measure the distance between student\nand teacher features in kernel space, ensuring stable and efficient feature\nlearning. To further enhance the quality of reconstructed images, we replace\nthe commonly used L1 or KL divergence loss with a contrastive learning loss at\nthe image level. Experiments on three tasks-image deraining, deblurring, and\ndenoising-demonstrate that our SKD strategy significantly reduces computational\ncomplexity while maintaining strong image restoration capabilities.\n","authors":["Yongheng Zhang","Danfeng Yan"],"pdf_url":"https://arxiv.org/pdf/2501.09321v1.pdf","comment":"Accepted by ICASSP2025"}]},"2025-01-14T00:00:00Z":{"NeRF":[{"id":"http://arxiv.org/abs/2306.09349v4","updated":"2025-01-14T22:05:06Z","published":"2023-06-15T17:59:59Z","title":"UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video","summary":"  We present UrbanIR (Urban Scene Inverse Rendering), a new inverse graphics\nmodel that enables realistic, free-viewpoint renderings of scenes under various\nlighting conditions with a single video. It accurately infers shape, albedo,\nvisibility, and sun and sky illumination from wide-baseline videos, such as\nthose from car-mounted cameras, differing from NeRF's dense view settings. In\nthis context, standard methods often yield subpar geometry and material\nestimates, such as inaccurate roof representations and numerous 'floaters'.\nUrbanIR addresses these issues with novel losses that reduce errors in inverse\ngraphics inference and rendering artifacts. Its techniques allow for precise\nshadow volume estimation in the original scene. The model's outputs support\ncontrollable editing, enabling photorealistic free-viewpoint renderings of\nnight simulations, relit scenes, and inserted objects, marking a significant\nimprovement over existing state-of-the-art methods.\n","authors":["Chih-Hao Lin","Bohan Liu","Yi-Ting Chen","Kuan-Sheng Chen","David Forsyth","Jia-Bin Huang","Anand Bhattad","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2306.09349v4.pdf","comment":"https://urbaninverserendering.github.io/"},{"id":"http://arxiv.org/abs/2501.07015v2","updated":"2025-01-14T21:02:31Z","published":"2025-01-13T02:28:13Z","title":"SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting","summary":"  Achieving high-fidelity 3D reconstruction from monocular video remains\nchallenging due to the inherent limitations of traditional methods like\nStructure-from-Motion (SfM) and monocular SLAM in accurately capturing scene\ndetails. While differentiable rendering techniques such as Neural Radiance\nFields (NeRF) address some of these challenges, their high computational costs\nmake them unsuitable for real-time applications. Additionally, existing 3D\nGaussian Splatting (3DGS) methods often focus on photometric consistency,\nneglecting geometric accuracy and failing to exploit SLAM's dynamic depth and\npose updates for scene refinement. We propose a framework integrating dense\nSLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach\nintroduces SLAM-Informed Adaptive Densification, which dynamically updates and\ndensifies the Gaussian model by leveraging dense point clouds from SLAM.\nAdditionally, we incorporate Geometry-Guided Optimization, which combines\nedge-aware geometric constraints and photometric consistency to jointly\noptimize the appearance and geometry of the 3DGS scene representation, enabling\ndetailed and accurate SLAM mapping reconstruction. Experiments on the Replica\nand TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving\nstate-of-the-art results among monocular systems. Specifically, our method\nachieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,\nrepresenting improvements of 10.7%, 6.4%, and 49.4%, respectively, over the\nprevious SOTA. On TUM-RGBD, our method outperforms the closest baseline by\n10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the\npotential of our framework in bridging the gap between photometric and\ngeometric dense 3D scene representations, paving the way for practical and\nefficient monocular dense reconstruction.\n","authors":["Yue Hu","Rong Liu","Meida Chen","Peter Beerel","Andrew Feng"],"pdf_url":"https://arxiv.org/pdf/2501.07015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08286v1","updated":"2025-01-14T18:01:15Z","published":"2025-01-14T18:01:15Z","title":"VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large\n  Scenes","summary":"  VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework\ndesigned for large scenes. The framework comprises four main components: VIO\nFront End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO\nFront End, RGB frames are processed through dense bundle adjustment and\nuncertainty estimation to extract scene geometry and poses. Based on this\noutput, the mapping module incrementally constructs and maintains a 2D Gaussian\nmap. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,\nScore Manager, and Pose Refinement, which collectively improve mapping speed\nand localization accuracy. This enables the SLAM system to handle large-scale\nurban environments with up to 50 million Gaussian ellipsoids. To ensure global\nconsistency in large-scale scenes, we design a Loop Closure module, which\ninnovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian\nSplatting for loop closure detection and correction of the Gaussian map.\nAdditionally, we propose a Dynamic Eraser to address the inevitable presence of\ndynamic objects in real-world outdoor scenes. Extensive evaluations in indoor\nand outdoor environments demonstrate that our approach achieves localization\nperformance on par with Visual-Inertial Odometry while surpassing recent\nGS/NeRF SLAM methods. It also significantly outperforms all existing methods in\nterms of mapping and rendering quality. Furthermore, we developed a mobile app\nand verified that our framework can generate high-quality Gaussian maps in real\ntime using only a smartphone camera and a low-frequency IMU sensor. To the best\nof our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method\ncapable of operating in outdoor environments and supporting kilometer-scale\nlarge scenes.\n","authors":["Ke Wu","Zicheng Zhang","Muer Tie","Ziqing Ai","Zhongxue Gan","Wenchao Ding"],"pdf_url":"https://arxiv.org/pdf/2501.08286v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2501.08415v1","updated":"2025-01-14T20:12:09Z","published":"2025-01-14T20:12:09Z","title":"Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics","summary":"  Recent studies have revealed that modern image and video quality assessment\n(IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can\nmanipulate a video through preprocessing to artificially increase its quality\nscore according to a certain metric, despite no actual improvement in visual\nquality. Most of the attacks studied in the literature are white-box attacks,\nwhile black-box attacks in the context of VQA have received less attention.\nMoreover, some research indicates a lack of transferability of adversarial\nexamples generated for one model to another when applied to VQA. In this paper,\nwe propose a cross-modal attack method, IC2VQA, aimed at exploring the\nvulnerabilities of modern VQA models. This approach is motivated by the\nobservation that the low-level feature spaces of images and videos are similar.\nWe investigate the transferability of adversarial perturbations across\ndifferent modalities; specifically, we analyze how adversarial perturbations\ngenerated on a white-box IQA model with an additional CLIP module can\neffectively target a VQA model. The addition of the CLIP module serves as a\nvaluable aid in increasing transferability, as the CLIP model is known for its\neffective capture of low-level semantics. Extensive experiments demonstrate\nthat IC2VQA achieves a high success rate in attacking three black-box VQA\nmodels. We compare our method with existing black-box attack strategies,\nhighlighting its superiority in terms of attack success within the same number\nof iterations and levels of attack strength. We believe that the proposed\nmethod will contribute to the deeper analysis of robust VQA metrics.\n","authors":["Georgii Gotin","Ekaterina Shumitskaya","Anastasia Antsiferova","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2501.08415v1.pdf","comment":"Accepted for VISAPP 2025"}]},"2025-01-20T00:00:00Z":{"HDR":[{"id":"http://arxiv.org/abs/2501.11515v1","updated":"2025-01-20T14:45:07Z","published":"2025-01-20T14:45:07Z","title":"UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion","summary":"  Capturing high dynamic range (HDR) scenes is one of the most important issues\nin camera design. Majority of cameras use exposure fusion technique, which\nfuses images captured by different exposure levels, to increase dynamic range.\nHowever, this approach can only handle images with limited exposure difference,\nnormally 3-4 stops. When applying to very high dynamic scenes where a large\nexposure difference is required, this approach often fails due to incorrect\nalignment or inconsistent lighting between inputs, or tone mapping artifacts.\nIn this work, we propose UltraFusion, the first exposure fusion technique that\ncan merge input with 9 stops differences. The key idea is that we model the\nexposure fusion as a guided inpainting problem, where the under-exposed image\nis used as a guidance to fill the missing information of over-exposed highlight\nin the over-exposed region. Using under-exposed image as a soft guidance,\ninstead of a hard constrain, our model is robust to potential alignment issue\nor lighting variations. Moreover, utilizing the image prior of the generative\nmodel, our model also generates natural tone mapping, even for very\nhigh-dynamic range scene. Our approach outperforms HDR-Transformer on latest\nHDR benchmarks. Moreover, to test its performance in ultra high dynamic range\nscene, we capture a new real-world exposure fusion benchmark, UltraFusion\nDataset, with exposure difference up to 9 stops, and experiments show that\n\\model~can generate beautiful and high-quality fusion results under various\nscenarios. An online demo is provided at\nhttps://openimaginglab.github.io/UltraFusion/.\n","authors":["Zixuan Chen","Yujin Wang","Xin Cai","Zhiyuan You","Zheming Lu","Fan Zhang","Shi Guo","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2501.11515v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2501.11561v1","updated":"2025-01-20T16:04:57Z","published":"2025-01-20T16:04:57Z","title":"Teaching Large Language Models to Regress Accurate Image Quality Scores\n  using Score Distribution","summary":"  With the rapid advancement of Multi-modal Large Language Models (MLLMs),\nMLLM-based Image Quality Assessment (IQA) methods have shown promising\nperformance in linguistic quality description. However, current methods still\nfall short in accurately scoring image quality. In this work, we aim to\nleverage MLLMs to regress accurate quality scores. A key challenge is that the\nquality score is inherently continuous, typically modeled as a Gaussian\ndistribution, whereas MLLMs generate discrete token outputs. This mismatch\nnecessitates score discretization. Previous approaches discretize the mean\nscore into a one-hot label, resulting in information loss and failing to\ncapture inter-image relationships. We propose a distribution-based approach\nthat discretizes the score distribution into a soft label. This method\npreserves the characteristics of the score distribution, achieving high\naccuracy and maintaining inter-image relationships. Moreover, to address\ndataset variation, where different IQA datasets exhibit various distributions,\nwe introduce a fidelity loss based on Thurstone's model. This loss captures\nintra-dataset relationships, facilitating co-training across multiple IQA\ndatasets. With these designs, we develop the distribution-based Depicted image\nQuality Assessment model for Score regression (DeQA-Score). Experiments across\nmultiple benchmarks show that DeQA-Score stably outperforms baselines in score\nregression. Also, DeQA-Score can predict the score distribution that closely\naligns with human annotations. Codes and model weights have been released in\nhttps://depictqa.github.io/deqa-score/.\n","authors":["Zhiyuan You","Xin Cai","Jinjin Gu","Tianfan Xue","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2501.11561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11520v1","updated":"2025-01-20T14:49:54Z","published":"2025-01-20T14:49:54Z","title":"Fundus Image Quality Assessment and Enhancement: a Systematic Review","summary":"  As an affordable and convenient eye scan, fundus photography holds the\npotential for preventing vision impairment, especially in resource-limited\nregions. However, fundus image degradation is common under intricate imaging\nenvironments, impacting following diagnosis and treatment. Consequently, image\nquality assessment (IQA) and enhancement (IQE) are essential for ensuring the\nclinical value and reliability of fundus images. While existing reviews offer\nsome overview of this field, a comprehensive analysis of the interplay between\nIQA and IQE, along with their clinical deployment challenges, is lacking. This\npaper addresses this gap by providing a thorough review of fundus IQA and IQE\nalgorithms, research advancements, and practical applications. We outline the\nfundamentals of the fundus photography imaging system and the associated\ninterferences, and then systematically summarize the paradigms in fundus IQA\nand IQE. Furthermore, we discuss the practical challenges and solutions in\ndeploying IQA and IQE, as well as offer insights into potential future research\ndirections.\n","authors":["Heng Li","Haojin Li","Mingyang Ou","Xiangyang Yu","Xiaoqing Zhang","Ke Niu","Huazhu Fu","Jiang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.11520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11511v1","updated":"2025-01-20T14:39:50Z","published":"2025-01-20T14:39:50Z","title":"Subjective and Objective Quality Assessment of Non-Uniformly Distorted\n  Omnidirectional Images","summary":"  Omnidirectional image quality assessment (OIQA) has been one of the hot\ntopics in IQA with the continuous development of VR techniques, and achieved\nmuch success in the past few years. However, most studies devote themselves to\nthe uniform distortion issue, i.e., all regions of an omnidirectional image are\nperturbed by the ``same amount'' of noise, while ignoring the non-uniform\ndistortion issue, i.e., partial regions undergo ``different amount'' of\nperturbation with the other regions in the same omnidirectional image.\nAdditionally, nearly all OIQA models are verified on the platforms containing a\nlimited number of samples, which largely increases the over-fitting risk and\ntherefore impedes the development of OIQA. To alleviate these issues, we\nelaborately explore this topic from both subjective and objective perspectives.\nSpecifically, we construct a large OIQA database containing 10,320\nnon-uniformly distorted omnidirectional images, each of which is generated by\nconsidering quality impairments on one or two camera len(s). Then we\nmeticulously conduct psychophysical experiments and delve into the influence of\nboth holistic and individual factors (i.e., distortion range and viewing\ncondition) on omnidirectional image quality. Furthermore, we propose a\nperception-guided OIQA model for non-uniform distortion by adaptively\nsimulating users' viewing behavior. Experimental results demonstrate that the\nproposed model outperforms state-of-the-art methods. The source code is\navailable at https://github.com/RJL2000/OIQAND.\n","authors":["Jiebin Yan","Jiale Rao","Xuelin Liu","Yuming Fang","Yifan Zuo","Weide Liu"],"pdf_url":"https://arxiv.org/pdf/2501.11511v1.pdf","comment":null}]}}